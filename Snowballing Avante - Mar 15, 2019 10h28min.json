{"title":"Snowballing Avante","researchers":"Leandro Ungari Cayres","description":"Etapa de snowballing avante na revisão sistemática","goals":"Avaliar os presentes estudos","addCriterion":[{"id":"CA0","criterion":"O estudo apresenta análise do histórico de versões do código-fonte no contexto de modificações de código-fonte ou análise de qualidade de código"},{"id":"CA1","criterion":"O estudo apresenta técnicas, abordagens relativas a detecção de modificações no código-fonte"},{"id":"CA2","criterion":"O estudo propõe ou relata algo relativo a aprendizado automatizado de modificações com análise direta do código-fonte"},{"id":"CA3","criterion":"O estudo relaciona-se ao aprendizado de padrões de modificação de código-fonte"},{"id":"CA4","criterion":"O estudo relaciona-se a mensuração da qualidade de código em modificações"},{"id":"CA5","criterion":"O estudo analisa o impacto de melhorias, defeitos na qualidade do código-fonte"},{"id":"CA6","criterion":"O estudo relata o uso de métricas para análise e classificação da qualidade do código-fonte"},{"id":"CA7","criterion":"O estudo apresenta alguma estratégia do sugestão de modificações"}],"deleteCriterion":[{"id":"CR0","criterion":"O estudo não possui resumo"},{"id":"CR1","criterion":"O estudo somente está publicado como resumo ou pôster"},{"id":"CR2","criterion":"O estudo não está escrito em inglês"},{"id":"CR3","criterion":"O estudo é uma versão mais antiga do outro estudo já considerado"},{"id":"CR4","criterion":"O estudo não é um estudo primário"},{"id":"CR5","criterion":"Não foi possível ter acesso ao estudo"},{"id":"CR6","criterion":"O estudo foi publicado antes de 2012"},{"id":"CR7","criterion":"O estudo não apresenta análise do código-fonte direta, mas sim utiliza outras abordagens"},{"id":"CR8","criterion":"O estudo envolve modificações de código-fonte muito específicas de um dado contexto"},{"id":"CR9","criterion":"O estudo não é relacionado a área de Engenharia de Software"},{"id":"CR10","criterion":"O estudo não é relacionado a modificações de código-fonte"},{"id":"CR11","criterion":"O estudo é relacionado a modificações de código, mas não visa identificar, aprender ou analisar o impacto das modificações na qualidade"},{"id":"CR12","criterion":"O estudo envolve modificações de código, porém é voltado para outras aplicações como visualização"}],"articles":[{"id":"Geral0","name":"Changes as first-class citizens: A research perspective on modern software tooling","authors":"Soetens, Q.D. and Robbes, R. and Demeyer, S.","year":2017,"base":["Geral"],"abstract":"Software must evolve to keep up with an ever-changing context, the real world. We discuss an emergent trend in software evolution research revolving around the central notion that drives evolution: Change. By reifying change, and by modelling it as a first-class entity, researchers can now analyse the complex phenomenon known as software evolution with an unprecedented degree of accuracy. We present a Systematic Mapping Study of 86 articles to give an overview on the state of the art in this area of research and present a roadmap with open issues and future directions. © 2017 ACM.","doi":"10.1145/3038926","bibtex":"@"},{"id":"Geral1","name":"Renaming and shifted code in structured merging: Looking ahead for precision and performance","authors":"Lesenich, O. and Apel, S. and Kastner, C. and Seibt, G. and Siegmund, J.","year":2017,"base":["Geral"],"abstract":"Diffing and merging of source-code artifacts is an essential task when integrating changes in software versions. While state-of-the-art line-based merge tools (e.g., git merge) are fast and independent of the programming language used, they have only a low precision. Recently, it has been shown that the precision of merging can be substantially improved by using a language-aware, structured approach that works on abstract syntax trees. But, precise structured merging is NP hard, especially, when considering the notoriously difficult scenarios of renamings and shifted code. To address these scenarios without compromising scalability, we propose a syntax-aware, heuristic optimization for structured merging that employs a lookahead mechanism during tree matching. The key idea is that renamings and shifted code are not arbitrarily distributed, but their occurrence follows patterns, which we address with a syntax-specific lookahead. Our experiments with 48 real-world open-source projects (4,878 merge scenarios with over 400 million lines of code) demonstrate that we can significantly improve matching precision in 28 percent of cases while maintaining performance. © 2017 IEEE.","doi":"10.1109/ASE.2017.8115665","bibtex":"@ARTICLE{Soetens2017,\nauthor={Soetens, Q.D. and Robbes, R. and Demeyer, S.},\ntitle={Changes as first-class citizens: A research perspective on modern software tooling},\njournal={ACM Computing Surveys},\nyear={2017},\nvolume={50},\nnumber={2},\ndoi={10.1145/3038926},\nart_number={18},\nnote={cited By 4},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017462023&doi=10.1145%2f3038926&partnerID=40&md5=726931a538fb97503e862ace4373c80f},\nabstract={Software must evolve to keep up with an ever-changing context, the real world. We discuss an emergent trend in software evolution research revolving around the central notion that drives evolution: Change. By reifying change, and by modelling it as a first-class entity, researchers can now analyse the complex phenomenon known as software evolution with an unprecedented degree of accuracy. We present a Systematic Mapping Study of 86 articles to give an overview on the state of the art in this area of research and present a roadmap with open issues and future directions. © 2017 ACM.},\nkeywords={Computer science;  Surveys, Atomic changes;  Change distilling;  Change recording;  Fine grained;  Fine-grained changes;  Systematic mapping studies, Mapping},\ndocument_type={Review},\nsource={Scopus},\n}\n\n"},{"id":"Geral2","name":"Imprecisions diagnostic in source code deltas","authors":"De La Torre, G. and Robbes, R. and Bergel, A.","year":2018,"base":["Geral"],"abstract":"Beyond a practical use in code review, source code change detection (SCCD) is an important component of many mining software repositories (MSR) approaches. As such, any error or imprecision in the detection may result in a wrong conclusion while mining repositories. We identified, analyzed, and characterized impressions in GumTree, which is the most advanced algorithm for SCCD. After analyzing its detection accuracy over a curated corpus of 107 C# projects, we diagnosed several imprecisions. Many of our findings confirm that a more language-aware perspective of GumTree can be helpful in reporting more precise changes. © 2018 ACM.","doi":"10.1145/3196398.3196404","bibtex":"@CONFERENCE{Lesenich2017543,\nauthor={Lesenich, O. and Apel, S. and Kastner, C. and Seibt, G. and Siegmund, J.},\ntitle={Renaming and shifted code in structured merging: Looking ahead for precision and performance},\njournal={ASE 2017 - Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering},\nyear={2017},\npages={543-553},\ndoi={10.1109/ASE.2017.8115665},\nart_number={8115665},\nnote={cited By 0},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041447888&doi=10.1109%2fASE.2017.8115665&partnerID=40&md5=0cd5074ea7a3c39e1528eee9dd91692f},\nabstract={Diffing and merging of source-code artifacts is an essential task when integrating changes in software versions. While state-of-the-art line-based merge tools (e.g., git merge) are fast and independent of the programming language used, they have only a low precision. Recently, it has been shown that the precision of merging can be substantially improved by using a language-aware, structured approach that works on abstract syntax trees. But, precise structured merging is NP hard, especially, when considering the notoriously difficult scenarios of renamings and shifted code. To address these scenarios without compromising scalability, we propose a syntax-aware, heuristic optimization for structured merging that employs a lookahead mechanism during tree matching. The key idea is that renamings and shifted code are not arbitrarily distributed, but their occurrence follows patterns, which we address with a syntax-specific lookahead. Our experiments with 48 real-world open-source projects (4,878 merge scenarios with over 400 million lines of code) demonstrate that we can significantly improve matching precision in 28 percent of cases while maintaining performance. © 2017 IEEE.},\nkeywords={Codes (symbols);  Mergers and acquisitions;  Merging;  Open source software;  Software engineering;  Syntactics;  Trees (mathematics), Abstract Syntax Trees;  Heuristic optimization;  Lines of code;  Open source projects;  Software versions;  State of the art;  Structured approach;  Tree-matching, Open systems},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral3","name":"Extracting Build Changes with BUILDDIFF","authors":"Macho, C. and McIntosh, S. and Pinzger, M.","year":2017,"base":["Geral"],"abstract":"Build systems are an essential part of modern software engineering projects. As software projects change continuously, it is crucial to understand how the build system changes because neglecting its maintenance can lead to expensive build breakage. Recent studies have investigated the (co-)evolution of build configurations and reasons for build breakage, but they did this only on a coarse grained level. In this paper, we present BUILDDIFF, an approach to extract detailed build changes from MAVEN build files and classify them into 95 change types. In a manual evaluation of 400 build changing commits, we show that BUILDDIFF can extract and classify build changes with an average precision and recall of 0.96 and 0.98, respectively. We then present two studies using the build changes extracted from 30 open source Java projects to study the frequency and time of build changes. The results show that the top 10 most frequent change types account for 73% of the build changes. Among them, changes to version numbers and changes to dependencies of the projects occur most frequently. Furthermore, our results show that build changes occur frequently around releases. With these results, we provide the basis for further research, such as for analyzing the (co-)evolution of build files with other artifacts or improving effort estimation approaches. Furthermore, our detailed change information enables improvements of refactoring approaches for build configurations and improvements of models to identify error-prone build files. © 2017 IEEE.","doi":"10.1109/MSR.2017.65","bibtex":"@CONFERENCE{DeLaTorre2018492,\nauthor={De La Torre, G. and Robbes, R. and Bergel, A.},\ntitle={Imprecisions diagnostic in source code deltas},\njournal={Proceedings - International Conference on Software Engineering},\nyear={2018},\npages={492-502},\ndoi={10.1145/3196398.3196404},\nnote={cited By 0},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051659811&doi=10.1145%2f3196398.3196404&partnerID=40&md5=6134dc01de9588886846ec348c75840c},\nabstract={Beyond a practical use in code review, source code change detection (SCCD) is an important component of many mining software repositories (MSR) approaches. As such, any error or imprecision in the detection may result in a wrong conclusion while mining repositories. We identified, analyzed, and characterized impressions in GumTree, which is the most advanced algorithm for SCCD. After analyzing its detection accuracy over a curated corpus of 107 C# projects, we diagnosed several imprecisions. Many of our findings confirm that a more language-aware perspective of GumTree can be helpful in reporting more precise changes. © 2018 ACM.},\nkeywords={Codes (symbols);  Computer programming languages;  Image quality, Detection accuracy;  differencing;  gumtree;  Mining repositories;  Mining software repository (MSR);  Practical use;  Source code changes;  Source codes, Software engineering},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral4","name":"Quantifying the transition from Python 2 to 3: an empirical study of Python applications","authors":"Malloy, Brian A and Power, James F","year":2017,"base":["Geral"],"booktitle":"2017 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)","bibtex":"@CONFERENCE{Macho2017368,\nauthor={Macho, C. and McIntosh, S. and Pinzger, M.},\ntitle={Extracting Build Changes with BUILDDIFF},\njournal={IEEE International Working Conference on Mining Software Repositories},\nyear={2017},\npages={368-378},\ndoi={10.1109/MSR.2017.65},\nart_number={7962386},\nnote={cited By 5},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026499815&doi=10.1109%2fMSR.2017.65&partnerID=40&md5=5b3ee73c53acc98957daf9a2d9cce1d5},\nabstract={Build systems are an essential part of modern software engineering projects. As software projects change continuously, it is crucial to understand how the build system changes because neglecting its maintenance can lead to expensive build breakage. Recent studies have investigated the (co-)evolution of build configurations and reasons for build breakage, but they did this only on a coarse grained level. In this paper, we present BUILDDIFF, an approach to extract detailed build changes from MAVEN build files and classify them into 95 change types. In a manual evaluation of 400 build changing commits, we show that BUILDDIFF can extract and classify build changes with an average precision and recall of 0.96 and 0.98, respectively. We then present two studies using the build changes extracted from 30 open source Java projects to study the frequency and time of build changes. The results show that the top 10 most frequent change types account for 73% of the build changes. Among them, changes to version numbers and changes to dependencies of the projects occur most frequently. Furthermore, our results show that build changes occur frequently around releases. With these results, we provide the basis for further research, such as for analyzing the (co-)evolution of build files with other artifacts or improving effort estimation approaches. Furthermore, our detailed change information enables improvements of refactoring approaches for build configurations and improvements of models to identify error-prone build files. © 2017 IEEE.},\nkeywords={Computer software maintenance;  Computer software selection and evaluation;  Maintenance;  Software engineering, Build systems;  Coarse-grained;  Effort Estimation;  Open sources;  Precision and recall;  Software engineering projects;  Software project;  Software Quality, Open source software},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n","abstract":"Background: Python is one of the most popular modern programming languages. In 2008 its authors introduced a new version of the language, Python 3.0, that was not backward compatible with Python 2, initiating a transitional phase for Python software developers. Aims: The study described in this paper investigates the degree to which Python software developers are making the transition from Python 2 to Python 3. Method: We have developed a Python compliance analyser, PyComply, and have assembled a large corpus of Python applications. We use PyComply to measure and quantify the degree to which Python 3 features are being used, as well as the rate and context of their adoption. Results: In fact, Python software developers are not exploiting the new features and advantages of Python 3, but rather are choosing to retain backward compatibility with Python 2. Conclusions: Python developers are confining themselves to a language subset, governed by the diminishing intersection of Python 2, which is not under development, and Python 3, which is under development with new features being introduced as the language continues to evolve."},{"id":"Geral5","name":"Evaluation of sampling techniques in software fault prediction using metrics and code smells","authors":"Kaur, Kamaldeep and Kaur, Parmeet","year":2017,"base":["Geral"],"booktitle":"2017 International Conference on Advances in Computing, Communications and Informatics (ICACCI)","bibtex":"@inproceedings{malloy2017quantifying,\n  title={Quantifying the transition from Python 2 to 3: an empirical study of Python applications},\n  author={Malloy, Brian A and Power, James F},\n  booktitle={2017 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},\n  pages={314--323},\n  year={2017},\n  organization={IEEE}\n}\n\n","abstract":"The highly imbalanced nature of software fault datasets results in poor performance of machine leaning techniques used for software fault prediction. The objective of this paper is to evaluate sampling techniques and Meta-Cost learning in software fault prediction to alleviate problem of imbalanced data. We evaluate four sampling techniques in metrics as well as code smells based fault prediction on fault data sets of two open source systems ANT and POI. Our results indicate that Resample technique is best for metrics based fault prediction whereas Synthetic Minority Oversampling is best suited for code smells based fault prediction. The results are presented in terms of accuracy measures like G-Mean, F-measure and area under ROC curve. We also evaluate Meta-Cost learning and found that all sampling techniques outperform Meta-Cost learning. Our results also indicate that software metrics are better predictor of software faults than code smells."},{"id":"Geral6","name":"Ten years of JDeodorant: Lessons learned from the hunt for smells","authors":"Tsantalis, Nikolaos and Chaikalis, Theodoros and Chatzigeorgiou, Alexander","year":2018,"base":["Geral"],"booktitle":"2018 IEEE 25th International Conference on Software Analysis, Evolution and Reengineering (SANER)","bibtex":"@inproceedings{kaur2017evaluation,\n  title={Evaluation of sampling techniques in software fault prediction using metrics and code smells},\n  author={Kaur, Kamaldeep and Kaur, Parmeet},\n  booktitle={2017 International Conference on Advances in Computing, Communications and Informatics (ICACCI)},\n  pages={1377--1387},\n  year={2017},\n  organization={IEEE}\n}\n\n","abstract":"Deodorants are different from perfumes, because they are applied directly on body and by killing bacteria they reduce odours and offer a refreshing fragrance. That was our goal when we first thought about \"bad smells\" in code: to develop techniques for effectively identifying and removing (i.e., deodorizing) code smells from object-oriented software. JDeodorant encompasses a number of techniques for suggesting and automatically applying refactoring opportunities on Java source code, in a way that requires limited effort on behalf of the developer. In contrast to other approaches that rely on generic strategies that can be adapted to various smells, JDeodorant adopts ad-hoc strategies for each smell considering the particular characteristics of the underlying design or code problem. In this retrospective paper, we discuss the impact of JDeodorant over the last ten years and a number of tools and techniques that have been developed for a similar purpose which either compare their results with JDeodorant or have built on top of JDeodorant. Finally, we discuss the empirical findings from a number of studies that employed JDeodorant to extract their datasets."},{"id":"Geral7","name":"Detection Strategies of Bad Smells in Highly Configurable Software","authors":"Faujdar,N. and Srivastav,K. and Gupta,M. and Saraswat,S.","year":2018,"base":["Geral"],"booktitle":"Proceedings of the 8th International Conference Confluence 2018 on Cloud Computing, Data Science and Engineering, Confluence 2018","bibtex":"@inproceedings{tsantalis2018ten,\n  title={Ten years of JDeodorant: Lessons learned from the hunt for smells},\n  author={Tsantalis, Nikolaos and Chaikalis, Theodoros and Chatzigeorgiou, Alexander},\n  booktitle={2018 IEEE 25th International Conference on Software Analysis, Evolution and Reengineering (SANER)},\n  pages={4--14},\n  year={2018},\n  organization={IEEE}\n}\n\n","abstract":"Software maintenance is a tough work and when code become very large its hard to track the changes and bad code makes it harder. One way to track the quality of software is to track for bad smells in the software. They can help track the code which can cause problems in near future. The objective is to build a bot that crawls through our code daily and gives a status of smells in the code. To achieve this, developed a set of instruction and strategies and implemented them in python to parse code of java and track the software with time. All statistics are shown with the python library matplotlib and the bot can be automated to crawl in Linux, Mac, and Windows. Found that, the bot can detect smells which are not detectable by the developer. The life of software can be increased by a refracting the code time to time with the help of smells detected by the bot. Only java is supported till now but this paper increased the support to other languages also."},{"id":"Geral8","name":"DiffViz: A diff algorithm independent visualization tool for edit scripts","authors":"Frick, V. and Wedenig, C. and Pinzger, M.","year":2018,"base":["Geral"],"abstract":"A number of approaches and tools exist that extract and visualize the changes between two versions of a file and thereby help developers to understand them. DiffViz is an interactive visualization tool that visualizes the changes independent from the differencing algorithm. It supports, but is not limited to, a granularity on the level of abstract syntax trees. Furthermore, it provides several new features, such as node matching and the mini-map, to navigate and analyze the changes. A demo of the installation and example usage of the tool is available here: https://youtu.be/RF93ey9GYoc. © 2018 IEEE.","doi":"10.1109/ICSME.2018.00081","bibtex":"@inproceedings{\n        author={Faujdar,N. and Srivastav,K. and Gupta,M. and Saraswat,S.},\n        editor={ },\n        year={2018},\n        title={Detection Strategies of Bad Smells in Highly Configurable Software},\n        booktitle={Proceedings of the 8th International Conference Confluence 2018 on Cloud Computing, Data Science and Engineering, Confluence 2018},\n        pages={31-35},\n        language={English},\n        url={www.scopus.com},\n}\n\n"},{"id":"Geral9","name":"A large-scale study on repetitiveness, containment, and composability of routines in open-source projects","authors":"Nguyen, Anh Tuan and Nguyen, Hoan Anh and Nguyen, Tien N","year":2016,"base":["Geral"],"booktitle":"2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR)","bibtex":"@CONFERENCE{Frick2018705,\nauthor={Frick, V. and Wedenig, C. and Pinzger, M.},\ntitle={DiffViz: A diff algorithm independent visualization tool for edit scripts},\njournal={Proceedings - 2018 IEEE International Conference on Software Maintenance and Evolution, ICSME 2018},\nyear={2018},\npages={705-709},\ndoi={10.1109/ICSME.2018.00081},\nart_number={8530084},\nnote={cited By 0},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058307098&doi=10.1109%2fICSME.2018.00081&partnerID=40&md5=215cc7fd14aa3544b5c3d9a72845b216},\nabstract={A number of approaches and tools exist that extract and visualize the changes between two versions of a file and thereby help developers to understand them. DiffViz is an interactive visualization tool that visualizes the changes independent from the differencing algorithm. It supports, but is not limited to, a granularity on the level of abstract syntax trees. Furthermore, it provides several new features, such as node matching and the mini-map, to navigate and analyze the changes. A demo of the installation and example usage of the tool is available here: https://youtu.be/RF93ey9GYoc. © 2018 IEEE.},\nkeywords={Computer software maintenance;  Flow visualization;  Trees (mathematics), Abstract Syntax Trees;  Differencing algorithm;  Edit scripts;  Interactive visualization tool;  It supports;  Visualization tools, Visualization},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n","abstract":"Source code in software systems has been shown to have a good degree of repetitiveness at the lexical, syntactical, and API usage levels. This paper presents a large-scale study on the repetitiveness, containment, and composability of source code at the semantic level. We collected a large dataset consisting of 9,224 Java projects with 2.79M class files, 17.54M methods with 187M SLOCs. For each method in a project, we build the program dependency graph (PDG) to represent a routine, and compare PDGs with one another as well as the subgraphs within them. We found that within a project, 12.1% of the routines are repeated, and most of them repeat from 2-7 times. As entirety, the routines are quite project-specific with only 3.3% of them exactly repeating in 1-4 other projects with at most 8 times. We also found that 26.1% and 7.27% of the routines are contained in other routine(s), i.e., implemented as part of other routine(s) elsewhere within a project and in other projects, respectively. Except for trivial routines, their repetitiveness and containment is independent of their complexity. Defining a subroutine via a per-variable slicing subgraph in a PDG, we found that 14.3% of all routines have all of their subroutines repeated. A high percentage of subroutines in a routine can be found/reused elsewhere. We collected 8,764,971 unique subroutines (with 323,564 unique JDK subroutines) as basic units for code searching/synthesis. We also provide practical implications of our findings to automated tools."},{"id":"Geral10","name":"Automatic clustering of code changes","authors":"Kreutzer, P. and Dotzler, G. and Ring, M. and Eskofier, B.M. and Philippsen, M.","year":2016,"base":["Geral"],"abstract":"Several research tools and projects require groups of similar code changes as input. Examples are recommendation and bug finding tools that can provide valuable information to developers based on such data. With the help of similar code changes they can simplify the application of bug fixes and code changes to multiple locations in a project. But despite their benefit, the practical value of existing tools is limited, as users need to manually specify the input data, i.e., the groups of similar code changes. To overcome this drawback, this paper presents and evaluates two syntactical similarity metrics, one of them is specifically designed to run fast, in combination with two carefully selected and self-tuning clustering algorithms to automatically detect groups of similar code changes. We evaluate the combinations of metrics and clustering algorithms by applying them to several open source projects and also publish the detected groups of similar code changes online as a reference dataset. The automatically detected groups of similar code changes work well when used as input for LASE, a recommendation system for code changes. © 2016 ACM.","doi":"10.1145/2901739.2901749","bibtex":"@inproceedings{nguyen2016large,\n  title={A large-scale study on repetitiveness, containment, and composability of routines in open-source projects},\n  author={Nguyen, Anh Tuan and Nguyen, Hoan Anh and Nguyen, Tien N},\n  booktitle={2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR)},\n  pages={362--373},\n  year={2016},\n  organization={IEEE}\n}\n\n"},{"id":"Geral11","name":"An empirical study on real bug fixes","authors":"Zhong, Hao and Su, Zhendong","year":2015,"base":["Geral"],"booktitle":"Proceedings of the 37th International Conference on Software Engineering-Volume 1","bibtex":"@CONFERENCE{Kreutzer201661,\nauthor={Kreutzer, P. and Dotzler, G. and Ring, M. and Eskofier, B.M. and Philippsen, M.},\ntitle={Automatic clustering of code changes},\njournal={Proceedings - 13th Working Conference on Mining Software Repositories, MSR 2016},\nyear={2016},\npages={61-72},\ndoi={10.1145/2901739.2901749},\nnote={cited By 6},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974623425&doi=10.1145%2f2901739.2901749&partnerID=40&md5=5ceb185eb95befc365edaab3d81d6045},\nabstract={Several research tools and projects require groups of similar code changes as input. Examples are recommendation and bug finding tools that can provide valuable information to developers based on such data. With the help of similar code changes they can simplify the application of bug fixes and code changes to multiple locations in a project. But despite their benefit, the practical value of existing tools is limited, as users need to manually specify the input data, i.e., the groups of similar code changes. To overcome this drawback, this paper presents and evaluates two syntactical similarity metrics, one of them is specifically designed to run fast, in combination with two carefully selected and self-tuning clustering algorithms to automatically detect groups of similar code changes. We evaluate the combinations of metrics and clustering algorithms by applying them to several open source projects and also publish the detected groups of similar code changes online as a reference dataset. The automatically detected groups of similar code changes work well when used as input for LASE, a recommendation system for code changes. © 2016 ACM.},\nkeywords={Codes (symbols);  Open source software;  Open systems, Automatic clustering;  Bug finding tools;  Clustering;  Code changes;  Open source projects;  Research tools;  Similarity metrics;  Software repositories, Clustering algorithms},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n","abstract":"Software bugs can cause significant financial loss and even the loss of human lives. To reduce such loss, developers devote substantial efforts to fixing bugs, which generally requires much expertise and experience. Various approaches have been proposed to aid debugging. An interesting recent research direction is automatic program repair, which achieves promising results, and attracts much academic and industrial attention. However, people also cast doubt on the effectiveness and promise of this direction. A key criticism is to what extent such approaches can fix real bugs. As only research prototypes for these approaches are available, it is infeasible to address the criticism by evaluating them directly on real bugs. Instead, in this paper, we design and develop BugStat, a tool that extracts and analyzes bug fixes. With BugStat's support, we conduct an empirical study on more than 9,000 real-world bug fixes from six popular Java projects. Comparing the nature of manual fixes with automatic program repair, we distill 15 findings, which are further summarized into four insights on the two key ingredients of automatic program repair: fault localization and faulty code fix. In addition, we provide indirect evidence on the size of the search space to fix real bugs and find that bugs may also reside in non-source files. Our results provide useful guidance and insights for improving the state-of-the-art of automatic program repair."},{"id":"Geral12","name":"A closer look at real-world patches","authors":"Liu, K. and Kim, D. and Koyuncu, A. and Li, L. and Bissyande, T.F. and Le Traon, Y.","year":2018,"base":["Geral"],"abstract":"Bug fixing is a time-consuming and tedious task. To reduce the manual efforts in bug fixing, researchers have presented automated approaches to software repair. Unfortunately, recent studies have shown that the state-of-The-Art techniques in automated repair tend to generate patches only for a small number of bugs even with quality issues (e.g., incorrect behavior and nonsensical changes). To improve automated program repair (APR) techniques, the community should deepen its knowledge on repair actions from real-world patches since most of the techniques rely on patches written by human developers. Previous investigations on real-world patches are limited to statement level that is not sufficiently fine-grained to build this knowledge. In this work, we contribute to building this knowledge via a systematic and fine-grained study of 16,450 bug fix commits from seven Java open-source projects. We find that there are opportunities for APR techniques to improve their effectiveness by looking at code elements that have not yet been investigated. We also discuss nine insights into tuning automated repair tools. For example, a small number of statement and expression types are recurrently impacted by real-world patches, and expression-level granularity could reduce search space of finding fix ingredients, where previous studies never explored. © 2018 IEEE.","doi":"10.1109/ICSME.2018.00037","bibtex":"@inproceedings{zhong2015empirical,\n  title={An empirical study on real bug fixes},\n  author={Zhong, Hao and Su, Zhendong},\n  booktitle={Proceedings of the 37th International Conference on Software Engineering-Volume 1},\n  pages={913--923},\n  year={2015},\n  organization={IEEE Press}\n}\n\n"},{"id":"Geral13","name":"An empirical study of multi-entity changes in real bug fixes","authors":"Wang, Y. and Meng, N. and Zhong, H.","year":2018,"base":["Geral"],"abstract":"Prior studies showed that developers applied repeated bug fixes-similar or identical code changes-To multiple locations. According to the observation, researchers built tools to automatically generate candidate patches from the repeated bug-fixing patterns. However, all such research focuses on the recurring change patterns within single methods. We are curious whether there are also repeated bug fixes that change multiple program entities (e.g., classes, methods, and fields); and if so, how we can leverage such recurring change patterns to further help developers fix bugs. In this paper, we present a comprehensive empirical study on multi-entity bug fixes in terms of their frequency, composition, and semantic meanings. Specifically for each bug fix, we first used our approach InterPart to perform static inter-procedural analysis on partial programs (i.e., the old and new versions of changed Java files), and to extract change dependency graphs (CDGs)-graphs that connect multiple changed entities based on their syntactic dependencies. By extracting common subgraphs from the CDGs of different fixes, we identified the recurring change patterns. Our study on Aries, Cassandra, Derby, and Mahout shows that (1) 52-58% of bug fixes involved multi-entity changes; (2) 6 recurring change patterns commonly exist in all projects; and (3) 19-210 entity pairs were repetitively co-changed mainly because the pairs invoked the same methods, accessed the same fields, or contained similar content. These results helped us better understand the gap between the fixes generated by existing automatic program repair (APR) approaches and the real fixes. Our observations will shed light on the follow-up research of automatic program comprehension and modification. © 2018 IEEE.","doi":"10.1109/ICSME.2018.00038","bibtex":"@CONFERENCE{Liu2018275,\nauthor={Liu, K. and Kim, D. and Koyuncu, A. and Li, L. and Bissyande, T.F. and Le Traon, Y.},\ntitle={A closer look at real-world patches},\njournal={Proceedings - 2018 IEEE International Conference on Software Maintenance and Evolution, ICSME 2018},\nyear={2018},\npages={275-286},\ndoi={10.1109/ICSME.2018.00037},\nart_number={8530036},\nnote={cited By 0},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058333823&doi=10.1109%2fICSME.2018.00037&partnerID=40&md5=b591978c95259da9ce8bbfbc17bbb7ec},\nabstract={Bug fixing is a time-consuming and tedious task. To reduce the manual efforts in bug fixing, researchers have presented automated approaches to software repair. Unfortunately, recent studies have shown that the state-of-The-Art techniques in automated repair tend to generate patches only for a small number of bugs even with quality issues (e.g., incorrect behavior and nonsensical changes). To improve automated program repair (APR) techniques, the community should deepen its knowledge on repair actions from real-world patches since most of the techniques rely on patches written by human developers. Previous investigations on real-world patches are limited to statement level that is not sufficiently fine-grained to build this knowledge. In this work, we contribute to building this knowledge via a systematic and fine-grained study of 16,450 bug fix commits from seven Java open-source projects. We find that there are opportunities for APR techniques to improve their effectiveness by looking at code elements that have not yet been investigated. We also discuss nine insights into tuning automated repair tools. For example, a small number of statement and expression types are recurrently impacted by real-world patches, and expression-level granularity could reduce search space of finding fix ingredients, where previous studies never explored. © 2018 IEEE.},\nkeywords={Automation;  Computer software maintenance;  Open systems;  Repair;  Trees (mathematics), Abstract Syntax Trees;  Automated approach;  Expression levels;  Fix pattern;  Open source projects;  Program patch;  Software repair;  State-of-the-art techniques, Open source software},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral14","name":"An Approach to Identifying Error Patterns for Infrastructure as Code","authors":"Chen, W. and Wu, G. and Wei, J.","year":2018,"base":["Geral"],"abstract":"Infrastructure as Code (IaC), which specifies system configurations in an imperative or declarative way, automates environment set up, system deployment and configuration. Despite wide adoption, developing and maintaining high-quality IaC artifacts is still challenging. This paper proposes an approach to handling the fine-grained and frequently occurring IaC code errors. The approach extracts code changes from historical commits and clusters them into groups, by constructing a feature model of code changes and employing an unsupervised machine learning algorithm. It identifies error patterns from the clusters and proposes a set of inspection rules to check the potential IaC code errors. In practice, we take Puppet code artifacts as subject objects and perform a comprehensive study on 14 popular Puppet artifacts. In our experiment, we get 41 cross-artifact error patterns, covering 42% crawled code changes. Based on these patterns, 30 rules are proposed, covering 60% identified error patterns, to proactively check IaC artifacts. The approach would be helpful in improving code quality of IaC artifacts. © 2018 IEEE.","doi":"10.1109/ISSREW.2018.00-19","bibtex":"@CONFERENCE{Wang2018287,\nauthor={Wang, Y. and Meng, N. and Zhong, H.},\ntitle={An empirical study of multi-entity changes in real bug fixes},\njournal={Proceedings - 2018 IEEE International Conference on Software Maintenance and Evolution, ICSME 2018},\nyear={2018},\npages={287-298},\ndoi={10.1109/ICSME.2018.00038},\nart_number={8530037},\nnote={cited By 1},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057470499&doi=10.1109%2fICSME.2018.00038&partnerID=40&md5=3e54a06b41738a4716cb9b51363baf12},\nabstract={Prior studies showed that developers applied repeated bug fixes-similar or identical code changes-To multiple locations. According to the observation, researchers built tools to automatically generate candidate patches from the repeated bug-fixing patterns. However, all such research focuses on the recurring change patterns within single methods. We are curious whether there are also repeated bug fixes that change multiple program entities (e.g., classes, methods, and fields); and if so, how we can leverage such recurring change patterns to further help developers fix bugs. In this paper, we present a comprehensive empirical study on multi-entity bug fixes in terms of their frequency, composition, and semantic meanings. Specifically for each bug fix, we first used our approach InterPart to perform static inter-procedural analysis on partial programs (i.e., the old and new versions of changed Java files), and to extract change dependency graphs (CDGs)-graphs that connect multiple changed entities based on their syntactic dependencies. By extracting common subgraphs from the CDGs of different fixes, we identified the recurring change patterns. Our study on Aries, Cassandra, Derby, and Mahout shows that (1) 52-58% of bug fixes involved multi-entity changes; (2) 6 recurring change patterns commonly exist in all projects; and (3) 19-210 entity pairs were repetitively co-changed mainly because the pairs invoked the same methods, accessed the same fields, or contained similar content. These results helped us better understand the gap between the fixes generated by existing automatic program repair (APR) approaches and the real fixes. Our observations will shed light on the follow-up research of automatic program comprehension and modification. © 2018 IEEE.},\nkeywords={Computer software;  Computer software maintenance;  Semantics, Automatic programs;  Change dependencies;  Change patterns;  Empirical studies;  Inter-procedural analysis;  Multiple program;  Software entities;  Syntactic dependencies, Program debugging},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral15","name":"Public Git Archive: A Big Code Dataset for All","authors":"Markovtsev, Vadim and Long, Waren","year":2018,"base":["Geral"],"abstract":"The number of open source software projects has been growing exponentially. The major online software repository host, GitHub, has accumulated tens of millions of publicly available Git version-controlled repositories. Although the research potential enabled by the available open source code is clearly substantial, no significant large-scale open source code datasets exist. In this paper, we present the Public Git Archive - dataset of 182,014 top-bookmarked Git repositories from GitHub. We describe the novel data retrieval pipeline to reproduce it. We also elaborate on the strategy for performing dataset updates and legal issues. The Public Git Archive occupies 3.0 TB on disk and is an order of magnitude larger than the current source code datasets. The dataset is made available through HTTP and provides the source code of the projects, the related metadata, and development history. The data retrieval pipeline employs an optimized worker queue model and an optimized archive format to efficiently store forked Git repositories, reducing the amount of data to download and persist. Public Git Archive aims to open a myriad of new opportunities for \"Big Code\" research.","booktitle":"Proceedings of the 15th International Conference on Mining Software Repositories","doi":"10.1145/3196398.3196464","bibtex":"@CONFERENCE{Chen2018124,\nauthor={Chen, W. and Wu, G. and Wei, J.},\ntitle={An Approach to Identifying Error Patterns for Infrastructure as Code},\njournal={Proceedings - 29th IEEE International Symposium on Software Reliability Engineering Workshops, ISSREW 2018},\nyear={2018},\npages={124-129},\ndoi={10.1109/ISSREW.2018.00-19},\nart_number={8539175},\nnote={cited By 0},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059848832&doi=10.1109%2fISSREW.2018.00-19&partnerID=40&md5=b3936dbaa3aa4aca753d94d1c60cb8e6},\naffiliation={Institute of Software, Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Sciences, Beijing, China},\nabstract={Infrastructure as Code (IaC), which specifies system configurations in an imperative or declarative way, automates environment set up, system deployment and configuration. Despite wide adoption, developing and maintaining high-quality IaC artifacts is still challenging. This paper proposes an approach to handling the fine-grained and frequently occurring IaC code errors. The approach extracts code changes from historical commits and clusters them into groups, by constructing a feature model of code changes and employing an unsupervised machine learning algorithm. It identifies error patterns from the clusters and proposes a set of inspection rules to check the potential IaC code errors. In practice, we take Puppet code artifacts as subject objects and perform a comprehensive study on 14 popular Puppet artifacts. In our experiment, we get 41 cross-artifact error patterns, covering 42% crawled code changes. Based on these patterns, 30 rules are proposed, covering 60% identified error patterns, to proactively check IaC artifacts. The approach would be helpful in improving code quality of IaC artifacts. © 2018 IEEE.},\nauthor_keywords={Error pattern;  Infrastructure as Code;  Puppet artifact},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral16","name":"Towards reusing hints from past fixes: An exploratory study on thousands of real samples","authors":"Zhong, H. and Meng, N.","year":2018,"base":["Geral"],"abstract":"With the usage of version control systems, many bug fixes have accumulated over the years. Researchers have proposed various automatic program repair (APR) approaches that reuse past fixes to fix new bugs. However, some fundamental questions, such as how new fixes overlap with old fixes, have not been investigated. Intuitively, the overlap between old and new fixes decides how APR approaches can construct new fixes with old ones. Based on this intuition, we systematically designed six overlap metrics, and performed an empirical study on 5,735 bug fixes to investigate the usefulness of past fixes when composing new fixes. For each bug fix, we created delta graphs (i.e., program dependency graphs for code changes), and identified how bug fixes overlap with each other in terms of the content, code structures, and identifier names of fixes. Our results show that if an APR approach knows all code name changes and composes new fixes by fully or partially reusing the content of past fixes, only 2.1% and 3.2% new fixes can be created from single or multiple past fixes in the same project, compared with 0.9% and 1.2% fixes created from past fixes across projects. However, if an APR approach knows all code name changes and composes new fixes by fully or partially reusing the code structures of past fixes, up to 41.3% and 29.7% new fixes can be created. By making the above observations and revealing other ten findings, we investigated the upper bound of reusable past fixes and composable new fixes, exploring the potential of existing and future APR approaches. © 2017, Springer Science+Business Media, LLC, part of Springer Nature.","doi":"10.1007/s10664-017-9584-3","bibtex":"@inproceedings{Markovtsev:2018:PGA:3196398.3196464,\n author = {Markovtsev, Vadim and Long, Waren},\n title = {Public Git Archive: A Big Code Dataset for All},\n booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},\n series = {MSR '18},\n year = {2018},\n isbn = {978-1-4503-5716-6},\n location = {Gothenburg, Sweden},\n pages = {34--37},\n numpages = {4},\n url = {http://doi.acm.org/10.1145/3196398.3196464},\n doi = {10.1145/3196398.3196464},\n acmid = {3196464},\n publisher = {ACM},\n address = {New York, NY, USA},\n keywords = {development history, git, github, open dataset, software repositories, source code},abstract={The number of open source software projects has been growing exponentially. The major online software repository host, GitHub, has accumulated tens of millions of publicly available Git version-controlled repositories. Although the research potential enabled by the available open source code is clearly substantial, no significant large-scale open source code datasets exist. In this paper, we present the Public Git Archive - dataset of 182,014 top-bookmarked Git repositories from GitHub. We describe the novel data retrieval pipeline to reproduce it. We also elaborate on the strategy for performing dataset updates and legal issues. The Public Git Archive occupies 3.0 TB on disk and is an order of magnitude larger than the current source code datasets. The dataset is made available through HTTP and provides the source code of the projects, the related metadata, and development history. The data retrieval pipeline employs an optimized worker queue model and an optimized archive format to efficiently store forked Git repositories, reducing the amount of data to download and persist. Public Git Archive aims to open a myriad of new opportunities for \"Big Code\" research.},\n} \n\n"},{"id":"Geral17","name":"Generating API call rules from version history and stack overflow posts","authors":"Azad, Shams and Rigby, Peter C and Guerrouj, Latifa","year":2017,"base":["Geral"],"bibtex":"@ARTICLE{Zhong20182521,\nauthor={Zhong, H. and Meng, N.},\ntitle={Towards reusing hints from past fixes: An exploratory study on thousands of real samples},\njournal={Empirical Software Engineering},\nyear={2018},\nvolume={23},\nnumber={5},\npages={2521-2549},\ndoi={10.1007/s10664-017-9584-3},\nnote={cited By 2},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038808079&doi=10.1007%2fs10664-017-9584-3&partnerID=40&md5=1d4291d45443069beb65daf6399cf884},\nabstract={With the usage of version control systems, many bug fixes have accumulated over the years. Researchers have proposed various automatic program repair (APR) approaches that reuse past fixes to fix new bugs. However, some fundamental questions, such as how new fixes overlap with old fixes, have not been investigated. Intuitively, the overlap between old and new fixes decides how APR approaches can construct new fixes with old ones. Based on this intuition, we systematically designed six overlap metrics, and performed an empirical study on 5,735 bug fixes to investigate the usefulness of past fixes when composing new fixes. For each bug fix, we created delta graphs (i.e., program dependency graphs for code changes), and identified how bug fixes overlap with each other in terms of the content, code structures, and identifier names of fixes. Our results show that if an APR approach knows all code name changes and composes new fixes by fully or partially reusing the content of past fixes, only 2.1% and 3.2% new fixes can be created from single or multiple past fixes in the same project, compared with 0.9% and 1.2% fixes created from past fixes across projects. However, if an APR approach knows all code name changes and composes new fixes by fully or partially reusing the code structures of past fixes, up to 41.3% and 29.7% new fixes can be created. By making the above observations and revealing other ten findings, we investigated the upper bound of reusable past fixes and composable new fixes, exploring the potential of existing and future APR approaches. © 2017, Springer Science+Business Media, LLC, part of Springer Nature.},\nkeywords={Computer software reusability;  Program debugging, Automatic programs;  Code changes;  Code structure;  Empirical studies;  Exploratory studies;  Program dependency graphs;  Real samples;  Version control system, Codes (symbols)},\ndocument_type={Article},\nsource={Scopus},\n}\n\n","abstract":"Researchers have shown that related functions can be mined from groupings of functions found in the version history of a system. Our first contribution is to expand this approach to a community of applications and set of similar applications. Android developers use a set of application programming interface (API) calls when creating apps. These API calls are used in similar ways across multiple applications. By clustering co-changing API calls used by 230 Android apps across 12k versions, we are able to predict the API calls that individual app developers will use with an average precision of 75% and recall of 22%. When we make predictions from the same category of app, such as Finance, we attain precision and recall of 81% and 28%, respectively.\n\nOur second contribution can be characterized as “programmers who discussed these functions were also interested in these functions.” Informal discussions on Stack Overflow provide a rich source of information about related API calls as developers provide solutions to common problems. By grouping API calls contained in each positively voted answer posts, we are able to create rules that predict the calls that app developers will use in their own apps with an average precision of 66% and recall of 13%.\n\nFor comparison purposes, we developed a baseline by clustering co-changing API calls for each individual app and generated association rules from them. The baseline predicts API calls used by app developers with a precision and recall of 36% and 23%, respectively."},{"id":"Geral18","name":"Redundancy-free analysis of multi-revision software artifacts","authors":"Alexandru, Carol V and Panichella, Sebastiano and Proksch, Sebastian and Gall, Harald C","year":2018,"base":["Geral"],"bibtex":"@article{azad2017generating,\n  title={Generating API call rules from version history and stack overflow posts},\n  author={Azad, Shams and Rigby, Peter C and Guerrouj, Latifa},\n  journal={ACM Transactions on Software Engineering and Methodology (TOSEM)},\n  volume={25},\n  number={4},\n  pages={29},\n  year={2017},\n  publisher={ACM}\n}\n\n","abstract":"Researchers often analyze several revisions of a software project to obtain historical data about its evolution. For example, they statically analyze the source code and monitor the evolution of certain metrics over multiple revisions. The time and resource requirements for running these analyses often make it necessary to limit the number of analyzed revisions, e.g., by only selecting major revisions or by using a coarse-grained sampling strategy, which could remove significant details of the evolution. Most existing analysis techniques are not designed for the analysis of multi-revision artifacts and they treat each revision individually. However, the actual difference between two subsequent revisions is typically very small. Thus, tools tailored for the analysis of multiple revisions should only analyze these differences, thereby preventing re-computation and storage of redundant data, improving scalability and enabling the study of a larger number of revisions. In this work, we propose the Lean Language-Independent Software Analyzer (LISA), a generic framework for representing and analyzing multi-revisioned software artifacts. It employs a redundancy-free, multi-revision representation for artifacts and avoids re-computation by only analyzing changed artifact fragments across thousands of revisions. The evaluation of our approach consists of measuring the effect of each individual technique incorporated, an in-depth study of LISA resource requirements and a large-scale analysis over 7 million program revisions of 4,000 software projects written in four languages. We show that the time and space requirements for multi-revision analyses can be reduced by multiple orders of magnitude, when compared to traditional, sequential approaches."},{"id":"Geral19","name":"Cross-project code clones in GitHub","authors":"Gharehyazie, M. and Ray, B. and Keshani, M. and Zavosht, M.S. and Heydarnoori, A. and Filkov, V.","year":2018,"base":["Geral"],"abstract":"Code reuse has well-known benefits on code quality, coding efficiency, and maintenance. Open Source Software (OSS) programmers gladly share their own code and they happily reuse others’. Social programming platforms like GitHub have normalized code foraging via their common platforms, enabling code search and reuse across different projects. Removing project borders may facilitate more efficient code foraging, and consequently faster programming. But looking for code across projects takes longer and, once found, may be more challenging to tailor to one’s needs. Learning how much code reuse goes on across projects, and identifying emerging patterns in past cross-project search behavior may help future foraging efforts. Our contribution is two fold. First, to understand cross-project code reuse, here we present an in-depth empirical study of cloning in GitHub. Using Deckard, a popular clone finding tool, we identified copies of code fragments across projects, and investigate their prevalence and characteristics using statistical and network science approaches, and with multiple case studies. By triangulating findings from different analysis methods, we find that cross-project cloning is prevalent in GitHub, ranging from cloning few lines of code to whole project repositories. Some of the projects serve as popular sources of clones, and others seem to contain more clones than their fair share. Moreover, we find that ecosystem cloning follows an onion model: most clones come from the same project, then from projects in the same application domain, and finally from projects in different domains. Second, we utilized these results to develop a novel tool named CLONE-HUNTRESS that streamlines finding and tracking code clones in GitHub. The tool is GitHub integrated, built around a user-friendly interface and runs efficiently over a modern database system. We describe the tool and make it publicly available at http://clone-det.ictic.sharif.edu/. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.","doi":"10.1007/s10664-018-9648-z","bibtex":"@article{alexandru2018redundancy,\n  title={Redundancy-free analysis of multi-revision software artifacts},\n  author={Alexandru, Carol V and Panichella, Sebastiano and Proksch, Sebastian and Gall, Harald C},\n  journal={Empirical Software Engineering},\n  pages={1--49},\n  year={2018},\n  publisher={Springer}\n}\n\n"},{"id":"Geral20","name":"Boosting complete-code tool for partial program","authors":"Zhong, Hao and Wang, Xiaoyin","year":2017,"base":["Geral"],"booktitle":"Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering","bibtex":"@ARTICLE{Gharehyazie2018,\nauthor={Gharehyazie, M. and Ray, B. and Keshani, M. and Zavosht, M.S. and Heydarnoori, A. and Filkov, V.},\ntitle={Cross-project code clones in GitHub},\njournal={Empirical Software Engineering},\nyear={2018},\ndoi={10.1007/s10664-018-9648-z},\nnote={cited By 0; Article in Press},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053470784&doi=10.1007%2fs10664-018-9648-z&partnerID=40&md5=27dab325aad12a4b2d61c5f022f30cbf},\nabstract={Code reuse has well-known benefits on code quality, coding efficiency, and maintenance. Open Source Software (OSS) programmers gladly share their own code and they happily reuse others’. Social programming platforms like GitHub have normalized code foraging via their common platforms, enabling code search and reuse across different projects. Removing project borders may facilitate more efficient code foraging, and consequently faster programming. But looking for code across projects takes longer and, once found, may be more challenging to tailor to one’s needs. Learning how much code reuse goes on across projects, and identifying emerging patterns in past cross-project search behavior may help future foraging efforts. Our contribution is two fold. First, to understand cross-project code reuse, here we present an in-depth empirical study of cloning in GitHub. Using Deckard, a popular clone finding tool, we identified copies of code fragments across projects, and investigate their prevalence and characteristics using statistical and network science approaches, and with multiple case studies. By triangulating findings from different analysis methods, we find that cross-project cloning is prevalent in GitHub, ranging from cloning few lines of code to whole project repositories. Some of the projects serve as popular sources of clones, and others seem to contain more clones than their fair share. Moreover, we find that ecosystem cloning follows an onion model: most clones come from the same project, then from projects in the same application domain, and finally from projects in different domains. Second, we utilized these results to develop a novel tool named CLONE-HUNTRESS that streamlines finding and tracking code clones in GitHub. The tool is GitHub integrated, built around a user-friendly interface and runs efficiently over a modern database system. We describe the tool and make it publicly available at http://clone-det.ictic.sharif.edu/. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.},\nkeywords={Cloning;  Codes (symbols);  Computer software reusability;  Open source software, Clone detection;  Coding efficiency;  Deckard;  Different domains;  GitHub;  Multiple-case study;  Social programming;  User friendly interface, Open systems},\ndocument_type={Article in Press},\nsource={Scopus},\n}\n\n","abstract":"To improve software quality, researchers and practitioners have proposed various static tools, for various purposes (e.g., detecting bugs, anomalies, and vulnerabilities). Although many such tools are quite powerful, they typically need complete code where all the code names are known. In many scenarios, researchers have to analyze partial code in bug fixes/reports, tutorials, and forums. Partial code is a subset of complete code, and many code names of partial code may be unknown. As a result, although partial code is often syntactically correct, existing complete-code tools cannot analyze partial code. To automate the analysis on partial code, some tools have been implemented. However, due to various limitations, tools for partial code are limited in both their number and analysis capability. Instead of proposing another tool for partial code analysis, we propose a general approach, called GRAPA, that boosts existing tools for complete code to analyze partial code. Our major insight is that after unknown bindings are resolved, tools for complete code can analyze partial code with minor modifications. In particular, GRAPA locates Java archive files to resolve unknown bindings, and resolves the remaining unknown bindings from resolved bindings. To illustrate GRAPA, we implement a tool that leverages the state-of-the-art tool, WALA, to analyze Java partial code. We thus implemented the first tool that is able to build system dependency graphs for partial code, complementing existing tools for partial code analysis. We conduct an evaluation on 8,198 partial-code commits from four popular open source projects. Our results show that GRAPA fully resolved unknown code names for 98.5% bug fixes, with an accuracy of 96.1% in total. Furthermore, our results show the significance of GRAPA’s internal techniques, which provides insights on how to integrate with more complete-code tools to analyze partial code."},{"id":"Geral21","name":"Tool support for managing clone refactorings to facilitate code review in evolving software","authors":"Chen, Zhiyuan and Mohanavilasam, Maneesh and Kwon, Young-Woo and Song, Myoungkyu","year":2017,"base":["Geral"],"booktitle":"2017 IEEE 41st Annual Computer Software and Applications Conference (COMPSAC)","bibtex":"@inproceedings{zhong2017boosting,\n  title={Boosting complete-code tool for partial program},\n  author={Zhong, Hao and Wang, Xiaoyin},\n  booktitle={Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering},\n  pages={671--681},\n  year={2017},\n  organization={IEEE Press}\n}\n\n","abstract":"Developers often perform copy-and-paste activities. This practice causes the similar code fragment (aka code clones) to be scattered throughout a code base. Refactoring for clone removal is beneficial, preventing clones from having negative effects on software quality, such as hidden bug propagation and unintentional inconsistent changes. However, recent research has provided evidence that factoring out clones is not always to reduce the risk of introducing defects, and it is often difficult or impossible to remove clones using standard refactoring techniques. To investigate which or how clones can be refactored, developers typically spend a significant amount of their time managing individual clone instances or clone groups scattered across a large code base. To address the problem, this paper presents a technique for managing clone refactorings, Pattern-based clone Refactoring Inspection (PRI), using refactoring pattern templates. By matching the refactoring pattern templates against a code base, it summarizes refactoring changes of clones, and detects the clone instances not consistently factored out as potential anomalies. PRI also provides novel visualization user interfaces specifically designed for inspecting clone refactorings. In the evaluation, PRI analyzes clone instances in six open source projects. It identifies clone refactorings with 94.1% accuracy and detects inconsistent refactorings with 98.4% accuracy. Our results show that PRI should help developers effectively inspect evolving clones and correctly apply refactorings to clone groups."},{"id":"Geral22","name":"Graph-based statistical language model for code","authors":"Nguyen, Anh Tuan and Nguyen, Tien N","year":2015,"base":["Geral"],"booktitle":"2015 IEEE/ACM 37th IEEE International Conference on Software Engineering","bibtex":"@inproceedings{chen2017tool,\n  title={Tool support for managing clone refactorings to facilitate code review in evolving software},\n  author={Chen, Zhiyuan and Mohanavilasam, Maneesh and Kwon, Young-Woo and Song, Myoungkyu},\n  booktitle={2017 IEEE 41st Annual Computer Software and Applications Conference (COMPSAC)},\n  volume={1},\n  pages={288--297},\n  year={2017},\n  organization={IEEE}\n}\n\n","abstract":"n-gram statistical language model has been successfully applied to capture programming patterns to support code completion and suggestion. However, the approaches using n-gram face challenges in capturing the patterns at higher levels of abstraction due to the mismatch between the sequence nature in n-grams and the structure nature of syntax and semantics in source code. This paper presents GraLan, a graph-based statistical language model and its application in code suggestion. GraLan can learn from a source code corpus and compute the appearance probabilities of any graphs given the observed (sub)graphs. We use GraLan to develop an API suggestion engine and an AST-based language model, ASTLan. ASTLan supports the suggestion of the next valid syntactic template and the detection of common syntactic templates. Our empirical evaluation on a large corpus of open-source projects has shown that our engine is more accurate in API code suggestion than the state-of-the-art approaches, and in 75% of the cases, it can correctly suggest the API with only five candidates. ASTLan has also high accuracy in suggesting the next syntactic template and is able to detect many useful and common syntactic templates."},{"id":"Geral23","name":"Experience report: how do techniques, programs, and tests impact automated program repair?","authors":"Kong, Xianglong and Zhang, Lingming and Wong, W Eric and Li, Bixin","year":2015,"base":["Geral"],"booktitle":"2015 IEEE 26th International Symposium on Software Reliability Engineering (ISSRE)","bibtex":"@inproceedings{nguyen2015graph,\n  title={Graph-based statistical language model for code},\n  author={Nguyen, Anh Tuan and Nguyen, Tien N},\n  booktitle={2015 IEEE/ACM 37th IEEE International Conference on Software Engineering},\n  volume={1},\n  pages={858--868},\n  year={2015},\n  organization={IEEE}\n}\n\n","abstract":"Automated program repair can save tremendous manual efforts in software debugging. Therefore, a huge body of research efforts have been dedicated to design and implement automated program repair techniques. Among the existing program repair techniques, genetic-programming-based techniques have shown promising results. Recently, researchers found that random-search-based and adaptive program repair techniques can also produce effective results. In this work, we performed an extensive study for four program repair techniques, including genetic-programming-based, random-search-based, brute-force-based and adaptive program repair techniques. Due to the extremely large time cost of the studied techniques, the study was performed on 153 bugs from 9 small to medium sized programs. In the study, we further investigated the impacts of different programs and test suites on effectiveness and efficiency of program repair techniques. We found that techniques that work well with small programs become too costly or ineffective when applied to medium sized programs. We also computed the false positive rates and discussed the ratio of the explored search space to the whole search space for each studied technique. Surprisingly, all the studied techniques except the random-search-based technique are consistent with the 80/20 rule, i.e., about 80% of successful patches are found within the first 20% of search space."},{"id":"Geral24","name":"Toward reusing code changes","authors":"Higo, Y. and Ohtani, A. and Hayashi, S. and Hata, H. and Shinji, K.","year":2015,"base":["Geral"],"abstract":"Existing techniques have succeeded to help developers implement new code. However, they are insufficient to help to change existing code. Previous studies have proposed techniques to support bug fixes but other kinds of code changes such as function enhancements and refactorings are not supported by them. In this paper, we propose a novel system that helps developers change existing code. Unlike existing techniques, our system can support any kinds of code changes if similar code changes occurred in the past. Our research is still on very early stage and we have not have any implementation or any prototype yet. This paper introduces our research purpose, an outline of our system, and how our system is different from existing techniques. © 2015 IEEE.","doi":"10.1109/MSR.2015.43","bibtex":"@inproceedings{kong2015experience,\n  title={Experience report: how do techniques, programs, and tests impact automated program repair?},\n  author={Kong, Xianglong and Zhang, Lingming and Wong, W Eric and Li, Bixin},\n  booktitle={2015 IEEE 26th International Symposium on Software Reliability Engineering (ISSRE)},\n  pages={194--204},\n  year={2015},\n  organization={IEEE}\n}\n\n"},{"id":"Geral25","name":"Some from Here, Some from There: Cross-Project Code Reuse in GitHub","authors":"Gharehyazie, M. and Ray, B. and Filkov, V.","year":2017,"base":["Geral"],"abstract":"Code reuse has well-known benefits on code quality, coding efficiency, and maintenance. Open Source Software (OSS) programmers gladly share their own code and they happily reuse others'. Social programming platforms like GitHub have normalized code foraging via their common platforms, enabling code search and reuse across different projects. Removing project borders may facilitate more efficient code foraging, and consequently faster programming. But looking for code across projects takes longer and, once found, may be more challenging to tailor to one's needs. Learning how much code reuse goes on across projects, and identifying emerging patterns in past cross-project search behavior may help future foraging efforts. To understand cross-project code reuse, here we present an in-depth study of cloning in GitHub. Using Deckard, a clone finding tool, we identified copies of code fragments across projects, and investigate their prevalence and characteristics using statistical and network science approaches, and with multiple case studies. By triangulating findings from different methods, we find that cross-project cloning is prevalent in GitHub, ranging from cloning few lines of code to whole project repositories. Some of the projects serve as popular sources of clones, and others seem to contain more clones than their fair share. Moreover, we find that ecosystem cloning follows an onion model: most clones come from the same project, then from projects in the same application domain, and finally from projects in different domains. Our results show directions for new tools that can facilitate code foraging and sharing within GitHub. © 2017 IEEE.","doi":"10.1109/MSR.2017.15","bibtex":"@CONFERENCE{Higo2015372,\nauthor={Higo, Y. and Ohtani, A. and Hayashi, S. and Hata, H. and Shinji, K.},\ntitle={Toward reusing code changes},\njournal={IEEE International Working Conference on Mining Software Repositories},\nyear={2015},\nvolume={2015-August},\npages={372-376},\ndoi={10.1109/MSR.2015.43},\nart_number={7180097},\nnote={cited By 2},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957095130&doi=10.1109%2fMSR.2015.43&partnerID=40&md5=ed2480f1173bf95555bbddc09bf3f076},\nabstract={Existing techniques have succeeded to help developers implement new code. However, they are insufficient to help to change existing code. Previous studies have proposed techniques to support bug fixes but other kinds of code changes such as function enhancements and refactorings are not supported by them. In this paper, we propose a novel system that helps developers change existing code. Unlike existing techniques, our system can support any kinds of code changes if similar code changes occurred in the past. Our research is still on very early stage and we have not have any implementation or any prototype yet. This paper introduces our research purpose, an outline of our system, and how our system is different from existing techniques. © 2015 IEEE.},\nkeywords={Bug fixes;  Change reuse;  Code changes;  Code clone;  Refactorings;  Research purpose;  Source code analysis, Codes (symbols)},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral26","name":"An empirical study on ranking change recommendations retrieved using code similarity","authors":"Mondal, Manishankar and Roy, Chanchal K and Schneider, Kevin A","year":2016,"base":["Geral"],"booktitle":"2016 IEEE 23rd International Conference on Software Analysis, Evolution, and Reengineering (SANER)","bibtex":"@CONFERENCE{Gharehyazie2017291,\nauthor={Gharehyazie, M. and Ray, B. and Filkov, V.},\ntitle={Some from Here, Some from There: Cross-Project Code Reuse in GitHub},\njournal={IEEE International Working Conference on Mining Software Repositories},\nyear={2017},\npages={291-301},\ndoi={10.1109/MSR.2017.15},\nart_number={7962379},\nnote={cited By 6},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026520340&doi=10.1109%2fMSR.2017.15&partnerID=40&md5=889f5e4fec932c0467a48a34aca0713e},\nabstract={Code reuse has well-known benefits on code quality, coding efficiency, and maintenance. Open Source Software (OSS) programmers gladly share their own code and they happily reuse others'. Social programming platforms like GitHub have normalized code foraging via their common platforms, enabling code search and reuse across different projects. Removing project borders may facilitate more efficient code foraging, and consequently faster programming. But looking for code across projects takes longer and, once found, may be more challenging to tailor to one's needs. Learning how much code reuse goes on across projects, and identifying emerging patterns in past cross-project search behavior may help future foraging efforts. To understand cross-project code reuse, here we present an in-depth study of cloning in GitHub. Using Deckard, a clone finding tool, we identified copies of code fragments across projects, and investigate their prevalence and characteristics using statistical and network science approaches, and with multiple case studies. By triangulating findings from different methods, we find that cross-project cloning is prevalent in GitHub, ranging from cloning few lines of code to whole project repositories. Some of the projects serve as popular sources of clones, and others seem to contain more clones than their fair share. Moreover, we find that ecosystem cloning follows an onion model: most clones come from the same project, then from projects in the same application domain, and finally from projects in different domains. Our results show directions for new tools that can facilitate code foraging and sharing within GitHub. © 2017 IEEE.},\nkeywords={Cloning;  Codes (symbols);  Computer software reusability;  Genetic engineering;  Open source software;  Software engineering, Code reuse;  Coding efficiency;  Different domains;  Emerging patterns;  GitHub;  Multiple-case study;  Search behavior;  Social programming, Open systems},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n","abstract":"Automated program repair can save tremendous manual efforts in software debugging. Therefore, a huge body of research efforts have been dedicated to design and implement automated program repair techniques. Among the existing program repair techniques, genetic-programming-based techniques have shown promising results. Recently, researchers found that random-search-based and adaptive program repair techniques can also produce effective results. In this work, we performed an extensive study for four program repair techniques, including genetic-programming-based, random-search-based, brute-force-based and adaptive program repair techniques. Due to the extremely large time cost of the studied techniques, the study was performed on 153 bugs from 9 small to medium sized programs. In the study, we further investigated the impacts of different programs and test suites on effectiveness and efficiency of program repair techniques. We found that techniques that work well with small programs become too costly or ineffective when applied to medium sized programs. We also computed the false positive rates and discussed the ratio of the explored search space to the whole search space for each studied technique. Surprisingly, all the studied techniques except the random-search-based technique are consistent with the 80/20 rule, i.e., about 80% of successful patches are found within the first 20% of search space."},{"id":"Geral27","name":"Automatically identifying code features for software defect prediction: Using AST N-grams","authors":"Shippey,T. and Bowes,D. and Hall,T.","year":2019,"base":["Geral"],"bibtex":"@inproceedings{mondal2016empirical,\n  title={An empirical study on ranking change recommendations retrieved using code similarity},\n  author={Mondal, Manishankar and Roy, Chanchal K and Schneider, Kevin A},\n  booktitle={2016 IEEE 23rd International Conference on Software Analysis, Evolution, and Reengineering (SANER)},\n  volume={3},\n  pages={44--50},\n  year={2016},\n  organization={IEEE}\n}\n\n","abstract":"Context: Identifying defects in code early is important. A wide range of static code metrics have been evaluated as potential defect indicators. Most of these metrics offer only high level insights and focus on particular pre-selected features of the code. None of the currently used metrics clearly performs best in defect prediction.\n\nObjective: We use Abstract Syntax Tree (AST) n-grams to identify features of defective Java code that improve defect prediction performance.\n\nMethod: Our approach is bottom-up and does not rely on pre-selecting any specific features of code. We use non-parametric testing to determine relationships between AST n-grams and faults in both open source and commercial systems. We build defect prediction models using three machine learning techniques.\n\nResults: We show that AST n-grams are very significantly related to faults in some systems, with very large effect sizes. The occurrence of some frequently occurring AST n-grams in a method can mean that the method is up to three times more likely to contain a fault. AST n-grams can have a large effect on the performance of defect prediction models.\n\nConclusions: We suggest that AST n-grams offer developers a promising approach to identifying potentially defective code."},{"id":"Geral28","name":"The birth, growth, death and rejuvenation of software maintenance communities","authors":"Feng,Q. and Cai,Y. and Kazman,R. and Mo,R.","year":2018,"base":["Geral"],"booktitle":"International Symposium on Empirical Software Engineering and Measurement","bibtex":"@article{\n        author={Shippey,T. and Bowes,D. and Hall,T.},\n        year={2019},\n        title={Automatically identifying code features for software defect prediction: Using AST N-grams},\n        journal={Information and Software Technology},\n        volume={106},\n        pages={142-160},\n        language={English},\n        url={www.scopus.com},\n}\n\n","abstract":"Background: Though much research has been conducted to investigate software maintenance activities, there has been little work charactering maintenance files as a community and exploring the evolution of this community. Aims: The goal of our research is to identify maintenance communities and monitor their evolution-birth, growth, death and rejuvenation. Method: In this paper, we leveraged a social community detection algorithm---clique prelocation method (CPM)---to identify file communities. Then we implemented an algorithm to detect new communities, active communities, inactive communities and reactivated communities by cumulatively detecting and constantly comparing communities in time sequences. Results: Based on our analysis of 14 open-source projects, we found that new communities are mostly caused by bug and improvement issues. An active community can be vigorous, on and off, through the entire life of a system, and so does an inactive community. In addition, an inactive community can be reactivated again, mostly through bug issues. Conclusions: These findings add to our understanding of software maintenance communities and help us identify the most expensive maintenance spots by identifying constantly active communities."},{"id":"Geral29","name":"Clone refactoring inspection by summarizing clone refactorings and detecting inconsistent changes during software evolution","authors":"Chen,Z. and Kwon,Y. -. and Song,M.","year":2018,"base":["Geral"],"bibtex":"@inproceedings{\n        author={Feng,Q. and Cai,Y. and Kazman,R. and Mo,R.},\n        editor={ },\n        year={2018},\n        title={The birth, growth, death and rejuvenation of software maintenance communities},\n        booktitle={International Symposium on Empirical Software Engineering and Measurement},\n        language={English},\n        url={www.scopus.com},\n}\n\n","abstract":"It has been broadly assumed that removing code clones by refactorings would solve the problems of code duplication. Despite recent empirical studies on the benefit of refactorings, contradicting evidence shows that it is often difficult or impossible to remove clones by using standard refactoring techniques. Developers cannot easily determine which clones can be refactored or how they should be maintained scattered throughout a large code base in evolving systems. We propose pattern‐based clone refactoring inspection (PRI), a technique for managing clone refactorings. PRI summarizes refactorings of clones and detects clones that are not consistently refactored. To help developers refactor these anomalies, PRI also visualizes clone evolution and refactorings and fixes refactoring anomalies to prevent the clone group from being left in an inconsistent state. We evaluated PRI on 6 open‐source projects and showed that it identifies clone refactorings with 94.1% accuracy and detects inconsistent refactorings with 98.4% accuracy, tracking clone change histories. In a study with 10 student developers, the participants reported that flexible PRI's summarization and detection features can be valuable for novice developers to learn about refactorings to clones. These results show that PRI should improve developer productivity in inspecting clone refactorings distributed across multiple files in evolving systems."},{"id":"Geral30","name":"Mining repair model for exception-related bug","authors":"Zhong, H. and Mei, H.","year":2018,"base":["Geral"],"abstract":"It has long been a hot research topic to detect and to repair bugs automatically. As a common practice, researchers propose approaches for specific bugs, and their approaches typically are limited in handling the variety among bugs. Recently, researchers start to explore automatic program repair. With predefined repair operators and test cases, test-based repair approaches use search algorithms to generate patches for a bug, until a patch passes all the test cases. To improve the effectiveness to generate patches, Martinez and Monperrus (2013b) proposed an approach that mines repair models from past fixes. Although their approach produces positive results, we argue that it can be feasible to further improve their approach, if we mine repair models for bug categories, instead of all bugs. However, the benefits are still unclear, since existing benchmarks do not classify bugs into categories and existing approaches cannot mine repair models for bug categories. In this paper, we implement a tool, called EXFI, that classifies bugs into categories based on their related exceptions. With its support, we construct a benchmark, in which bug categories are marked. Furthermore, we propose an approach, called MIMO, that mines a repair model for each exception. We compared the general repair model with our mined repair models. Our results show that our mined models are all significantly different from the general model. Outside of the projects where our models are mined, we selected 59 real bugs. For each bug, we used our models and the general model to generate correct repair shapes for these bugs. The results show that for 43 out of 59 bugs, our models found faster a correct shape than the general repair model (Martinez and Monperrus, 2013b), and for 5 bugs, our models were able to find correct shapes that were not found by the compared model. © 2018 Elsevier Inc.","doi":"10.1016/j.jss.2018.03.046","bibtex":"@article{\n        author={Chen,Z. and Kwon,Y. -. and Song,M.},\n        year={2018},\n        title={Clone refactoring inspection by summarizing clone refactorings and detecting inconsistent changes during software evolution},\n        journal={Journal of Software: Evolution and Process},\n        volume={30},\n        number={10},\n        language={English},\n        url={www.scopus.com},\n}\n\n"},{"id":"Geral31","name":"Mining repair model for exception-related bug","authors":"Zhong, H. and Mei, H.","year":2018,"base":["Geral"],"abstract":"It has long been a hot research topic to detect and to repair bugs automatically. As a common practice, researchers propose approaches for specific bugs, and their approaches typically are limited in handling the variety among bugs. Recently, researchers start to explore automatic program repair. With predefined repair operators and test cases, test-based repair approaches use search algorithms to generate patches for a bug, until a patch passes all the test cases. To improve the effectiveness to generate patches, Martinez and Monperrus (2013b) proposed an approach that mines repair models from past fixes. Although their approach produces positive results, we argue that it can be feasible to further improve their approach, if we mine repair models for bug categories, instead of all bugs. However, the benefits are still unclear, since existing benchmarks do not classify bugs into categories and existing approaches cannot mine repair models for bug categories. In this paper, we implement a tool, called EXFI, that classifies bugs into categories based on their related exceptions. With its support, we construct a benchmark, in which bug categories are marked. Furthermore, we propose an approach, called MIMO, that mines a repair model for each exception. We compared the general repair model with our mined repair models. Our results show that our mined models are all significantly different from the general model. Outside of the projects where our models are mined, we selected 59 real bugs. For each bug, we used our models and the general model to generate correct repair shapes for these bugs. The results show that for 43 out of 59 bugs, our models found faster a correct shape than the general repair model (Martinez and Monperrus, 2013b), and for 5 bugs, our models were able to find correct shapes that were not found by the compared model. © 2018 Elsevier Inc.","doi":"10.1016/j.jss.2018.03.046","bibtex":"@ARTICLE{Zhong201816,\nauthor={Zhong, H. and Mei, H.},\ntitle={Mining repair model for exception-related bug},\njournal={Journal of Systems and Software},\nyear={2018},\nvolume={141},\npages={16-31},\ndoi={10.1016/j.jss.2018.03.046},\nnote={cited By 1},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044617398&doi=10.1016%2fj.jss.2018.03.046&partnerID=40&md5=5ff81feb82e40849366596153ec91a55},\nabstract={It has long been a hot research topic to detect and to repair bugs automatically. As a common practice, researchers propose approaches for specific bugs, and their approaches typically are limited in handling the variety among bugs. Recently, researchers start to explore automatic program repair. With predefined repair operators and test cases, test-based repair approaches use search algorithms to generate patches for a bug, until a patch passes all the test cases. To improve the effectiveness to generate patches, Martinez and Monperrus (2013b) proposed an approach that mines repair models from past fixes. Although their approach produces positive results, we argue that it can be feasible to further improve their approach, if we mine repair models for bug categories, instead of all bugs. However, the benefits are still unclear, since existing benchmarks do not classify bugs into categories and existing approaches cannot mine repair models for bug categories. In this paper, we implement a tool, called EXFI, that classifies bugs into categories based on their related exceptions. With its support, we construct a benchmark, in which bug categories are marked. Furthermore, we propose an approach, called MIMO, that mines a repair model for each exception. We compared the general repair model with our mined repair models. Our results show that our mined models are all significantly different from the general model. Outside of the projects where our models are mined, we selected 59 real bugs. For each bug, we used our models and the general model to generate correct repair shapes for these bugs. The results show that for 43 out of 59 bugs, our models found faster a correct shape than the general repair model (Martinez and Monperrus, 2013b), and for 5 bugs, our models were able to find correct shapes that were not found by the compared model. © 2018 Elsevier Inc.},\nkeywords={Benchmarking;  Program debugging, Automatic programs;  Common practices;  Exception-related bugs;  General repair;  Hot research topics;  Repair models;  Repair operator;  Search Algorithms, Repair},\ndocument_type={Article},\nsource={Scopus},\n}\n\n"},{"id":"Geral32","name":"Extracting the source code context to predict import changes using GPES","authors":"Lee, J. and Kim, K. and Lee, Y.-H. and Hong, J.-E. and Seo, Y.-H. and Yang, B.-D. and Jung, W.","year":2017,"base":["Geral"],"abstract":"One of the difficulties developers encounter in maintaining tasks of a large-scale software system is the updating of suitable libraries on time. Developers tend to miss or make mistakes when searching for and choosing libraries during the development process, or there may not be a stable library for the developers to use. We present a novel approach for helping developers modify software easily and on time and avoid software failures. Using a tool previously built by us called GPES, we collected information of projects, such as abstract syntax trees, tokens, software metrics, relations, and evolutions, for our experiments. We analyzed the contexts of source codes in existing projects to predict changes automatically and to recommend suitable libraries for the projects. The collected data show that researchers can reduce the overall cost of data analysis by transforming the extracted data into the required input formats with a simple query-based implementation. Also, we manually evaluated how the extracted contexts are similar to the description and we found that a sufficient number of the words in the contexts is similar and it might help developers grasp the domain of the source codes easily. © 2017 KSII.","doi":"10.3837/tiis.2017.02.035","bibtex":"@ARTICLE{Zhong201816,\nauthor={Zhong, H. and Mei, H.},\ntitle={Mining repair model for exception-related bug},\njournal={Journal of Systems and Software},\nyear={2018},\nvolume={141},\npages={16-31},\ndoi={10.1016/j.jss.2018.03.046},\nnote={cited By 1},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044617398&doi=10.1016%2fj.jss.2018.03.046&partnerID=40&md5=5ff81feb82e40849366596153ec91a55},\nabstract={It has long been a hot research topic to detect and to repair bugs automatically. As a common practice, researchers propose approaches for specific bugs, and their approaches typically are limited in handling the variety among bugs. Recently, researchers start to explore automatic program repair. With predefined repair operators and test cases, test-based repair approaches use search algorithms to generate patches for a bug, until a patch passes all the test cases. To improve the effectiveness to generate patches, Martinez and Monperrus (2013b) proposed an approach that mines repair models from past fixes. Although their approach produces positive results, we argue that it can be feasible to further improve their approach, if we mine repair models for bug categories, instead of all bugs. However, the benefits are still unclear, since existing benchmarks do not classify bugs into categories and existing approaches cannot mine repair models for bug categories. In this paper, we implement a tool, called EXFI, that classifies bugs into categories based on their related exceptions. With its support, we construct a benchmark, in which bug categories are marked. Furthermore, we propose an approach, called MIMO, that mines a repair model for each exception. We compared the general repair model with our mined repair models. Our results show that our mined models are all significantly different from the general model. Outside of the projects where our models are mined, we selected 59 real bugs. For each bug, we used our models and the general model to generate correct repair shapes for these bugs. The results show that for 43 out of 59 bugs, our models found faster a correct shape than the general repair model (Martinez and Monperrus, 2013b), and for 5 bugs, our models were able to find correct shapes that were not found by the compared model. © 2018 Elsevier Inc.},\nkeywords={Benchmarking;  Program debugging, Automatic programs;  Common practices;  Exception-related bugs;  General repair;  Hot research topics;  Repair models;  Repair operator;  Search Algorithms, Repair},\ndocument_type={Article},\nsource={Scopus},\n}\n\n"},{"id":"Geral33","name":"Extracting the source code context to predict import changes using GPES","authors":"Lee, J. and Kim, K. and Lee, Y.-H. and Hong, J.-E. and Seo, Y.-H. and Yang, B.-D. and Jung, W.","year":2017,"base":["Geral"],"abstract":"One of the difficulties developers encounter in maintaining tasks of a large-scale software system is the updating of suitable libraries on time. Developers tend to miss or make mistakes when searching for and choosing libraries during the development process, or there may not be a stable library for the developers to use. We present a novel approach for helping developers modify software easily and on time and avoid software failures. Using a tool previously built by us called GPES, we collected information of projects, such as abstract syntax trees, tokens, software metrics, relations, and evolutions, for our experiments. We analyzed the contexts of source codes in existing projects to predict changes automatically and to recommend suitable libraries for the projects. The collected data show that researchers can reduce the overall cost of data analysis by transforming the extracted data into the required input formats with a simple query-based implementation. Also, we manually evaluated how the extracted contexts are similar to the description and we found that a sufficient number of the words in the contexts is similar and it might help developers grasp the domain of the source codes easily. © 2017 KSII.","doi":"10.3837/tiis.2017.02.035","bibtex":"@ARTICLE{Lee20171234,\nauthor={Lee, J. and Kim, K. and Lee, Y.-H. and Hong, J.-E. and Seo, Y.-H. and Yang, B.-D. and Jung, W.},\ntitle={Extracting the source code context to predict import changes using GPES},\njournal={KSII Transactions on Internet and Information Systems},\nyear={2017},\nvolume={11},\nnumber={2},\npages={1234-1249},\ndoi={10.3837/tiis.2017.02.035},\nnote={cited By 0},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014125055&doi=10.3837%2ftiis.2017.02.035&partnerID=40&md5=9435eb58b415c74fe66e287ad5889ea8},\nabstract={One of the difficulties developers encounter in maintaining tasks of a large-scale software system is the updating of suitable libraries on time. Developers tend to miss or make mistakes when searching for and choosing libraries during the development process, or there may not be a stable library for the developers to use. We present a novel approach for helping developers modify software easily and on time and avoid software failures. Using a tool previously built by us called GPES, we collected information of projects, such as abstract syntax trees, tokens, software metrics, relations, and evolutions, for our experiments. We analyzed the contexts of source codes in existing projects to predict changes automatically and to recommend suitable libraries for the projects. The collected data show that researchers can reduce the overall cost of data analysis by transforming the extracted data into the required input formats with a simple query-based implementation. Also, we manually evaluated how the extracted contexts are similar to the description and we found that a sufficient number of the words in the contexts is similar and it might help developers grasp the domain of the source codes easily. © 2017 KSII.},\nkeywords={Computer programming languages;  Libraries;  Metadata;  Natural language processing systems;  Trees (mathematics), Abstract Syntax Trees;  Context analysis;  Development process;  Large-scale software systems;  Software failure;  Software metrics;  Software repositories;  Source code changes, Codes (symbols)},\ndocument_type={Article},\nsource={Scopus},\n}\n\n"},{"id":"Geral34","name":"Boa: An Enabling Language and Infrastructure for Ultra-Large-Scale MSR Studies","authors":"Dyer, R. and Nguyen, H. and Rajan, H. and Nguyen, T.","year":2015,"base":["Geral"],"abstract":"Mining software repositories (MSR) on a large scale is important for more generalizable research results. Large collections of software artifacts are openly available (e.g., SourceForge has more than 350,000 projects, GitHub has more than 10 million projects, and Google Code has more than 250,000 projects), but capitalizing on this data is extremely difficult. This chapter serves as a reference guide for Boa, a language and infrastructure designed to decrease the barrier to entry for ultra-large-scale MSR studies. Boa consists of a domain-specific language, its compiler, a dataset that contains almost 700,000 open source projects as of this writing, a back end based on MapReduce to effectively analyze this dataset, and a Web-based front end for writing MSR programs. This chapter describes how researchers and software practitioners can start using this resource. © 2015 Elsevier Inc. All rights reserved.","doi":"10.1016/B978-0-12-411519-4.00020-3","bibtex":"@ARTICLE{Lee20171234,\nauthor={Lee, J. and Kim, K. and Lee, Y.-H. and Hong, J.-E. and Seo, Y.-H. and Yang, B.-D. and Jung, W.},\ntitle={Extracting the source code context to predict import changes using GPES},\njournal={KSII Transactions on Internet and Information Systems},\nyear={2017},\nvolume={11},\nnumber={2},\npages={1234-1249},\ndoi={10.3837/tiis.2017.02.035},\nnote={cited By 0},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014125055&doi=10.3837%2ftiis.2017.02.035&partnerID=40&md5=9435eb58b415c74fe66e287ad5889ea8},\nabstract={One of the difficulties developers encounter in maintaining tasks of a large-scale software system is the updating of suitable libraries on time. Developers tend to miss or make mistakes when searching for and choosing libraries during the development process, or there may not be a stable library for the developers to use. We present a novel approach for helping developers modify software easily and on time and avoid software failures. Using a tool previously built by us called GPES, we collected information of projects, such as abstract syntax trees, tokens, software metrics, relations, and evolutions, for our experiments. We analyzed the contexts of source codes in existing projects to predict changes automatically and to recommend suitable libraries for the projects. The collected data show that researchers can reduce the overall cost of data analysis by transforming the extracted data into the required input formats with a simple query-based implementation. Also, we manually evaluated how the extracted contexts are similar to the description and we found that a sufficient number of the words in the contexts is similar and it might help developers grasp the domain of the source codes easily. © 2017 KSII.},\nkeywords={Computer programming languages;  Libraries;  Metadata;  Natural language processing systems;  Trees (mathematics), Abstract Syntax Trees;  Context analysis;  Development process;  Large-scale software systems;  Software failure;  Software metrics;  Software repositories;  Source code changes, Codes (symbols)},\ndocument_type={Article},\nsource={Scopus},\n}\n\n"},{"id":"Geral35","name":"Do the fix ingredients already exist? An empirical inquiry into the redundancy assumptions of program repair approaches","authors":"Martinezy,M. and Weimerz,W. and Monperrusy,M.","year":2014,"base":["Geral"],"booktitle":"36th International Conference on Software Engineering, ICSE Companion 2014 - Proceedings","bibtex":"@BOOK{Dyer2015593,\nauthor={Dyer, R. and Nguyen, H. and Rajan, H. and Nguyen, T.},\ntitle={Boa: An Enabling Language and Infrastructure for Ultra-Large-Scale MSR Studies},\njournal={The Art and Science of Analyzing Software Data},\nyear={2015},\npages={593-621},\ndoi={10.1016/B978-0-12-411519-4.00020-3},\nnote={cited By 0},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944068581&doi=10.1016%2fB978-0-12-411519-4.00020-3&partnerID=40&md5=cb68f450a14c974443b22e2e146af905},\nabstract={Mining software repositories (MSR) on a large scale is important for more generalizable research results. Large collections of software artifacts are openly available (e.g., SourceForge has more than 350,000 projects, GitHub has more than 10 million projects, and Google Code has more than 250,000 projects), but capitalizing on this data is extremely difficult. This chapter serves as a reference guide for Boa, a language and infrastructure designed to decrease the barrier to entry for ultra-large-scale MSR studies. Boa consists of a domain-specific language, its compiler, a dataset that contains almost 700,000 open source projects as of this writing, a back end based on MapReduce to effectively analyze this dataset, and a Web-based front end for writing MSR programs. This chapter describes how researchers and software practitioners can start using this resource. © 2015 Elsevier Inc. All rights reserved.},\nkeywords={Problem oriented languages;  Program compilers, Domain specific languages;  Mining software repositories;  Mining software repository (MSR);  Open source projects;  Software practitioners;  Source-code mining;  Ultra large scale;  Web-based front end, Open source software},\ndocument_type={Book Chapter},\nsource={Scopus},\n}\n\n","abstract":"Much initial research on automatic program repair has focused on experimental results to probe their potential to find patches and reduce development effort. Relatively less effort has been put into understanding the hows and whys of such approaches. For example, a critical assumption of the GenProg technique is that certain bugs can be fixed by copying and re-arranging existing code. In other words, GenProg assumes that the fix ingredients already exist elsewhere in the code. In this paper, we formalize these assumptions around the concept of ``temporal redundancy''. A temporally redundant commit is only composed of what has already existed in previous commits. Our experiments show that a large proportion of commits that add existing code are temporally redundant. This validates the fundamental redundancy assumption of GenProg."},{"id":"Geral36","name":"Graph data management of evolving dependency graphs for multi-versioned codebases","authors":"Goonetilleke, Oshini and Meibusch, David and Barham, Ben","year":2017,"base":["Geral"],"booktitle":"2017 IEEE International Conference on Software Maintenance and Evolution (ICSME)","bibtex":"@inproceedings{\n        author={Martinezy,M. and Weimerz,W. and Monperrusy,M.},\n        editor={ },\n        year={2014},\n        title={Do the fix ingredients already exist? An empirical inquiry into the redundancy assumptions of program repair approaches},\n        booktitle={36th International Conference on Software Engineering, ICSE Companion 2014 - Proceedings},\n        pages={492-495},\n        note={Cited By :25},\n        language={English},\n        url={www.scopus.com},\n}\n\n","abstract":"Frappé is a code comprehension tool developed by Oracle Labs that extracts the code dependencies from a codebase and stores them in a graph database enabling advanced comprehension tasks. In addition to traditional text-based queries, such context-sensitive tools allow developers to express navigational queries of the form Does function X or something it calls write to global variable Y? providing more insight into the underlying codebases. Frappe captures the dependencies based on the most recent snapshot of the codebase.In this work we focus on the challenges associated with the management of multiple source code revisions, and investigate strategies to enable advanced code comprehension when the underlying codebase evolves over time. To find the deltas, we detail how entities can be resolved across versions, and propose a model for representing evolving dependency graphs. Our versioned graphs are built using snapshots of large codebases in the order of 13 million lines of code.We show growth and storage benefits of versioned graphs compared to independently storing individual snapshots. We also demonstrate how existing Frappe queries can be executed on versioned graphs and new queries can retrieve a history of changes in a function for a code review use case."},{"id":"Geral37","name":"Untangling Composite Commits Using Program Slicing","authors":"Muylaert, Ward and De Roover, Coen","year":2018,"base":["Geral"],"booktitle":"2018 IEEE 18th International Working Conference on Source Code Analysis and Manipulation (SCAM)","bibtex":"@inproceedings{goonetilleke2017graph,\n  title={Graph data management of evolving dependency graphs for multi-versioned codebases},\n  author={Goonetilleke, Oshini and Meibusch, David and Barham, Ben},\n  booktitle={2017 IEEE International Conference on Software Maintenance and Evolution (ICSME)},\n  pages={574--583},\n  year={2017},\n  organization={IEEE}\n}\n\n","abstract":"Composite commits are a common mistake in the use of version control software. A composite commit groups many unrelated tasks, rendering the commit difficult for developers to understand, revert, or integrate and for empirical researchers to analyse. We propose an algorithmic foundation for tool support to identify such composite commits. Our algorithm computes both a program dependence graph and the changes to the abstract syntax tree for the files that have been changed in a commit. Our algorithm then groups these fine-grained changes according to the slices through the dependence graph they belong to. To evaluate our technique, we analyse and refine an established dataset of Java commits, the results of which we also make available. We find that our algorithm can determine whether or not a commit is composite. For the majority of commits, this analysis takes but a few seconds. The parts of a commit that our algorithm identifies do not map directly to the commit's tasks. The parts tend to be smaller, but stay within their respective tasks."},{"id":"Geral38","name":"The Android update problem: An empirical study","authors":"Mahmoudi, M. and Nadi, S.","year":2018,"base":["Geral"],"abstract":"Many phone vendors use Android as their underlying OS, but often extend it to add new functionality and to make it compatible with their specific phones. When a new version of Android is released, phone vendors need to merge or re-apply their customizations and changes to the new release. This is a difficult and time-consuming process, which often leads to late adoption of new versions. In this paper, we perform an empirical study to understand the nature of changes that phone vendors make, versus changes made in the original development of Android. By investigating the overlap of different changes, we also determine the possibility of having automated support for merging them. We develop a publicly available tool chain, based on a combination of existing tools, to study such changes and their overlap. As a proxy case study, we analyze the changes in the popular community-based variant of Android, LineageOS, and its corresponding Android versions. We investigate and report the common types of changes that occur in practice. Our findings show that 83% of subsystems modified by LineageOS are also modified in the next release of Android. By taking the nature of overlapping changes into account, we assess the feasibility of having automated tool support to help phone vendors with the Android update problem. Our results show that 56% of the changes in LineageOS have the potential to be safely automated. © 2018 ACM.","doi":"10.1145/3196398.3196434","bibtex":"@inproceedings{muylaert2018untangling,\n  title={Untangling Composite Commits Using Program Slicing},\n  author={Muylaert, Ward and De Roover, Coen},\n  booktitle={2018 IEEE 18th International Working Conference on Source Code Analysis and Manipulation (SCAM)},\n  pages={193--202},\n  year={2018},\n  organization={IEEE}\n}\n\n"},{"id":"Geral39","name":"Querying distilled code changes to extract executable transformations","authors":"Stevens, R. and Molderez, T. and De Roover, C.","year":2019,"base":["Geral"],"abstract":"Change distilling algorithms compute a sequence of fine-grained changes that, when executed in order, transform a given source AST into a given target AST. The resulting change sequences are used in the field of mining software repositories to study source code evolution. Unfortunately, detecting and specifying source code evolutions in such a change sequence is cumbersome. We therefore introduce a tool-supported approach that identifies minimal executable subsequences in a sequence of distilled changes that implement a particular evolution pattern, specified in terms of intermediate states of the AST that undergoes each change. This enables users to describe the effect of multiple changes, irrespective of their execution order, while ensuring that different change sequences that implement the same code evolution are recalled. Correspondingly, our evaluation is two-fold. We show that our approach is able to recall different implementation variants of the same source code evolution in histories of different software projects. We also evaluate the expressiveness and ease-of-use of our approach in a user study. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.","doi":"10.1007/s10664-018-9644-3","bibtex":"@CONFERENCE{Mahmoudi2018220,\nauthor={Mahmoudi, M. and Nadi, S.},\ntitle={The Android update problem: An empirical study},\njournal={Proceedings - International Conference on Software Engineering},\nyear={2018},\npages={220-230},\ndoi={10.1145/3196398.3196434},\nnote={cited By 0},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051663029&doi=10.1145%2f3196398.3196434&partnerID=40&md5=fce9bbf8fef60b369bb445d5cdf3c6cc},\nabstract={Many phone vendors use Android as their underlying OS, but often extend it to add new functionality and to make it compatible with their specific phones. When a new version of Android is released, phone vendors need to merge or re-apply their customizations and changes to the new release. This is a difficult and time-consuming process, which often leads to late adoption of new versions. In this paper, we perform an empirical study to understand the nature of changes that phone vendors make, versus changes made in the original development of Android. By investigating the overlap of different changes, we also determine the possibility of having automated support for merging them. We develop a publicly available tool chain, based on a combination of existing tools, to study such changes and their overlap. As a proxy case study, we analyze the changes in the popular community-based variant of Android, LineageOS, and its corresponding Android versions. We investigate and report the common types of changes that occur in practice. Our findings show that 83% of subsystems modified by LineageOS are also modified in the next release of Android. By taking the nature of overlapping changes into account, we assess the feasibility of having automated tool support to help phone vendors with the Android update problem. Our results show that 56% of the changes in LineageOS have the potential to be safely automated. © 2018 ACM.},\nkeywords={Automation;  Merging;  Telephone sets, Android;  Automated support;  Automated tool support;  Community-based;  Empirical studies;  Software Evolution;  Software merging, Android (operating system)},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral40","name":"Codeflaws: A programming competition benchmark for evaluating automated program repair tools","authors":"Tan, S.H. and Yi, J. and Yulis and Mechtaev, S. and Roychoudhury, A.","year":2017,"base":["Geral"],"abstract":"Several automated program repair techniques have been proposed to reduce the time and effort spent in bug-fixing. While these repair tools are designed to be generic such that they could address many software faults, different repair tools may fix certain types of faults more effectively than other tools. Therefore, it is important to compare more objectively the effectiveness of different repair tools on various fault types. However, existing benchmarks on automated program repairs do not allow thorough investigation of the relationship between fault types and the effectiveness of repair tools. We present Codeflaws, a set of 3902 defects from 7436 programs automatically classified across 39 defect classes (we refer to different types of fault as defect classes derived from the syntactic differences between a buggy program and a patched program). © 2017 IEEE.","doi":"10.1109/ICSE-C.2017.76","bibtex":"@ARTICLE{Stevens2019491,\nauthor={Stevens, R. and Molderez, T. and De Roover, C.},\ntitle={Querying distilled code changes to extract executable transformations},\njournal={Empirical Software Engineering},\nyear={2019},\nvolume={24},\nnumber={1},\npages={491-535},\ndoi={10.1007/s10664-018-9644-3},\nnote={cited By 0},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053271297&doi=10.1007%2fs10664-018-9644-3&partnerID=40&md5=e63534634789c9c6f417132721c3f25a},\nabstract={Change distilling algorithms compute a sequence of fine-grained changes that, when executed in order, transform a given source AST into a given target AST. The resulting change sequences are used in the field of mining software repositories to study source code evolution. Unfortunately, detecting and specifying source code evolutions in such a change sequence is cumbersome. We therefore introduce a tool-supported approach that identifies minimal executable subsequences in a sequence of distilled changes that implement a particular evolution pattern, specified in terms of intermediate states of the AST that undergoes each change. This enables users to describe the effect of multiple changes, irrespective of their execution order, while ensuring that different change sequences that implement the same code evolution are recalled. Correspondingly, our evaluation is two-fold. We show that our approach is able to recall different implementation variants of the same source code evolution in histories of different software projects. We also evaluate the expressiveness and ease-of-use of our approach in a user study. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.},\nkeywords={Computer programming languages, Change distilling;  Change querying;  Evolution patterns;  Fine-grained changes;  Intermediate state;  Logic meta programming;  Mining software repositories;  Software project, Codes (symbols)},\ndocument_type={Article},\nsource={Scopus},\n}\n\n"},{"id":"Geral41","name":"relifix: Automated repair of software regressions","authors":"Tan, Shin Hwei and Roychoudhury, Abhik","year":2015,"base":["Geral"],"booktitle":"Proceedings of the 37th International Conference on Software Engineering-Volume 1","bibtex":"@CONFERENCE{Tan2017180,\nauthor={Tan, S.H. and Yi, J. and Yulis and Mechtaev, S. and Roychoudhury, A.},\ntitle={Codeflaws: A programming competition benchmark for evaluating automated program repair tools},\njournal={Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering Companion, ICSE-C 2017},\nyear={2017},\npages={180-182},\ndoi={10.1109/ICSE-C.2017.76},\nart_number={7965296},\nnote={cited By 12},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026737226&doi=10.1109%2fICSE-C.2017.76&partnerID=40&md5=3b93ea710a3d0cae0fd27eb47a7438a8},\nabstract={Several automated program repair techniques have been proposed to reduce the time and effort spent in bug-fixing. While these repair tools are designed to be generic such that they could address many software faults, different repair tools may fix certain types of faults more effectively than other tools. Therefore, it is important to compare more objectively the effectiveness of different repair tools on various fault types. However, existing benchmarks on automated program repairs do not allow thorough investigation of the relationship between fault types and the effectiveness of repair tools. We present Codeflaws, a set of 3902 defects from 7436 programs automatically classified across 39 defect classes (we refer to different types of fault as defect classes derived from the syntactic differences between a buggy program and a patched program). © 2017 IEEE.},\nkeywords={Automation;  Benchmarking;  Defects;  Repair;  Software engineering, Bug-fixing;  Defect class;  Empirical evaluations;  Fault types;  Repair techniques;  Repair tools;  Software fault, C (programming language)},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n","abstract":"Regression occurs when code changes introduce failures in previously passing test cases. As software evolves, regressions may be introduced. Fixing regression errors manually is time-consuming and error-prone. We propose an approach of automated repair of software regressions, called relifix, that considers the regression repair problem as a problem of reconciling problematic changes. Specifically, we derive a set of code transformations obtained from our manual inspection of 73 real software regressions; this set of code transformations uses syntactical information from changed statements. Regression repair is then accomplished via a search over the code transformation operators -- which operator to apply, and where. Our evaluation compares the repairability of relifix with GenProg on 35 real regression errors. relifix repairs 23 bugs, while GenProg only fixes five bugs. We also measure the likelihood of both approaches in introducing new regressions given a reduced test suite. Our experimental results shows that our approach is less likely to introduce new regressions than GenProg."},{"id":"Geral42","name":"Identifying the exact fixing actions of static rule violation","authors":"Oumarou, Hayatou and Anquetil, Nicolas and Etien, Anne and Ducasse, St{\\'e}phane and Taiwe, Kolyang Dina","year":2015,"base":["Geral"],"booktitle":"2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering (SANER)","bibtex":"@inproceedings{tan2015relifix,\n  title={relifix: Automated repair of software regressions},\n  author={Tan, Shin Hwei and Roychoudhury, Abhik},\n  booktitle={Proceedings of the 37th International Conference on Software Engineering-Volume 1},\n  pages={471--482},\n  year={2015},\n  organization={IEEE Press}\n}\n\n","abstract":"We study good programming practices expressed in rules and detected by static analysis checkers such as PMD or FindBugs. To understand how violations to these rules are corrected and whether this can be automated, we need to identify in the source code where they appear and how they were fixed. This presents some similarities with research on understanding software bugs, their causes, their fixes, and how they could be avoided. The traditional method to identify how a bug or a rule violation were fixed consists in finding the commit that contains this fix and identifying what was changed in this commit. If the commit is small, all the lines changed are ascribed to the fixing of the rule violation or the bug. However, commits are not always atomic, and several fixes and even enhancements can be mixed in a single one (a large commit). In this case, it is impossible to detect which modifications contribute to which fix. In this paper, we are proposing a method that identifies precisely the modifications that are related to the correction of a rule violation. The same method could be applied to bug fixes, providing there is a test illustrating this bug. We validate our solution on a real world system and actual rules."},{"id":"Geral43","name":"How is IF Statement Fixed Through Code Review? A Case Study of Qt Project","authors":"Ueda, Yuki and Ihara, Akinori and Hirao, Toshiki and Ishio, Takashi and Matsumoto, Kenichi","year":2017,"base":["Geral"],"booktitle":"2017 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)","bibtex":"@inproceedings{oumarou2015identifying,\n  title={Identifying the exact fixing actions of static rule violation},\n  author={Oumarou, Hayatou and Anquetil, Nicolas and Etien, Anne and Ducasse, St{\\'e}phane and Taiwe, Kolyang Dina},\n  booktitle={2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering (SANER)},\n  pages={371--379},\n  year={2015},\n  organization={IEEE}\n}\n\n","abstract":"Peer code review is key to ensuring the absence of software defects. To improve the review process, many code review tools provide OSS(Open Source Software) project CI(Continuous Integration) tests that automatically verify code quality issues such as a code convention issues. However, these tests do not cover project policy issues and a code readability issues. In this study, our main goal is to understand how a code owner fixes conditional statement issues based on reviewers feedback. We conduct an empirical study to understand if statement changes after review. Using 69,325 review requests in the Qt project, we analyze changes of the if conditional statements that (1) are requested to be reviewed, and (2) that are implemented after review. As a result, we find the most common symbolic changes are \"(\" and \")\" (35%), \"!\" operator (20%) and \"->\" operator (12%). Also, \"!\" operator is frequently replaced with \"(\" and \")\"."},{"id":"Geral44","name":"Towards the quality improvement of cross-platform mobile applications","authors":"Martinez, Matias and Lecomte, Sylvain","year":2017,"base":["Geral"],"booktitle":"2017 IEEE/ACM 4th International Conference on Mobile Software Engineering and Systems (MOBILESoft)","bibtex":"@inproceedings{ueda2017if,\n  title={How is IF Statement Fixed Through Code Review? A Case Study of Qt Project},\n  author={Ueda, Yuki and Ihara, Akinori and Hirao, Toshiki and Ishio, Takashi and Matsumoto, Kenichi},\n  booktitle={2017 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)},\n  pages={207--213},\n  year={2017},\n  organization={IEEE}\n}\n\n","abstract":"During last ten years, the number of smartphones and mobile applications has been constantly growing. Android, iOS and Windows Mobile are three mobile platforms that cover almost all smartphones in the world in 2017. Developing a mobile app involves first to choose the platforms the app will run, and then to develop specific solutions (i.e., native apps) for each chosen platform using platform-related toolkits such as Android SDK. A cross-platform mobile application is an app that runs on two or more mobile platforms. Several frameworks have been proposed to simplify the development of cross-platform mobile applications and to reduce development and maintenance costs. They are called cross-platform mobile app development frameworks. However, to our knowledge, the life-cycle and the quality of crossplatforms mobile applications built using those frameworks have not been studied in depth. Our main goal is to first study the processes of development and maintenance of mobile applications built using cross-platform mobile app development frameworks, focusing particularly on the bug-fixing activity. Then, we aim at defining tools for automated repairing bugs from cross-platform mobile applications."},{"id":"Geral45","name":"Using temporal and semantic developer-level information to predict maintenance activity profiles","authors":"Levin, Stanislav and Yehudai, Amiram","year":2016,"base":["Geral"],"booktitle":"2016 IEEE International Conference on Software Maintenance and Evolution (ICSME)","bibtex":"@inproceedings{martinez2017towards,\n  title={Towards the quality improvement of cross-platform mobile applications},\n  author={Martinez, Matias and Lecomte, Sylvain},\n  booktitle={2017 IEEE/ACM 4th International Conference on Mobile Software Engineering and Systems (MOBILESoft)},\n  pages={184--188},\n  year={2017},\n  organization={IEEE}\n}\n\n","abstract":"Predictive models for software projects' characteristics have been traditionally based on project-level metrics, employing only little developer-level information, or none at all. In this work we suggest novel metrics that capture temporal and semantic developer-level information collected on a per developer basis. To address the scalability challenges involved in computing these metrics for each and every developer for a large number of source code repositories, we have built a designated repository mining platform. This platform was used to create a metrics dataset based on processing nearly 1000 highly popular open source GitHub repositories, consisting of 147 million LOC, and maintained by 30,000 developers. The computed metrics were then employed to predict the corrective, perfective, and adaptive maintenance activity profiles identified in previous works. Our results show both strong correlation and promising predictive power with R^2 values of 0.83, 0.64, and 0.75. We also show how these results may help project managers to detect anomalies in the development process and to build better development teams. In addition, the platform we built has the potential to yield further predictive models leveraging developer-level metrics at scale."},{"id":"Geral46","name":"How are IF-Conditional Statements Fixed Through Peer CodeReview?","authors":"Ueda, Yuki and Ihara, Akinori and Ishio, Takashi and Hirao, Toshiki and Matsumoto, Kenichi","year":2018,"base":["Geral"],"bibtex":"@inproceedings{levin2016using,\n  title={Using temporal and semantic developer-level information to predict maintenance activity profiles},\n  author={Levin, Stanislav and Yehudai, Amiram},\n  booktitle={2016 IEEE International Conference on Software Maintenance and Evolution (ICSME)},\n  pages={463--467},\n  year={2016},\n  organization={IEEE}\n}\n\n","abstract":"Peer code review is key to ensuring the absence of software defects. To reduce review costs, software developers adopt code convention checking tools that automatically identify maintainability issues in source code. However, these tools do not always address the maintainability issue for a particular project. The goal of this study is to understand how code review fixes conditional statement issues, which are the most frequent changes in software development. We conduct an empirical study to understand if-statement changes through code review. Using review requests in the Qt and OpenStack projects, we analyze changes of the if-conditional statements that are (1) requested to be reviewed, and are (2) revised through code review. We find the most frequently changed symbols are “( )”, “.”, and “!”. We also find project-specific fixing patterns for improving code readability by association rule mining. For example “!” operator is frequently replaced with a function call. These rules are useful for improving a coding convention checker tailored for the projects."},{"id":"Geral47","name":"Mining software repositories for adaptive change commits using machine learning techniques","authors":"Meqdadi, O. and Alhindawi, N. and Alsakran, J. and Saifan, A. and Migdadi, H.","year":2019,"base":["Geral"],"abstract":"Context: Version Control Systems, such as Subversion, are standard repositories that preserve all of the maintenance changes undertaken to source code artifacts during the evolution of a software system. The documented data of the version history are organized as commits; however, these commits do not keep a tag that would identify the purpose of the relevant undertaken change of a commit, thus, there is rarely enough detail to clearly direct developers to the changes associated with a specific type of maintenance. Objective: This work examines the version histories of an open source system to automatically classify version commits into one of two categories, namely adaptive commits and non-adaptive commits. Method: We collected the commits from the version history of three open source systems, then we obtained eight different code change metrics related to, for example, the number of changed statements, methods, hunks, and files. Based on these change metrics, we built a machine learning approach to classify whether a commit was adaptive or not. Results: It is observed that code change metrics can be indicative of adaptive maintenance activities. Also, the classification findings show that the machine learning classifier developed has approximately 75% prediction accuracy within labeled change histories. Conclusion: The proposed method automates the process of examining the version history of a software system and identifies which commits to the system are related to an adaptive maintenance task. The evaluation of the method supports its applicability and efficiency. Although the evaluation of the proposed classifier on unlabeled change histories shows that it is not much better than the random guessing in terms of F-measure, we feel that our classifier would serve as a better basis for developing advanced classifiers that have predictive power of adaptive commits without the need of manual efforts. © 2019 Elsevier B.V.","doi":"10.1016/j.infsof.2019.01.008","bibtex":"@article{ueda2018if,\n  title={How are IF-Conditional Statements Fixed Through Peer CodeReview?},\n  author={Ueda, Yuki and Ihara, Akinori and Ishio, Takashi and Hirao, Toshiki and Matsumoto, Kenichi},\n  journal={IEICE TRANSACTIONS on Information and Systems},\n  volume={101},\n  number={11},\n  pages={2720--2729},\n  year={2018},\n  publisher={The Institute of Electronics, Information and Communication Engineers}\n}\n\n"},{"id":"Geral48","name":"Cldiff: Generating concise linked code differences","authors":"Huang, K. and Zhou, D. and Chen, B. and Wang, Y. and Zhao, W. and Peng, X. and Liu, Y.","year":2018,"base":["Geral"],"abstract":"Analyzing and understanding source code changes is important in a variety of software maintenance tasks. To this end, many code differencing and code change summarization methods have been proposed. For some tasks (e.g. code review and software merging), however, those differencing methods generate too fine-grained a representation of code changes, and those summarization methods generate too coarse-grained a representation of code changes. Moreover, they do not consider the relationships among code changes. Therefore, the generated differences or summaries make it not easy to analyze and understand code changes in some software maintenance tasks. In this paper, we propose a code differencing approach, named ClDiff, to generate concise linked code differences whose granularity is in between the existing code differencing and code change summarization methods. The goal of ClDiff is to generate more easily understandable code differences. ClDiff takes source code files before and after changes as inputs, and consists of three steps. First, it pre-processes the source code files by pruning unchanged declarations from the parsed abstract syntax trees. Second, it generates concise code differences by grouping fine-grained code differences at or above the statement level and describing high-level changes in each group. Third, it links the related concise code differences according to five pre-defined links. Experiments with 12 Java projects (74,387 commits) and a human study with 10 participants have indicated the accuracy, conciseness, performance and usefulness of ClDiff. © 2018 Association for Computing Machinery.","doi":"10.1145/3238147.3238219","bibtex":"@ARTICLE{Meqdadi201980,\nauthor={Meqdadi, O. and Alhindawi, N. and Alsakran, J. and Saifan, A. and Migdadi, H.},\ntitle={Mining software repositories for adaptive change commits using machine learning techniques},\njournal={Information and Software Technology},\nyear={2019},\nvolume={109},\npages={80-91},\ndoi={10.1016/j.infsof.2019.01.008},\nnote={cited By 0},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061095706&doi=10.1016%2fj.infsof.2019.01.008&partnerID=40&md5=62711a485ee5912c0cd26f79e7ff9368},\nabstract={Context: Version Control Systems, such as Subversion, are standard repositories that preserve all of the maintenance changes undertaken to source code artifacts during the evolution of a software system. The documented data of the version history are organized as commits; however, these commits do not keep a tag that would identify the purpose of the relevant undertaken change of a commit, thus, there is rarely enough detail to clearly direct developers to the changes associated with a specific type of maintenance. Objective: This work examines the version histories of an open source system to automatically classify version commits into one of two categories, namely adaptive commits and non-adaptive commits. Method: We collected the commits from the version history of three open source systems, then we obtained eight different code change metrics related to, for example, the number of changed statements, methods, hunks, and files. Based on these change metrics, we built a machine learning approach to classify whether a commit was adaptive or not. Results: It is observed that code change metrics can be indicative of adaptive maintenance activities. Also, the classification findings show that the machine learning classifier developed has approximately 75% prediction accuracy within labeled change histories. Conclusion: The proposed method automates the process of examining the version history of a software system and identifies which commits to the system are related to an adaptive maintenance task. The evaluation of the method supports its applicability and efficiency. Although the evaluation of the proposed classifier on unlabeled change histories shows that it is not much better than the random guessing in terms of F-measure, we feel that our classifier would serve as a better basis for developing advanced classifiers that have predictive power of adaptive commits without the need of manual efforts. © 2019 Elsevier B.V.},\nkeywords={Codes (symbols);  Computer software maintenance;  Learning systems;  Machine learning;  Open source software, Adaptive maintenance;  Code changes;  Commit types;  Machine learning approaches;  Machine learning techniques;  Mining software repositories;  Prediction accuracy;  Version control system, Open systems},\ndocument_type={Article},\nsource={Scopus},\n}\n\n"},{"id":"Geral49","name":"Understanding the similarity of log revision behaviors in open source software","authors":"Niu, X. and Li, S. and Jia, Z. and Zhou, S. and Li, W. and Liao, X.","year":2018,"base":["Geral"],"abstract":"As logging code evolves with bug fixes and feature updates, developers may miss some log revisions due to a lack of general specifications and attention from developers. This makes it more troublesome to achieve good logging practices. In this paper, we try to study log revision behaviors from evolutionary history. Motivated by similar edits of clone codes, we assume there also exist similar log revisions that implicated log revision behaviors. Based on this assumption, we study the similarity of log revision behaviors and answer six research questions. Specifically, we find that 54.14% of log revisions belong to groups of similar log revisions and 64.4% of groups contain log revisions that are missed by developers. We stress the importance of branch statements on learning from similar log revisions since 53.51% of sampled similar log revisions are related to the semantics of branch statements. © 2018 Totem Publisher, Inc. All rights reserved.","doi":"10.23940/ijpe.18.08.p27.18871895","bibtex":"@CONFERENCE{Huang2018679,\nauthor={Huang, K. and Zhou, D. and Chen, B. and Wang, Y. and Zhao, W. and Peng, X. and Liu, Y.},\ntitle={Cldiff: Generating concise linked code differences},\njournal={ASE 2018 - Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},\nyear={2018},\npages={679-690},\ndoi={10.1145/3238147.3238219},\nnote={cited By 0},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056521780&doi=10.1145%2f3238147.3238219&partnerID=40&md5=adbe44e4f4489bfda3a269e289f2618f},\nabstract={Analyzing and understanding source code changes is important in a variety of software maintenance tasks. To this end, many code differencing and code change summarization methods have been proposed. For some tasks (e.g. code review and software merging), however, those differencing methods generate too fine-grained a representation of code changes, and those summarization methods generate too coarse-grained a representation of code changes. Moreover, they do not consider the relationships among code changes. Therefore, the generated differences or summaries make it not easy to analyze and understand code changes in some software maintenance tasks. In this paper, we propose a code differencing approach, named ClDiff, to generate concise linked code differences whose granularity is in between the existing code differencing and code change summarization methods. The goal of ClDiff is to generate more easily understandable code differences. ClDiff takes source code files before and after changes as inputs, and consists of three steps. First, it pre-processes the source code files by pruning unchanged declarations from the parsed abstract syntax trees. Second, it generates concise code differences by grouping fine-grained code differences at or above the statement level and describing high-level changes in each group. Third, it links the related concise code differences according to five pre-defined links. Experiments with 12 Java projects (74,387 commits) and a human study with 10 participants have indicated the accuracy, conciseness, performance and usefulness of ClDiff. © 2018 Association for Computing Machinery.},\nkeywords={Abstracting;  Computer programming languages;  Computer software maintenance;  Software engineering;  Trees (mathematics), Abstract Syntax Trees;  Coarse-grained;  Code differencing;  Program comprehension;  Software merging;  Software-maintenance tasks;  Source code changes;  Source codes, Codes (symbols)},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral50","name":"Shaping program repair space with existing patches and similar code","authors":"Jiang, J. and Xiong, Y. and Zhang, H. and Gao, Q. and Chen, X.","year":2018,"base":["Geral"],"abstract":"Automated program repair (APR) has great potential to reduce bugfixing effort and many approaches have been proposed in recent years. APRs are often treated as a search problem where the search space consists of all the possible patches and the goal is to identify the correct patch in the space. Many techniques take a data-driven approach and analyze data sources such as existing patches and similar source code to help identify the correct patch. However, while existing patches and similar code provide complementary information, existing techniques analyze only a single source and cannot be easily extended to analyze both. In this paper, we propose a novel automatic program repair approach that utilizes both existing patches and similar code. Our approach mines an abstract search space from existing patches and obtains a concrete search space by differencing with similar code snippets. Then we search within the intersection of the two search spaces.We have implemented our approach as a tool called SimFix, and evaluated it on the Defects4J benchmark. Our tool successfully fixed 34 bugs. To our best knowledge, this is the largest number of bugs fixed by a single technology on the Defects4J benchmark. Furthermore, as far as we know, 13 bugs fixed by our approach have never been fixed by the current approaches. © 2018 Association for Computing Machinery.","doi":"10.1145/3213846.3213871","bibtex":"@ARTICLE{Niu20181887,\nauthor={Niu, X. and Li, S. and Jia, Z. and Zhou, S. and Li, W. and Liao, X.},\ntitle={Understanding the similarity of log revision behaviors in open source software},\njournal={International Journal of Performability Engineering},\nyear={2018},\nvolume={14},\nnumber={8},\npages={1887-1895},\ndoi={10.23940/ijpe.18.08.p27.18871895},\nnote={cited By 0},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054725435&doi=10.23940%2fijpe.18.08.p27.18871895&partnerID=40&md5=c13d5d5c8e13deefefd3135f685095a6},\nabstract={As logging code evolves with bug fixes and feature updates, developers may miss some log revisions due to a lack of general specifications and attention from developers. This makes it more troublesome to achieve good logging practices. In this paper, we try to study log revision behaviors from evolutionary history. Motivated by similar edits of clone codes, we assume there also exist similar log revisions that implicated log revision behaviors. Based on this assumption, we study the similarity of log revision behaviors and answer six research questions. Specifically, we find that 54.14% of log revisions belong to groups of similar log revisions and 64.4% of groups contain log revisions that are missed by developers. We stress the importance of branch statements on learning from similar log revisions since 53.51% of sampled similar log revisions are related to the semantics of branch statements. © 2018 Totem Publisher, Inc. All rights reserved.},\nkeywords={Open systems;  Semantics, Bug fixes;  Evolutionary history;  Failure diagnose;  General specification;  Log revision;  Research questions;  Software Evolution, Open source software},\ndocument_type={Article},\nsource={Scopus},\n}\n\n"},{"id":"Geral51","name":"Genetic Improvement of Software: A Comprehensive Survey","authors":"Petke, J. and Haraldsson, S.O. and Harman, M. and Langdon, W.B. and White, D.R. and Woodward, J.R.","year":2018,"base":["Geral"],"abstract":"Genetic improvement (GI) uses automated search to find improved versions of existing software. We present a comprehensive survey of this nascent field of research with a focus on the core papers in the area published between 1995 and 2015. We identified core publications including empirical studies, 96% of which use evolutionary algorithms (genetic programming in particular). Although we can trace the foundations of GI back to the origins of computer science itself, our analysis reveals a significant upsurge in activity since 2012. GI has resulted in dramatic performance improvements for a diverse set of properties such as execution time, energy and memory consumption, as well as results for fixing and extending existing system functionality. Moreover, we present examples of research work that lies on the boundary between GI and other areas, such as program transformation, approximate computing, and software repair, with the intention of encouraging further exchange of ideas between researchers in these fields. © 1997-2012 IEEE.","doi":"10.1109/TEVC.2017.2693219","bibtex":"@CONFERENCE{Jiang2018298,\nauthor={Jiang, J. and Xiong, Y. and Zhang, H. and Gao, Q. and Chen, X.},\ntitle={Shaping program repair space with existing patches and similar code},\njournal={ISSTA 2018 - Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis},\nyear={2018},\npages={298-309},\ndoi={10.1145/3213846.3213871},\nnote={cited By 0},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051477803&doi=10.1145%2f3213846.3213871&partnerID=40&md5=eff0c7ae75fe5569b9e1190f5f2993a7},\nabstract={Automated program repair (APR) has great potential to reduce bugfixing effort and many approaches have been proposed in recent years. APRs are often treated as a search problem where the search space consists of all the possible patches and the goal is to identify the correct patch in the space. Many techniques take a data-driven approach and analyze data sources such as existing patches and similar source code to help identify the correct patch. However, while existing patches and similar code provide complementary information, existing techniques analyze only a single source and cannot be easily extended to analyze both. In this paper, we propose a novel automatic program repair approach that utilizes both existing patches and similar code. Our approach mines an abstract search space from existing patches and obtains a concrete search space by differencing with similar code snippets. Then we search within the intersection of the two search spaces.We have implemented our approach as a tool called SimFix, and evaluated it on the Defects4J benchmark. Our tool successfully fixed 34 bugs. To our best knowledge, this is the largest number of bugs fixed by a single technology on the Defects4J benchmark. Furthermore, as far as we know, 13 bugs fixed by our approach have never been fixed by the current approaches. © 2018 Association for Computing Machinery.},\nkeywords={Codes (symbols);  Defects;  Repair;  Software testing, Automatic programs;  Code adaptation;  Code differencing;  Data-driven approach;  Data-sources;  Search problem;  Search spaces;  Single source, Program debugging},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral52","name":"Logtracker: Learning log revision behaviors proactively from software evolution history","authors":"Li, S. and Niu, X. and Jia, Z. and Wang, J. and He, H. and Wang, T.","year":2018,"base":["Geral"],"abstract":"Log statements are widely used for postmortem debugging. Despite the importance of log messages, it is difficult for developers to establish good logging practices. There are two main reasons for this. First, there are no rigorous specifications or systematic processes to guide the practices of software logging. Second, logging code co-evolves with bug fixes or feature updates. While previous works on log enhancement have successfully focused on the first problem, they are hard to solve the latter. For taking the first step towards solving the second problem, this paper is inspired by code clones and assumes that logging code with similar context is pervasive in software and deserves similar modifications. To verify our assumptions, we conduct an empirical study on eight open-source projects. Based on the observation, we design and implement LogTracker, an automatic tool that can predict log revisions by mining the correlation between logging context and modifications. With an enhanced modeling of logging context, LogTracker is able to guide more intricate log revisions that cannot be covered by existing tools. We evaluate the effectiveness of LogTracker by applying it to the latest version of subject projects. The results of our experiments show that LogTracker can detect 199 instances of log revisions. So far, we have reported 25 of them, and 6 have been accepted. © 2018 ACM.","doi":"10.1145/3196321.3196328","bibtex":"@ARTICLE{Petke2018415,\nauthor={Petke, J. and Haraldsson, S.O. and Harman, M. and Langdon, W.B. and White, D.R. and Woodward, J.R.},\ntitle={Genetic Improvement of Software: A Comprehensive Survey},\njournal={IEEE Transactions on Evolutionary Computation},\nyear={2018},\nvolume={22},\nnumber={3},\npages={415-432},\ndoi={10.1109/TEVC.2017.2693219},\nnote={cited By 15},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026851053&doi=10.1109%2fTEVC.2017.2693219&partnerID=40&md5=cc2ec8d4506e62ebdf30433caef7ed23},\nabstract={Genetic improvement (GI) uses automated search to find improved versions of existing software. We present a comprehensive survey of this nascent field of research with a focus on the core papers in the area published between 1995 and 2015. We identified core publications including empirical studies, 96% of which use evolutionary algorithms (genetic programming in particular). Although we can trace the foundations of GI back to the origins of computer science itself, our analysis reveals a significant upsurge in activity since 2012. GI has resulted in dramatic performance improvements for a diverse set of properties such as execution time, energy and memory consumption, as well as results for fixing and extending existing system functionality. Moreover, we present examples of research work that lies on the boundary between GI and other areas, such as program transformation, approximate computing, and software repair, with the intention of encouraging further exchange of ideas between researchers in these fields. © 1997-2012 IEEE.},\nkeywords={Computer software;  Engineering research;  Genetic algorithms;  History;  Software engineering;  Software testing;  Surveying;  Surveys, Approximate computing;  Automated searches;  Empirical studies;  Existing systems;  Genetic improvements;  Memory consumption;  Performance improvements;  Program transformations, Genetic programming},\ndocument_type={Article},\nsource={Scopus},\n}\n\n"},{"id":"Geral53","name":"Automatic inference of Java-to-swift translation rules for porting mobile applications","authors":"An, K. and Meng, N. and Tilevich, E.","year":2018,"base":["Geral"],"abstract":"A native cross-platform mobile app has multiple platform-specific implementations. Typically, an app is developed for one platform and then ported to the remaining ones. Translating an app from one language (e.g., Java) to another (e.g., Swift) by hand is tedious and error-prone, while automated translators either require manually defined translation rules or focus on translating APIs. To automate the translation of native cross-platform apps, we present J2SINFERER, a novel approach that iteratively infers syntactic transformation rules and API mappings from Java to Swift. Given a software corpus in both languages, J2SLNFERER first identifies the syntactically equivalent code based on braces and string similarity. For each pair of similar code segments, J2SLNFERER then creates syntax trees of both languages, leveraging the minimalist domain knowledge of language correspondence (e.g., operators and markers) to iteratively align syntax tree nodes, and to infer both syntax and API mapping rules. J2SLNFERER represents inferred rules as string templates, stored in a database, to translate code from Java to Swift. We evaluated J2SLNFERER with four applications, using one part of the data to infer translation rules, and the other part to apply the rules. With 76% in-project accuracy and 65% cross-project accuracy, J2SLNFERER outperforms in accuracy j2swift, a state-of-the-art Java-to-Swift conversion tool. As native cross-platform mobile apps grow in popularity, J2SLNFERER can shorten their time to market by automating the tedious and error prone task of source-to-source translation. © 2018 ACM.","doi":"10.1145/3197231.3197240","bibtex":"@CONFERENCE{Li2018178,\nauthor={Li, S. and Niu, X. and Jia, Z. and Wang, J. and He, H. and Wang, T.},\ntitle={Logtracker: Learning log revision behaviors proactively from software evolution history},\njournal={Proceedings - International Conference on Software Engineering},\nyear={2018},\npages={178-188},\ndoi={10.1145/3196321.3196328},\nnote={cited By 0},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051637000&doi=10.1145%2f3196321.3196328&partnerID=40&md5=d0215d35ac181445405a174136daefba},\nabstract={Log statements are widely used for postmortem debugging. Despite the importance of log messages, it is difficult for developers to establish good logging practices. There are two main reasons for this. First, there are no rigorous specifications or systematic processes to guide the practices of software logging. Second, logging code co-evolves with bug fixes or feature updates. While previous works on log enhancement have successfully focused on the first problem, they are hard to solve the latter. For taking the first step towards solving the second problem, this paper is inspired by code clones and assumes that logging code with similar context is pervasive in software and deserves similar modifications. To verify our assumptions, we conduct an empirical study on eight open-source projects. Based on the observation, we design and implement LogTracker, an automatic tool that can predict log revisions by mining the correlation between logging context and modifications. With an enhanced modeling of logging context, LogTracker is able to guide more intricate log revisions that cannot be covered by existing tools. We evaluate the effectiveness of LogTracker by applying it to the latest version of subject projects. The results of our experiments show that LogTracker can detect 199 instances of log revisions. So far, we have reported 25 of them, and 6 have been accepted. © 2018 ACM.},\nkeywords={Codes (symbols);  Computer programming, Automatic tools;  Design and implements;  Empirical studies;  Failure diagnose;  log revision;  Open source projects;  Software Evolution;  Systematic process, Open source software},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral54","name":"Systematic adaptation of dynamically generated source code via domain-specific examples","authors":"Song, M. and Tilevich, E.","year":2018,"base":["Geral"],"abstract":"In modern web-based applications, an increasing amount of source code is generated dynamically at runtime. Web applications commonly execute dynamically generated code (DGC) emitted by third-party, black-box generators, run at remote sites. Web developers often need to adapt DGC before it can be executed: embedded HTML can be vulnerable to cross-site scripting attacks; an API may be incompatible with some browsers; and the program's state created by DGC may not be persisting. Lacking any systematic approaches for adapting DGC, web developers resort to ad-hoc techniques that are unsafe and error-prone. This study presents an approach for adapting DGC systematically that follows the program-transformation-by-example paradigm. The proposed approach provides predefined, domain-specific before/after examples that capture the variability of commonly used adaptations. By approving or rejecting these examples, web developers determine the required adaptation transformations, which are encoded in an adaptation script operating on the generated code's abstract syntax tree. The proposed approach is a suite of practical JavaScript program adaptations and their corresponding before/after examples. The authors have successfully applied the approach to real web applications to adapt third-party generated JavaScript code for security, browser compatibility, and persistence. © The Institution of Engineering and Technology 2017.","doi":"10.1049/iet-sen.2016.0211","bibtex":"@CONFERENCE{An2018180,\nauthor={An, K. and Meng, N. and Tilevich, E.},\ntitle={Automatic inference of Java-to-swift translation rules for porting mobile applications},\njournal={Proceedings - International Conference on Software Engineering},\nyear={2018},\npages={180-190},\ndoi={10.1145/3197231.3197240},\nnote={cited By 0},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051632117&doi=10.1145%2f3197231.3197240&partnerID=40&md5=8a4fa9c806a76d64581564d9b2f10ec4},\nabstract={A native cross-platform mobile app has multiple platform-specific implementations. Typically, an app is developed for one platform and then ported to the remaining ones. Translating an app from one language (e.g., Java) to another (e.g., Swift) by hand is tedious and error-prone, while automated translators either require manually defined translation rules or focus on translating APIs. To automate the translation of native cross-platform apps, we present J2SINFERER, a novel approach that iteratively infers syntactic transformation rules and API mappings from Java to Swift. Given a software corpus in both languages, J2SLNFERER first identifies the syntactically equivalent code based on braces and string similarity. For each pair of similar code segments, J2SLNFERER then creates syntax trees of both languages, leveraging the minimalist domain knowledge of language correspondence (e.g., operators and markers) to iteratively align syntax tree nodes, and to infer both syntax and API mapping rules. J2SLNFERER represents inferred rules as string templates, stored in a database, to translate code from Java to Swift. We evaluated J2SLNFERER with four applications, using one part of the data to infer translation rules, and the other part to apply the rules. With 76% in-project accuracy and 65% cross-project accuracy, J2SLNFERER outperforms in accuracy j2swift, a state-of-the-art Java-to-Swift conversion tool. As native cross-platform mobile apps grow in popularity, J2SLNFERER can shorten their time to market by automating the tedious and error prone task of source-to-source translation. © 2018 ACM.},\nkeywords={Codes (symbols);  Iterative methods;  Knowledge management;  Mapping;  Software engineering;  Syntactics;  Translation (languages);  Trees (mathematics), Automatic inference;  Error prone tasks;  Mobile applications;  Multiple platforms;  Source-to-source translations;  String similarity;  Syntactic transformations;  Translation rules, Java programming language},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral55","name":"On the use of replacement messages in API deprecation: An empirical study","authors":"Brito, G. and Hora, A. and Valente, M.T. and Robbes, R.","year":2018,"base":["Geral"],"abstract":"Libraries are commonly used to support code reuse and increase productivity. As any other system, they evolve over time, and so do their APIs. Consequently, client applications should be updated to benefit from better APIs. To facilitate this task, API elements should always be deprecated with replacement messages. However, in practice, there are evidences that API elements are deprecated without these messages. In this paper, we study questions regarding the adoption of deprecation messages. Our goal is twofold: to measure the real usage of deprecation messages and to investigate whether a tool is needed to recommend them. We assess (i) the frequency of deprecated elements with replacement messages, (ii) the impact of software evolution on this frequency, and (iii) the characteristics of systems that deprecate API elements in a correct way. Our analysis on 622 Java and 229 C# systems shows that: (i) on the median, 66.7% and 77.8% of the API elements are deprecated with replacement messages per project, (ii) there is no major effort to improve deprecation messages, and (iii) systems that deprecated API elements with messages are different in terms of size and community. As a result, we provide the basis for creating a tool to support clients detecting missing deprecation messages. © 2017 Elsevier Inc.","doi":"10.1016/j.jss.2017.12.007","bibtex":"@ARTICLE{Song2018112,\nauthor={Song, M. and Tilevich, E.},\ntitle={Systematic adaptation of dynamically generated source code via domain-specific examples},\njournal={IET Software},\nyear={2018},\nvolume={12},\nnumber={2},\npages={112-119},\ndoi={10.1049/iet-sen.2016.0211},\nnote={cited By 0},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045114635&doi=10.1049%2fiet-sen.2016.0211&partnerID=40&md5=00823ac3428dc19bd8e154d3d7570340},\nabstract={In modern web-based applications, an increasing amount of source code is generated dynamically at runtime. Web applications commonly execute dynamically generated code (DGC) emitted by third-party, black-box generators, run at remote sites. Web developers often need to adapt DGC before it can be executed: embedded HTML can be vulnerable to cross-site scripting attacks; an API may be incompatible with some browsers; and the program's state created by DGC may not be persisting. Lacking any systematic approaches for adapting DGC, web developers resort to ad-hoc techniques that are unsafe and error-prone. This study presents an approach for adapting DGC systematically that follows the program-transformation-by-example paradigm. The proposed approach provides predefined, domain-specific before/after examples that capture the variability of commonly used adaptations. By approving or rejecting these examples, web developers determine the required adaptation transformations, which are encoded in an adaptation script operating on the generated code's abstract syntax tree. The proposed approach is a suite of practical JavaScript program adaptations and their corresponding before/after examples. The authors have successfully applied the approach to real web applications to adapt third-party generated JavaScript code for security, browser compatibility, and persistence. © The Institution of Engineering and Technology 2017.},\nkeywords={High level languages;  Trees (mathematics), Abstract Syntax Trees;  Ad-hoc techniques;  Cross Site Scripting Attacks;  Domain specific;  JavaScript programs;  Program transformations;  WEB application;  Web-based applications, Codes (symbols)},\ndocument_type={Article},\nsource={Scopus},\n}\n\n"},{"id":"Geral56","name":"Automatic software repair: A bibliography","authors":"Monperrus, M.","year":2018,"base":["Geral"],"abstract":"This article presents a survey on automatic software repair. Automatic software repair consists of automatically finding a solution to software bugs without human intervention. This article considers all kinds of repairs. First, it discusses behavioral repair where test suites, contracts, models, and crashing inputs are taken as oracle. Second, it discusses state repair, also known as runtime repair or runtime recovery, with techniques such as checkpoint and restart, reconfiguration, and invariant restoration. The uniqueness of this article is that it spans the research communities that contribute to this body of knowledge: software engineering, dependability, operating systems, programming languages, and security. It provides a novel and structured overview of the diversity of bug oracles and repair operators used in the literature. © 2017 ACM.","doi":"10.1145/3105906","bibtex":"@ARTICLE{Brito2018306,\nauthor={Brito, G. and Hora, A. and Valente, M.T. and Robbes, R.},\ntitle={On the use of replacement messages in API deprecation: An empirical study},\njournal={Journal of Systems and Software},\nyear={2018},\nvolume={137},\npages={306-321},\ndoi={10.1016/j.jss.2017.12.007},\nnote={cited By 3},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042222030&doi=10.1016%2fj.jss.2017.12.007&partnerID=40&md5=9eaf775ca7768ea3eac148d44cb169c6},\nabstract={Libraries are commonly used to support code reuse and increase productivity. As any other system, they evolve over time, and so do their APIs. Consequently, client applications should be updated to benefit from better APIs. To facilitate this task, API elements should always be deprecated with replacement messages. However, in practice, there are evidences that API elements are deprecated without these messages. In this paper, we study questions regarding the adoption of deprecation messages. Our goal is twofold: to measure the real usage of deprecation messages and to investigate whether a tool is needed to recommend them. We assess (i) the frequency of deprecated elements with replacement messages, (ii) the impact of software evolution on this frequency, and (iii) the characteristics of systems that deprecate API elements in a correct way. Our analysis on 622 Java and 229 C# systems shows that: (i) on the median, 66.7% and 77.8% of the API elements are deprecated with replacement messages per project, (ii) there is no major effort to improve deprecation messages, and (iii) systems that deprecated API elements with messages are different in terms of size and community. As a result, we provide the basis for creating a tool to support clients detecting missing deprecation messages. © 2017 Elsevier Inc.},\nkeywords={Software engineering, API deprecation;  Client applications;  Code reuse;  Empirical Software Engineering;  Empirical studies;  Mining software repositories;  Software Evolution, Application programming interfaces (API)},\ndocument_type={Article},\nsource={Scopus},\n}\n\n"},{"id":"Geral57","name":"Exploiting abstract change pattern from code changes","authors":"Huang, Q. and Huang, B. and Fang, Z. and Xiao, M. and Yu, Y.","year":2018,"base":["Geral"],"abstract":"Benefited on the open source software movement, many code search tools are proposed to retrieve source code over the internet. However, the retrieved source code rarely meets user needs perfectly so that it has to be changed manually. This is because the retrieved source code is concretely over-specific to some particular context. To solve this problem, we propose an Abstract Change Pattern Model (ACPM) to ensure the context-specific source code general for various contexts. This model consists of the ACP abstracting and the ACP concretizing algorithms. The former exploits the abstractly context-aware change pattern from the code changes. Based on the change pattern, the latter transforms the context-specific source code into the correct one meeting different user needs. To evaluate ACPM, we extract 7 topics and collect 5-6 code snippets per topic from the Github, while performing 5 different experiments where we explore 2 sensitivity-related rules and use them to raise the accuracy gradually. Our experimental results show that ACPM is feasible and practical with 73.84% accuracy. © 2018 - IOS Press and the authors. All rights reserved.","doi":"10.3233/JIFS-169698","bibtex":"@ARTICLE{Monperrus2018,\nauthor={Monperrus, M.},\ntitle={Automatic software repair: A bibliography},\njournal={ACM Computing Surveys},\nyear={2018},\nvolume={51},\nnumber={1},\ndoi={10.1145/3105906},\nart_number={17},\nnote={cited By 10},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042496901&doi=10.1145%2f3105906&partnerID=40&md5=f6f626f4680e93ec7c8a84c467982798},\nabstract={This article presents a survey on automatic software repair. Automatic software repair consists of automatically finding a solution to software bugs without human intervention. This article considers all kinds of repairs. First, it discusses behavioral repair where test suites, contracts, models, and crashing inputs are taken as oracle. Second, it discusses state repair, also known as runtime repair or runtime recovery, with techniques such as checkpoint and restart, reconfiguration, and invariant restoration. The uniqueness of this article is that it spans the research communities that contribute to this body of knowledge: software engineering, dependability, operating systems, programming languages, and security. It provides a novel and structured overview of the diversity of bug oracles and repair operators used in the literature. © 2017 ACM.},\nkeywords={Program debugging;  Repair;  Software engineering, Body of knowledge;  Checkpoint-and-restart;  Human intervention;  Repair operator;  Research communities;  Self-healing;  Software bug;  Software repair, Restoration},\ndocument_type={Review},\nsource={Scopus},\n}\n\n"},{"id":"Geral58","name":"CMSuggester: Method Change Suggestion to Complement Multi-entity Edits","authors":"Wang, Y. and Meng, N. and Zhong, H.","year":2018,"base":["Geral"],"abstract":"Developers spend significant time and effort in maintaining software. In a maintenance task, developers sometimes have to simultaneously modify multiple program entities (i.e., classes, methods, and fields). We refer to such complex changes as multi-entity edits. It is challenging for developers to apply multi-entity edits consistently and completely. Existing tools provide limited support for such edits, mainly because the co-changed entities usually contain diverse program contexts and experience different changes. This paper introduces CMSuggester, an automatic approach that suggests complementary changes for multi-entity edits. Given a multi-entity edit that adds a field and modifies one or more methods to access the field, CMSuggester suggests other methods to co-change for the new field access. CMSuggester is inspired by our previous empirical study, which reveals that the methods co-changed to access a new field usually commonly access the same set of fields declared in the same class. By extracting the fields accessed by the given changed method(s), CMSuggester identifies and recommends any unchanged method that also accesses those fields. Our evaluation shows that CMSuggester recommends changes for 279 out of 408 suggestion tasks. With the recommended methods, CMSuggester achieves 73% F-score on average, while the widely used tool ROSE achieves 48% F-score. In most cases, as shown in our evaluation results, CMSuggester are useful for developers, since it recommend complete and correct multi-entity edits. © 2018, Springer Nature Switzerland AG.","doi":"10.1007/978-3-030-04272-1_9","bibtex":"@ARTICLE{Huang20181597,\nauthor={Huang, Q. and Huang, B. and Fang, Z. and Xiao, M. and Yu, Y.},\ntitle={Exploiting abstract change pattern from code changes},\njournal={Journal of Intelligent and Fuzzy Systems},\nyear={2018},\nvolume={35},\nnumber={2},\npages={1597-1608},\ndoi={10.3233/JIFS-169698},\nnote={cited By 0},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053287644&doi=10.3233%2fJIFS-169698&partnerID=40&md5=71cae02a72d80ec138bab3761c6344dc},\nabstract={Benefited on the open source software movement, many code search tools are proposed to retrieve source code over the internet. However, the retrieved source code rarely meets user needs perfectly so that it has to be changed manually. This is because the retrieved source code is concretely over-specific to some particular context. To solve this problem, we propose an Abstract Change Pattern Model (ACPM) to ensure the context-specific source code general for various contexts. This model consists of the ACP abstracting and the ACP concretizing algorithms. The former exploits the abstractly context-aware change pattern from the code changes. Based on the change pattern, the latter transforms the context-specific source code into the correct one meeting different user needs. To evaluate ACPM, we extract 7 topics and collect 5-6 code snippets per topic from the Github, while performing 5 different experiments where we explore 2 sensitivity-related rules and use them to raise the accuracy gradually. Our experimental results show that ACPM is feasible and practical with 73.84% accuracy. © 2018 - IOS Press and the authors. All rights reserved.},\nkeywords={Codes (symbols);  Computer programming languages;  Data mining;  Open source software, Change patterns;  Code changes;  Code search;  Context-Aware;  Program transformations;  Source codes;  User need, Open systems},\ndocument_type={Article},\nsource={Scopus},\n}\n\n"},{"id":"Geral59","name":"Leveraging syntax-related code for automated program repair","authors":"Xin, Q. and Reiss, S.P.","year":2017,"base":["Geral"],"abstract":"We present our automated program repair technique ssFix which leverages existing code (from a code database) that is syntax-related to the context of a bug to produce patches for its repair. Given a faulty program and a fault-exposing test suite, ssFix does fault localization to identify suspicious statements that are likely to be faulty. For each such statement, ssFix identifies a code chunk (or target chunk) including the statement and its local context. ssFix works on the target chunk to produce patches. To do so, it first performs syntactic code search to find candidate code chunks that are syntax-related, i.e., structurally similar and conceptually related, to the target chunk from a code database (or codebase) consisting of the local faulty program and an external code repository. ssFix assumes the correct fix to be contained in the candidate chunks, and it leverages each candidate chunk to produce patches for the target chunk. To do so, ssFix translates the candidate chunk by unifying the names used in the candidate chunk with those in the target chunk; matches the chunk components (expressions and statements) between the translated candidate chunk and the target chunk; and produces patches for the target chunk based on the syntactic differences that exist between the matched components and in the unmatched components. ssFix finally validates the patched programs generated against the test suite and reports the first one that passes the test suite. We evaluated ssFix on 357 bugs in the Defects4J bug dataset. Our results show that ssFix successfully repaired 20 bugs with valid patches generated and that it outperformed five other repair techniques for Java. © 2017 IEEE.","doi":"10.1109/ASE.2017.8115676","bibtex":"@ARTICLE{Wang2018137,\nauthor={Wang, Y. and Meng, N. and Zhong, H.},\ntitle={CMSuggester: Method Change Suggestion to Complement Multi-entity Edits},\njournal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},\nyear={2018},\nvolume={11293 LNCS},\npages={137-153},\ndoi={10.1007/978-3-030-04272-1_9},\nnote={cited By 0},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057504932&doi=10.1007%2f978-3-030-04272-1_9&partnerID=40&md5=99ed5873fd4e160cd5bb523e88d2f47d},\nabstract={Developers spend significant time and effort in maintaining software. In a maintenance task, developers sometimes have to simultaneously modify multiple program entities (i.e., classes, methods, and fields). We refer to such complex changes as multi-entity edits. It is challenging for developers to apply multi-entity edits consistently and completely. Existing tools provide limited support for such edits, mainly because the co-changed entities usually contain diverse program contexts and experience different changes. This paper introduces CMSuggester, an automatic approach that suggests complementary changes for multi-entity edits. Given a multi-entity edit that adds a field and modifies one or more methods to access the field, CMSuggester suggests other methods to co-change for the new field access. CMSuggester is inspired by our previous empirical study, which reveals that the methods co-changed to access a new field usually commonly access the same set of fields declared in the same class. By extracting the fields accessed by the given changed method(s), CMSuggester identifies and recommends any unchanged method that also accesses those fields. Our evaluation shows that CMSuggester recommends changes for 279 out of 408 suggestion tasks. With the recommended methods, CMSuggester achieves 73% F-score on average, while the widely used tool ROSE achieves 48% F-score. In most cases, as shown in our evaluation results, CMSuggester are useful for developers, since it recommend complete and correct multi-entity edits. © 2018, Springer Nature Switzerland AG.},\nkeywords={Artificial intelligence;  Computer science;  Computers, Automatic approaches;  Change suggestion;  Common field access;  Empirical studies;  Evaluation results;  Maintenance tasks;  Multi-entity edit;  Multiple program, Software testing},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral60","name":"A code inspection tool by mining recurring changes in evolving software","authors":"Fish, A. and Nguyen, T.L. and Song, M.","year":2017,"base":["Geral"],"abstract":"Mining software repositories have frequently been investigated in recent research. Software modification in repositories are often recurring changes, similar but different changes across multiple locations. It is not easy for developers to find all the relevant locations to maintain such changes, including bug-fixes, new feature addition, and refactorings. Performing recurring changes is tedious and error-prone, resulting in in-consistent and missing updates. To address this problem, we present CloneMap, a clone-aware code inspection tool that helps developers ensure correctness of recurring changes to multiple locations in an evolving software. CloneMap allows developers to specify the old and new versions of a program. It then applies a clone detection technique to (1) mine repositories for extracting differences of recurring changes, (2) visualize the clone evolution, and (3) help developers focus their attention to potential anomalies, such as inconsistent and missing updates. © 2017 IEEE.","doi":"10.1109/SOFTWAREMINING.2017.8100853","bibtex":"@CONFERENCE{Xin2017660,\nauthor={Xin, Q. and Reiss, S.P.},\ntitle={Leveraging syntax-related code for automated program repair},\njournal={ASE 2017 - Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering},\nyear={2017},\npages={660-670},\ndoi={10.1109/ASE.2017.8115676},\nart_number={8115676},\nnote={cited By 7},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041434410&doi=10.1109%2fASE.2017.8115676&partnerID=40&md5=d6fd3994375177e5397fddaadf7c1d01},\nabstract={We present our automated program repair technique ssFix which leverages existing code (from a code database) that is syntax-related to the context of a bug to produce patches for its repair. Given a faulty program and a fault-exposing test suite, ssFix does fault localization to identify suspicious statements that are likely to be faulty. For each such statement, ssFix identifies a code chunk (or target chunk) including the statement and its local context. ssFix works on the target chunk to produce patches. To do so, it first performs syntactic code search to find candidate code chunks that are syntax-related, i.e., structurally similar and conceptually related, to the target chunk from a code database (or codebase) consisting of the local faulty program and an external code repository. ssFix assumes the correct fix to be contained in the candidate chunks, and it leverages each candidate chunk to produce patches for the target chunk. To do so, ssFix translates the candidate chunk by unifying the names used in the candidate chunk with those in the target chunk; matches the chunk components (expressions and statements) between the translated candidate chunk and the target chunk; and produces patches for the target chunk based on the syntactic differences that exist between the matched components and in the unmatched components. ssFix finally validates the patched programs generated against the test suite and reports the first one that passes the test suite. We evaluated ssFix on 357 bugs in the Defects4J bug dataset. Our results show that ssFix successfully repaired 20 bugs with valid patches generated and that it outperformed five other repair techniques for Java. © 2017 IEEE.},\nkeywords={Automation;  Repair;  Software engineering;  Software testing;  Syntactics, Code database;  Code search;  code transfer;  External code;  Fault localization;  Local contexts;  Repair techniques, Codes (symbols)},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral61","name":"A feasibility study of using automated program repair for introductory programming assignments","authors":"Yi, J. and Ahmed, U.Z. and Karkare, A. and Tan, S.H. and Roychoudhury, A.","year":2017,"base":["Geral"],"abstract":"Despite the fact that an intelligent tutoring system for programming (ITSP) has long attracted interest, its widespread use has been hindered by the difficulty of generating personalized feedback automatically. Meanwhile, automated program repair (APR) is an emerging new technology that automatically fixes software bugs, and it has been shown that APR can fix the bugs of large real-world software. In this paper, we study the feasibility of marrying an ITSP and APR. We perform our feasibility study with four stateof- the-art APR tools (GenProg, AE, Angelix, and Prophet), and 661 programs written by the students taking an introductory programming course. We found that when APR tools are used out of the box, only about 30% of the programs in our dataset are repaired. This low repair rate is largely due to the student programs often being significantly incorrect - in contrast, professional software for which APR was successfully applied typically fails only a small portion of tests. To bridge this gap, we adopt in APR a new repair policy akin to the hint generation policy employed in the existing ITSP. This new repair policy admits partial repairs that address part of failing tests, which results in 84% improvement of repair rate. We also performed a user study with 263 novice students and 37 graders, and identified an understudied problem; while the graders seem to gain benefits from repairs, novice students do not seem to know how to effectively make use of generated repairs as hints. © 2017 Association for Computing Machinery.","doi":"10.1145/3106237.3106262","bibtex":"@CONFERENCE{Fish201748,\nauthor={Fish, A. and Nguyen, T.L. and Song, M.},\ntitle={A code inspection tool by mining recurring changes in evolving software},\njournal={SoftwareMining 2017 - Proceedings of the 2017 6th IEEE/ACM International Workshop on Software Mining, co-located with ASE 2017},\nyear={2017},\npages={48-51},\ndoi={10.1109/SOFTWAREMINING.2017.8100853},\nart_number={8100853},\nnote={cited By 0},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040762340&doi=10.1109%2fSOFTWAREMINING.2017.8100853&partnerID=40&md5=4fc7038021b5ef1af07631d5236c03cd},\nabstract={Mining software repositories have frequently been investigated in recent research. Software modification in repositories are often recurring changes, similar but different changes across multiple locations. It is not easy for developers to find all the relevant locations to maintain such changes, including bug-fixes, new feature addition, and refactorings. Performing recurring changes is tedious and error-prone, resulting in in-consistent and missing updates. To address this problem, we present CloneMap, a clone-aware code inspection tool that helps developers ensure correctness of recurring changes to multiple locations in an evolving software. CloneMap allows developers to specify the old and new versions of a program. It then applies a clone detection technique to (1) mine repositories for extracting differences of recurring changes, (2) visualize the clone evolution, and (3) help developers focus their attention to potential anomalies, such as inconsistent and missing updates. © 2017 IEEE.},\nkeywords={Inspection equipment;  Location;  Machine tools, Bug fixes;  Clone detection techniques;  Code inspections;  Error prones;  Mining software repositories;  Recent researches;  Refactorings;  Software modification, Cloning},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral62","name":"Automatic inference of code transforms for patch generation","authors":"Long, F. and Amidon, P. and Rinard, M.","year":2017,"base":["Geral"],"abstract":"We present a new system, Genesis, that processes human patches to automatically infer code transforms for automatic patch generation. We present results that characterize the effectiveness of the Genesis inference algorithms and the complete Genesis patch generation system working with real-world patches and defects collected from 372 Java projects. To the best of our knowledge, Genesis is the first system to automatically infer patch generation transforms or candidate patch search spaces from previous successful patches. © 2017 Association for Computing Machinery.","doi":"10.1145/3106237.3106253","bibtex":"@CONFERENCE{Yi2017740,\nauthor={Yi, J. and Ahmed, U.Z. and Karkare, A. and Tan, S.H. and Roychoudhury, A.},\ntitle={A feasibility study of using automated program repair for introductory programming assignments},\njournal={Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},\nyear={2017},\nvolume={Part F130154},\npages={740-751},\ndoi={10.1145/3106237.3106262},\nnote={cited By 6},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030782216&doi=10.1145%2f3106237.3106262&partnerID=40&md5=12bd9d8ae4955e88d48f136a1f811852},\nabstract={Despite the fact that an intelligent tutoring system for programming (ITSP) has long attracted interest, its widespread use has been hindered by the difficulty of generating personalized feedback automatically. Meanwhile, automated program repair (APR) is an emerging new technology that automatically fixes software bugs, and it has been shown that APR can fix the bugs of large real-world software. In this paper, we study the feasibility of marrying an ITSP and APR. We perform our feasibility study with four stateof- the-art APR tools (GenProg, AE, Angelix, and Prophet), and 661 programs written by the students taking an introductory programming course. We found that when APR tools are used out of the box, only about 30% of the programs in our dataset are repaired. This low repair rate is largely due to the student programs often being significantly incorrect - in contrast, professional software for which APR was successfully applied typically fails only a small portion of tests. To bridge this gap, we adopt in APR a new repair policy akin to the hint generation policy employed in the existing ITSP. This new repair policy admits partial repairs that address part of failing tests, which results in 84% improvement of repair rate. We also performed a user study with 263 novice students and 37 graders, and identified an understudied problem; while the graders seem to gain benefits from repairs, novice students do not seem to know how to effectively make use of generated repairs as hints. © 2017 Association for Computing Machinery.},\nkeywords={Computer aided instruction;  Education computing;  Planning;  Repair;  Software engineering;  Software testing;  Students;  Teaching;  Technology transfer, Feasibility studies;  Intelligent tutoring system;  Introductory programming;  Introductory programming course;  Personalized feedback;  Professional software;  State of the art;  Student project, Program debugging},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral63","name":"An empirical study on using hints from past fixes","authors":"Zhong, H. and Meng, N.","year":2017,"base":["Geral"],"abstract":"With the usage of version control systems, many bugfixes have accumulated over the years. Researchers have proposedvarious approaches that reuse past fixes to fix new bugs. However, some fundamental questions, such as how new bug fixes can beconstructed from old fixes, have not been investigated. When anapproach reuses past fixes to fix a new bug, the new bug fixshould overlap with past fixes in terms of code structures and/orcode names. Based on this intuition, we systematically design sixoverlap metrics, and conduct an empirical study on 5,735 bugfixes to investigate the usefulness of past fixes. For each bug fix, we create delta dependency graphs, and identify how bug fixesoverlap with each other by detecting isomorphic subgraphs. Ourresults show Besides that above two major findings, we haveadditional ten findings, which can deepen the understanding onautomatic program repair. © 2017 IEEE.","doi":"10.1109/ICSE-C.2017.88","bibtex":"@CONFERENCE{Long2017727,\nauthor={Long, F. and Amidon, P. and Rinard, M.},\ntitle={Automatic inference of code transforms for patch generation},\njournal={Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},\nyear={2017},\nvolume={Part F130154},\npages={727-739},\ndoi={10.1145/3106237.3106253},\nnote={cited By 14},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030788844&doi=10.1145%2f3106237.3106253&partnerID=40&md5=5cd20407333b20227c6a027cb3fc15c5},\nabstract={We present a new system, Genesis, that processes human patches to automatically infer code transforms for automatic patch generation. We present results that characterize the effectiveness of the Genesis inference algorithms and the complete Genesis patch generation system working with real-world patches and defects collected from 372 Java projects. To the best of our knowledge, Genesis is the first system to automatically infer patch generation transforms or candidate patch search spaces from previous successful patches. © 2017 Association for Computing Machinery.},\nkeywords={Codes (symbols);  Inference engines;  Software engineering, Automatic inference;  First systems;  Generation systems;  Inference algorithm;  Patch generation;  Real-world;  Search spaces, Automatic programming},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral64","name":"An empirical study of supplementary patches in open source projects","authors":"Park, J. and Kim, M. and Bae, D.-H.","year":2017,"base":["Geral"],"abstract":"Developers occasionally make more than one patch to fix a bug. The related patches sometimes are intentionally separated, but unintended omission errors require supplementary patches. Several change recommendation systems have been suggested based on clone analysis, structural dependency, and historical change coupling in order to reduce or prevent incomplete patches. However, very few studies have examined the reason that incomplete patches occur and how real-world omission errors could be reduced. This paper systematically studies a group of bugs that were fixed more than once in open source projects in order to understand the characteristics of incomplete patches. Our study on Eclipse JDT core, Eclipse SWT, Mozilla, and Equinox p2 showed that a significant portion of the resolved bugs require more than one attempt to fix. Compared to single-fix bugs, the multi-fix bugs did not have a lower quality of bug reports, but more attribute changes (i.e., cc’ed developers or title) were made to the multi-fix bugs than to the single-fix bugs. Multi-fix bugs are more likely to have high severity levels than single-fix bugs. Hence, more developers have participated in discussions about multi-fix bugs compared to single-fix bugs. Multi-fix bugs take more time to resolve than single-fix bugs do. Incomplete patches are longer and more scattered, and they are related to more files than regular patches are. Our manual inspection showed that the causes of incomplete patches were diverse, including missed porting updates, incorrect handling of conditional statements, and incomplete refactoring. Our investigation showed that only 7 % to 17 % of supplementary patches had content similar to their initial patches, which implies that supplementary patch locations cannot be predicted by code clone analysis alone. Furthermore, 16 % to 46 % of supplementary patches were beyond the scope of the immediate structural dependency of their initial patch locations. Historical co-change patterns also showed low precision in predicting supplementary patch locations. Code clones, structural dependencies, and historical co-change analyses predicted different supplementary patch locations, and there was little overlap between them. Combining these analyses did not cover all supplementary patch locations. The present study investigates the characteristics of incomplete patches and multi-fix bugs, which have not been systematically examined in previous research. We reveal that predicting supplementary patch is a difficult problem that existing change recommendation approaches could not cover. New type of approaches should be developed and validated on a supplementary patch data set, which developers failed to make the complete patches at once in practice. © 2016, Springer Science+Business Media New York.","doi":"10.1007/s10664-016-9432-x","bibtex":"@CONFERENCE{Zhong2017144,\nauthor={Zhong, H. and Meng, N.},\ntitle={An empirical study on using hints from past fixes},\njournal={Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering Companion, ICSE-C 2017},\nyear={2017},\npages={144-145},\ndoi={10.1109/ICSE-C.2017.88},\nart_number={7965283},\nnote={cited By 3},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026755401&doi=10.1109%2fICSE-C.2017.88&partnerID=40&md5=e56812c9b9192ba1782ebbd39b3064e8},\nabstract={With the usage of version control systems, many bugfixes have accumulated over the years. Researchers have proposedvarious approaches that reuse past fixes to fix new bugs. However, some fundamental questions, such as how new bug fixes can beconstructed from old fixes, have not been investigated. When anapproach reuses past fixes to fix a new bug, the new bug fixshould overlap with past fixes in terms of code structures and/orcode names. Based on this intuition, we systematically design sixoverlap metrics, and conduct an empirical study on 5,735 bugfixes to investigate the usefulness of past fixes. For each bug fix, we create delta dependency graphs, and identify how bug fixesoverlap with each other by detecting isomorphic subgraphs. Ourresults show Besides that above two major findings, we haveadditional ten findings, which can deepen the understanding onautomatic program repair. © 2017 IEEE.},\nkeywords={Costs;  Repair;  Software engineering, Automatic programs;  Bug fixes;  Code structure;  Dependency graphs;  Empirical studies;  Isomorphic subgraphs;  Version control system, C (programming language)},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral65","name":"SrcType: A tool for efficient static type resolution","authors":"Newman, C.D. and Maletic, J.I. and Collard, M.L.","year":2017,"base":["Geral"],"abstract":"An efficient, static type resolution tool is presented. The tool is implemented on top of srcML; an XML representation of source code and abstract syntax. The approach computes the type of every identifier (i.e., function names and variable names) within the provided body of code. The result is a dictionary that can be used to lookup the type of each name. Type information includes metadata such as constness, class membership, aliasing, line number, file, and namespace. The approach is highly scalable and can generate a dictionary for Linux (13 MLOC) in less than 7 minutes. The tool is open source under a GPL license and available for download at srcML.org. © 2016 IEEE.","doi":"10.1109/ICSME.2016.38","bibtex":"@ARTICLE{Park2017436,\nauthor={Park, J. and Kim, M. and Bae, D.-H.},\ntitle={An empirical study of supplementary patches in open source projects},\njournal={Empirical Software Engineering},\nyear={2017},\nvolume={22},\nnumber={1},\npages={436-473},\ndoi={10.1007/s10664-016-9432-x},\nnote={cited By 4},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966430205&doi=10.1007%2fs10664-016-9432-x&partnerID=40&md5=6acc9bac416ceeb6c49a59d2f8ad4733},\nabstract={Developers occasionally make more than one patch to fix a bug. The related patches sometimes are intentionally separated, but unintended omission errors require supplementary patches. Several change recommendation systems have been suggested based on clone analysis, structural dependency, and historical change coupling in order to reduce or prevent incomplete patches. However, very few studies have examined the reason that incomplete patches occur and how real-world omission errors could be reduced. This paper systematically studies a group of bugs that were fixed more than once in open source projects in order to understand the characteristics of incomplete patches. Our study on Eclipse JDT core, Eclipse SWT, Mozilla, and Equinox p2 showed that a significant portion of the resolved bugs require more than one attempt to fix. Compared to single-fix bugs, the multi-fix bugs did not have a lower quality of bug reports, but more attribute changes (i.e., cc’ed developers or title) were made to the multi-fix bugs than to the single-fix bugs. Multi-fix bugs are more likely to have high severity levels than single-fix bugs. Hence, more developers have participated in discussions about multi-fix bugs compared to single-fix bugs. Multi-fix bugs take more time to resolve than single-fix bugs do. Incomplete patches are longer and more scattered, and they are related to more files than regular patches are. Our manual inspection showed that the causes of incomplete patches were diverse, including missed porting updates, incorrect handling of conditional statements, and incomplete refactoring. Our investigation showed that only 7 % to 17 % of supplementary patches had content similar to their initial patches, which implies that supplementary patch locations cannot be predicted by code clone analysis alone. Furthermore, 16 % to 46 % of supplementary patches were beyond the scope of the immediate structural dependency of their initial patch locations. Historical co-change patterns also showed low precision in predicting supplementary patch locations. Code clones, structural dependencies, and historical co-change analyses predicted different supplementary patch locations, and there was little overlap between them. Combining these analyses did not cover all supplementary patch locations. The present study investigates the characteristics of incomplete patches and multi-fix bugs, which have not been systematically examined in previous research. We reveal that predicting supplementary patch is a difficult problem that existing change recommendation approaches could not cover. New type of approaches should be developed and validated on a supplementary patch data set, which developers failed to make the complete patches at once in practice. © 2016, Springer Science+Business Media New York.},\nkeywords={Cloning;  Location;  Open source software;  Open systems;  Program debugging, Bug fixes;  Change patterns;  Empirical studies;  Historical changes;  Manual inspection;  Open source projects;  Patches;  Software Evolution, Costs},\ndocument_type={Article},\nsource={Scopus},\n}\n\n"},{"id":"Geral66","name":"AutoQuery: automatic construction of dependency queries for code search","authors":"Wang, S. and Lo, D. and Jiang, L.","year":2016,"base":["Geral"],"abstract":"Many code search techniques have been proposed to return relevant code for a user query expressed as textual descriptions. However, source code is not mere text. It contains dependency relations among various program elements. To leverage these dependencies for more accurate code search results, techniques have been proposed to allow user queries to be expressed as control and data dependency relationships among program elements. Although such techniques have been shown to be effective for finding relevant code, it remains a question whether appropriate queries can be generated by average users. In this work, we address this concern by proposing a technique, AutoQuery, that can automatically construct dependency queries from a set of code snippets. We realize AutoQuery by the following major steps: firstly, code snippets (that are not necessarily compilable) are converted into program dependence graphs (PDGs); secondly, a new graph mining solution is built to return common structures in the PDGs; thirdly, the common structures are converted to dependency queries, which are used to retrieve results by using a dependence-based code search technique. We have evaluated AutoQuery on real systems with 47 different code search tasks. The results show that the automatically constructed dependency queries retrieve relevant code with a precision, recall, and F-measure of 68.4, 72.1, and 70.2 %, respectively. We have also performed a user study to compare the effectiveness of AutoQuery with that of human generated queries. The results show that queries constructed by AutoQuery on average help to retrieve code fragments with comparable F-measures to those retrieved by human constructed queries. © 2014, Springer Science+Business Media New York.","doi":"10.1007/s10515-014-0170-2","bibtex":"@CONFERENCE{Newman2017604,\nauthor={Newman, C.D. and Maletic, J.I. and Collard, M.L.},\ntitle={SrcType: A tool for efficient static type resolution},\njournal={Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016},\nyear={2017},\npages={604-606},\ndoi={10.1109/ICSME.2016.38},\nart_number={7816517},\nnote={cited By 0},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013135300&doi=10.1109%2fICSME.2016.38&partnerID=40&md5=1b9e6a4cc24aa6aa56c4bbb8ef842210},\nabstract={An efficient, static type resolution tool is presented. The tool is implemented on top of srcML; an XML representation of source code and abstract syntax. The approach computes the type of every identifier (i.e., function names and variable names) within the provided body of code. The result is a dictionary that can be used to lookup the type of each name. Type information includes metadata such as constness, class membership, aliasing, line number, file, and namespace. The approach is highly scalable and can generate a dictionary for Linux (13 MLOC) in less than 7 minutes. The tool is open source under a GPL license and available for download at srcML.org. © 2016 IEEE.},\nkeywords={Computer operating systems;  Computer software maintenance;  Static analysis, Abstract syntax;  Aliasing;  Namespaces;  Open sources;  Source codes;  SrcML;  Type information;  XML representation, Open source software},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral67","name":"Rule-directed code clone synchronization","authors":"Cheng, X. and Zhong, H. and Chen, Y. and Hu, Z. and Zhao, J.","year":2016,"base":["Geral"],"abstract":"Code clones are prevalent in software systems due to many factors in software development. Detecting code clones and managing consistency between them along code evolution can be very useful for reducing clone-related bugs and maintenance costs. Despite some early attempts at detecting code clones and managing the consistency between them, the state-of-the-art tool can only handle simple code clones whose structures are identical or quite similar. However, existing empirical studies show that clones can have quite different structures with their evolution, which can easily go beyond the capability of the state-of-the-art tool. In this paper, we propose CCSync, a novel, rule-directed approach, which paves the structure differences between the code clones and synchronizes them even when code clones become quite different in their structures. The key steps of this approach are, given two code clones, to (1) extract a synchronization rule from the relationship between the clones, and (2) once one code fragment is updated, propagate the modifications to the other following the synchronization rule. We have implemented a tool for CCSync and evaluated its effectiveness on five Java projects. Our results shows that there are many code clones suitable for synchronization, and our tool achieves precisions of up to 92% and recalls of up to 84%. In particular, more than 76% of our generated revisions are identical with manual revisions. © 2016 IEEE.","doi":"10.1109/ICPC.2016.7503722","bibtex":"@ARTICLE{Wang2016393,\nauthor={Wang, S. and Lo, D. and Jiang, L.},\ntitle={AutoQuery: automatic construction of dependency queries for code search},\njournal={Automated Software Engineering},\nyear={2016},\nvolume={23},\nnumber={3},\npages={393-425},\ndoi={10.1007/s10515-014-0170-2},\nnote={cited By 2},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908519669&doi=10.1007%2fs10515-014-0170-2&partnerID=40&md5=63c09f676ed6bd7b5d7824c48db980c3},\nabstract={Many code search techniques have been proposed to return relevant code for a user query expressed as textual descriptions. However, source code is not mere text. It contains dependency relations among various program elements. To leverage these dependencies for more accurate code search results, techniques have been proposed to allow user queries to be expressed as control and data dependency relationships among program elements. Although such techniques have been shown to be effective for finding relevant code, it remains a question whether appropriate queries can be generated by average users. In this work, we address this concern by proposing a technique, AutoQuery, that can automatically construct dependency queries from a set of code snippets. We realize AutoQuery by the following major steps: firstly, code snippets (that are not necessarily compilable) are converted into program dependence graphs (PDGs); secondly, a new graph mining solution is built to return common structures in the PDGs; thirdly, the common structures are converted to dependency queries, which are used to retrieve results by using a dependence-based code search technique. We have evaluated AutoQuery on real systems with 47 different code search tasks. The results show that the automatically constructed dependency queries retrieve relevant code with a precision, recall, and F-measure of 68.4, 72.1, and 70.2 %, respectively. We have also performed a user study to compare the effectiveness of AutoQuery with that of human generated queries. The results show that queries constructed by AutoQuery on average help to retrieve code fragments with comparable F-measures to those retrieved by human constructed queries. © 2014, Springer Science+Business Media New York.},\nkeywords={Codes (symbols);  Structures (built objects), Automatic construction;  Code search;  Dependency query;  Dependency relation;  Graph mining;  Program dependence graph;  Query construction;  Textual description, Query processing},\ndocument_type={Article},\nsource={Scopus},\n}\n\n"},{"id":"Geral68","name":"Prodirect manipulation: Bidirectional programming for the masses","authors":"Chugh, R.","year":2016,"base":["Geral"],"abstract":"Software interfaces today generally fall at either end of a spectrum. On one end are programmable systems, which allow expert users (i.e. programmers) to write software artifacts that describe complex abstractions, but programs are disconnected from their eventual output. On the other end are domain-specific graphical user interfaces (GUIs), which allow end users (i.e. non-programmers) to easily create varied content but present insurmountable walls when a desired feature is not built-in. Both programmatic and direct manipulation have distinct strengths, but users must typically choose one over the other or use some ad-hoc combination of systems. Our goal, put simply, is to bridge this divide. We envision novel software systems that tightly couple programmatic and direct manipulation - - a combination we dub prodirect manipulation - - for a variety of use cases. This will require advances in a broad range of software engineering disciplines, from program analysis and program synthesis technology to user interface design and evaluation. In this extended abstract, we propose two general strategies - - real-time program synthesis and domain-specific synthesis of general-purpose programs - - that may prove fruitful for overcoming the technical challenges. We also discuss metrics that will be important in evaluating the usability and utility of prodirect manipulation systems. © 2016 ACM.","doi":"10.1145/2889160.2889210","bibtex":"@CONFERENCE{Cheng2016,\nauthor={Cheng, X. and Zhong, H. and Chen, Y. and Hu, Z. and Zhao, J.},\ntitle={Rule-directed code clone synchronization},\njournal={IEEE International Conference on Program Comprehension},\nyear={2016},\nvolume={2016-July},\ndoi={10.1109/ICPC.2016.7503722},\nart_number={7503722},\nnote={cited By 3},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979771503&doi=10.1109%2fICPC.2016.7503722&partnerID=40&md5=fd90ba24b000c19011e5153fdb1a7657},\nabstract={Code clones are prevalent in software systems due to many factors in software development. Detecting code clones and managing consistency between them along code evolution can be very useful for reducing clone-related bugs and maintenance costs. Despite some early attempts at detecting code clones and managing the consistency between them, the state-of-the-art tool can only handle simple code clones whose structures are identical or quite similar. However, existing empirical studies show that clones can have quite different structures with their evolution, which can easily go beyond the capability of the state-of-the-art tool. In this paper, we propose CCSync, a novel, rule-directed approach, which paves the structure differences between the code clones and synchronizes them even when code clones become quite different in their structures. The key steps of this approach are, given two code clones, to (1) extract a synchronization rule from the relationship between the clones, and (2) once one code fragment is updated, propagate the modifications to the other following the synchronization rule. We have implemented a tool for CCSync and evaluated its effectiveness on five Java projects. Our results shows that there are many code clones suitable for synchronization, and our tool achieves precisions of up to 92% and recalls of up to 84%. In particular, more than 76% of our generated revisions are identical with manual revisions. © 2016 IEEE.},\nkeywords={Codes (symbols);  Computer programming;  Software design;  Synchronization, Code clone;  Code fragments;  Different structure;  Empirical studies;  Maintenance cost;  Software systems;  State of the art;  Structure difference, Cloning},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral69","name":"Performance issues and optimizations in Java script: An empirical study","authors":"Selakovic, M. and Pradel, M.","year":2016,"base":["Geral"],"abstract":"As JavaScript is becoming increasingly popular, the performance of JavaScript programs is crucial to ensure the responsiveness and energy-effciency of thousands of programs. Yet, little is known about performance issues that developers face in practice and they address these issues. This paper presents an empirical study of 98 fixed performance issues from 16 popular client-side and server-side JavaScript projects. We identify eight root causes of issues and show that ineffcient usage of APIs is the most prevalent root cause. Furthermore, we find that most issues are addressed by optimizations that modify only a few lines of code, without significantly affecting the complexity of the source code. By studying the performance impact of optimizations on several versions of the SpiderMonkey and V8 engines, we find that only 42.68% of all optimizations improve performance consistently across all versions of both engines. Finally, we observe that many optimizations are instances of patterns applicable across projects, as evidenced by 139 previously unknown optimization opportunities that we find based on the patterns identified during the study. The results of the study help application developers to avoid common mistakes, researchers to develop performance-related techniques that address relevant problems, and engine developers to address prevalent bottleneck patterns. © 2016 ACM.","doi":"10.1145/2884781.2884829","bibtex":"@CONFERENCE{Chugh2016781,\nauthor={Chugh, R.},\ntitle={Prodirect manipulation: Bidirectional programming for the masses},\njournal={Proceedings - International Conference on Software Engineering},\nyear={2016},\npages={781-784},\ndoi={10.1145/2889160.2889210},\nnote={cited By 1},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975843943&doi=10.1145%2f2889160.2889210&partnerID=40&md5=f73cb929f8a53369cadc6ca98e99ca61},\nabstract={Software interfaces today generally fall at either end of a spectrum. On one end are programmable systems, which allow expert users (i.e. programmers) to write software artifacts that describe complex abstractions, but programs are disconnected from their eventual output. On the other end are domain-specific graphical user interfaces (GUIs), which allow end users (i.e. non-programmers) to easily create varied content but present insurmountable walls when a desired feature is not built-in. Both programmatic and direct manipulation have distinct strengths, but users must typically choose one over the other or use some ad-hoc combination of systems. Our goal, put simply, is to bridge this divide. We envision novel software systems that tightly couple programmatic and direct manipulation - - a combination we dub prodirect manipulation - - for a variety of use cases. This will require advances in a broad range of software engineering disciplines, from program analysis and program synthesis technology to user interface design and evaluation. In this extended abstract, we propose two general strategies - - real-time program synthesis and domain-specific synthesis of general-purpose programs - - that may prove fruitful for overcoming the technical challenges. We also discuss metrics that will be important in evaluating the usability and utility of prodirect manipulation systems. © 2016 ACM.},\nkeywords={Domain walls;  Graphical user interfaces;  Human computer interaction;  Software engineering;  User interfaces, Bi-directional programming;  End user programming;  Engineering disciplines;  Graphical user interface (GUIs);  Prodirect manipulation;  Program synthesis;  Programmable systems;  User interface designs, Computer programming},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral70","name":"Programming by examples (and its applications in data wrangling)","authors":"Gulwani, S.","year":2016,"base":["Geral"],"abstract":"Programming by Examples (PBE) has the potential to revolutionize enduser programming by enabling end users, most of whom are non-programmers, to create scripts for automating repetitive tasks. PBE involves synthesizing intended programs in an underlying domain-specific language (DSL) from example based specifications (Ispec).We formalize the notion of Ispec and discuss some principles behind designing useful DSLs for synthesis. A key technical challenge in PBE is to search for programs that are consistent with the Ispec provided by the user. We present a divide-and-conquer based search paradigm that leverages deductive rules and version space algebras for manipulating sets of programs. Another technical challenge in PBE is to resolve the ambiguity that is inherent in the Ispec. We show how machine learning based ranking techniques can be used to predict an intended program within a set of programs that are consistent with the Ispec. We also present some user interaction models including program navigation and active-learning based conversational clarification that communicate actionable information to the user to help resolve ambiguity in the Ispec. The above-mentioned concepts are illustrated using practical PBE systems for data wrangling (including FlashFill, FlashExtract, FlashRelate), several of which have already been deployed in the real world. © 2016 The authors and IOS Press. All rights reserved.","doi":"10.3233/978-1-61499-627-9-137","bibtex":"@CONFERENCE{Selakovic201661,\nauthor={Selakovic, M. and Pradel, M.},\ntitle={Performance issues and optimizations in Java script: An empirical study},\njournal={Proceedings - International Conference on Software Engineering},\nyear={2016},\nvolume={14-22-May-2016},\npages={61-72},\ndoi={10.1145/2884781.2884829},\nnote={cited By 23},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971483495&doi=10.1145%2f2884781.2884829&partnerID=40&md5=6fe6b730ce029b2e0572c4fb441821fe},\nabstract={As JavaScript is becoming increasingly popular, the performance of JavaScript programs is crucial to ensure the responsiveness and energy-effciency of thousands of programs. Yet, little is known about performance issues that developers face in practice and they address these issues. This paper presents an empirical study of 98 fixed performance issues from 16 popular client-side and server-side JavaScript projects. We identify eight root causes of issues and show that ineffcient usage of APIs is the most prevalent root cause. Furthermore, we find that most issues are addressed by optimizations that modify only a few lines of code, without significantly affecting the complexity of the source code. By studying the performance impact of optimizations on several versions of the SpiderMonkey and V8 engines, we find that only 42.68% of all optimizations improve performance consistently across all versions of both engines. Finally, we observe that many optimizations are instances of patterns applicable across projects, as evidenced by 139 previously unknown optimization opportunities that we find based on the patterns identified during the study. The results of the study help application developers to avoid common mistakes, researchers to develop performance-related techniques that address relevant problems, and engine developers to address prevalent bottleneck patterns. © 2016 ACM.},\nkeywords={Engines;  High level languages, Application developers;  Empirical studies;  Improve performance;  JavaScript programs;  Lines of code;  Performance impact;  Performance issues;  Source codes, Software engineering},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral71","name":"Fixing recurring crash bugs via analyzing Q&A sites","authors":"Gao, Q. and Zhang, H. and Wang, J. and Xiong, Y. and Zhang, L. and Mei, H.","year":2016,"base":["Geral"],"abstract":"Recurring bugs are common in software systems, especially in client programs that depend on the same framework. Existing research uses human-written templates, and is limited to certain types of bugs. In this paper, we propose a fully automatic approach to fixing recurring crash bugs via analyzing Q&A sites. By extracting queries from crash traces and retrieving a list of Q&A pages, we analyze the pages and generate edit scripts. Then we apply these scripts to target source code and filter out the incorrect patches. The empirical results show that our approach is accurate in fixing real-world crash bugs, and can complement existing bug-fixing approaches. © 2015 IEEE.","doi":"10.1109/ASE.2015.81","bibtex":"@BOOK{Gulwani2016137,\nauthor={Gulwani, S.},\ntitle={Programming by examples (and its applications in data wrangling)},\njournal={Dependable Software Systems Engineering},\nyear={2016},\nvolume={45},\npages={137-158},\ndoi={10.3233/978-1-61499-627-9-137},\nnote={cited By 2},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84980006611&doi=10.3233%2f978-1-61499-627-9-137&partnerID=40&md5=bd32ebd0cd9f9c2c5cb779e705d80859},\nabstract={Programming by Examples (PBE) has the potential to revolutionize enduser programming by enabling end users, most of whom are non-programmers, to create scripts for automating repetitive tasks. PBE involves synthesizing intended programs in an underlying domain-specific language (DSL) from example based specifications (Ispec).We formalize the notion of Ispec and discuss some principles behind designing useful DSLs for synthesis. A key technical challenge in PBE is to search for programs that are consistent with the Ispec provided by the user. We present a divide-and-conquer based search paradigm that leverages deductive rules and version space algebras for manipulating sets of programs. Another technical challenge in PBE is to resolve the ambiguity that is inherent in the Ispec. We show how machine learning based ranking techniques can be used to predict an intended program within a set of programs that are consistent with the Ispec. We also present some user interaction models including program navigation and active-learning based conversational clarification that communicate actionable information to the user to help resolve ambiguity in the Ispec. The above-mentioned concepts are illustrated using practical PBE systems for data wrangling (including FlashFill, FlashExtract, FlashRelate), several of which have already been deployed in the real world. © 2016 The authors and IOS Press. All rights reserved.},\ndocument_type={Book Chapter},\nsource={Scopus},\n}\n\n"},{"id":"Geral72","name":"Automatic change recommendation of models and meta models based on change histories","authors":"Kögel, S. and Groner, R. and Tichy, M.","year":2016,"base":["Geral"],"abstract":"Model-driven software engineering uses models and meta models as key artefacts in the software development process. Typically, changes in the models (or meta models) do not come in isolation but are part of more complex change sets where a single change depends on other changes, e.g., a component is added to an architectural model and thereafter ports and connectors connect this component to other components. Furthermore, these sets of related and depending changes are often recurring, e.g., always when a component is added to an architecture, it is highly likely that ports are added to that component, too. This is similar for changes in meta models. Our goal is to help engineers by (1) automatically identifying clusters of related changes on model histories and (2) recommending corresponding changes after the engineer performs a single change. In this position paper, we present an initial technique to achieve our goal. We evaluate our technique with models from the Eclipse GMF project and present our recommendations as well as the recommendation quality. Our evaluation found an average precision between 0:43 and 0:82 for our recommendations.","bibtex":"@CONFERENCE{Gao2016307,\nauthor={Gao, Q. and Zhang, H. and Wang, J. and Xiong, Y. and Zhang, L. and Mei, H.},\ntitle={Fixing recurring crash bugs via analyzing Q&A sites},\njournal={Proceedings - 2015 30th IEEE/ACM International Conference on Automated Software Engineering, ASE 2015},\nyear={2016},\npages={307-318},\ndoi={10.1109/ASE.2015.81},\nart_number={7372020},\nnote={cited By 30},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963852339&doi=10.1109%2fASE.2015.81&partnerID=40&md5=075a4269b366c7b5059eecd63deb39c7},\nabstract={Recurring bugs are common in software systems, especially in client programs that depend on the same framework. Existing research uses human-written templates, and is limited to certain types of bugs. In this paper, we propose a fully automatic approach to fixing recurring crash bugs via analyzing Q&A sites. By extracting queries from crash traces and retrieving a list of Q&A pages, we analyze the pages and generate edit scripts. Then we apply these scripts to target source code and filter out the incorrect patches. The empirical results show that our approach is accurate in fixing real-world crash bugs, and can complement existing bug-fixing approaches. © 2015 IEEE.},\nkeywords={Software engineering, And filters;  Automatic approaches;  Bug-fixing;  Client programs;  Real-world;  Software systems;  Target source, Program debugging},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral73","name":"Search-based generalization and refinement of code templates","authors":"Molderez, T. and de Roover, C.","year":2016,"base":["Geral"],"abstract":"Several tools support code templates as a means to specify searches within a program’s source code. Despite their ubiquity, code templates can often prove difficult to specify, and may produce too many or too few match results. In this paper, we present a search-based approach to support developers in specifying templates. This approach uses a suite of mutation operators to recommend changes to a given template, such that it matches with a desired set of code snippets. We evaluate our approach on the problem of inferring a code template that matches all instances of a design pattern, given one instance as a starting template. © Springer International Publishing AG 2016.","doi":"10.1007/978-3-319-47106-8_13","bibtex":"@CONFERENCE{Kögel201614,\nauthor={Kögel, S. and Groner, R. and Tichy, M.},\ntitle={Automatic change recommendation of models and meta models based on change histories},\njournal={CEUR Workshop Proceedings},\nyear={2016},\nvolume={1706},\npages={14-19},\nnote={cited By 1},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996799553&partnerID=40&md5=5adf5f9259609fe9fd4bedd87fc420f2},\nabstract={Model-driven software engineering uses models and meta models as key artefacts in the software development process. Typically, changes in the models (or meta models) do not come in isolation but are part of more complex change sets where a single change depends on other changes, e.g., a component is added to an architectural model and thereafter ports and connectors connect this component to other components. Furthermore, these sets of related and depending changes are often recurring, e.g., always when a component is added to an architecture, it is highly likely that ports are added to that component, too. This is similar for changes in meta models. Our goal is to help engineers by (1) automatically identifying clusters of related changes on model histories and (2) recommending corresponding changes after the engineer performs a single change. In this position paper, we present an initial technique to achieve our goal. We evaluate our technique with models from the Eclipse GMF project and present our recommendations as well as the recommendation quality. Our evaluation found an average precision between 0:43 and 0:82 for our recommendations.},\nkeywords={Quality control;  Software engineering, Architectural modeling;  Change history;  Change recommendation;  History minings;  Model driven development;  Model driven software engineering;  Position papers;  Software development process, Software design},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral74","name":"Programming by examples: Applications, algorithms, and ambiguity resolution","authors":"Gulwani, S.","year":2016,"base":["Geral"],"abstract":"99% of computer end users do not know programming, and struggle with repetitive tasks. Programming by Examples (PBE) can revolutionize this landscape by enabling users to synthesize intended programs from example based specifications. A key technical challenge in PBE is to search for programs that are consistent with the examples provided by the user. Our efficient search methodology is based on two key ideas: (i) Restriction of the search space to an appropriate domainspecific language that offers balanced expressivity and readability (ii) A divide-and-conquer based deductive search paradigm that inductively reduces the problem of synthesizing a program of a certain kind that satisfies a given specification into sub-problems that refer to sub-programs or sub-specifications. Another challenge in PBE is to resolve the ambiguity in the example based specification. We will discuss two complementary approaches: (a) machine learning based ranking techniques that can pick an intended program from among those that satisfy the specification, and (b) active-learning based user interaction models. The above concepts will be illustrated using FlashFill, FlashExtract, and FlashRelate— PBE technologies for data manipulation domains. These technologies, which have been released inside various Microsoft products, are useful for data scientists who spend 80% of their time wrangling with data. The Microsoft PROSE SDK allows easy construction of such technologies. © Springer International Publishing Switzerland 2016.","doi":"10.1007/978-3-319-40229-1_2","bibtex":"@ARTICLE{Molderez2016192,\nauthor={Molderez, T. and de Roover, C.},\ntitle={Search-based generalization and refinement of code templates},\njournal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},\nyear={2016},\nvolume={9962 LNCS},\npages={192-208},\ndoi={10.1007/978-3-319-47106-8_13},\nnote={cited By 1},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989880911&doi=10.1007%2f978-3-319-47106-8_13&partnerID=40&md5=246d2f112af4470291c1d69fae993ba4},\nabstract={Several tools support code templates as a means to specify searches within a program’s source code. Despite their ubiquity, code templates can often prove difficult to specify, and may produce too many or too few match results. In this paper, we present a search-based approach to support developers in specifying templates. This approach uses a suite of mutation operators to recommend changes to a given template, such that it matches with a desired set of code snippets. We evaluate our approach on the problem of inferring a code template that matches all instances of a design pattern, given one instance as a starting template. © Springer International Publishing AG 2016.},\nkeywords={Codes (symbols);  Evolutionary algorithms;  Recommender systems;  Software engineering, Design Patterns;  Mutation operators;  Search-based;  Source codes;  Templates, Template matching},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral75","name":"BlinkFill: Semisupervised programming by example for syntactic string transformations","authors":"Singh, R.","year":2016,"base":["Geral"],"abstract":"The recent Programming By Example (PBE) techniques such as FlashFill have shown great promise for enabling end-users to perform data transformation tasks using inputoutput examples. Since examples are inherently an underspecification, there are typically a large number of hypotheses conforming to the examples, and the PBE techniques suffer from scalability issues for finding the intended program amongst the large space. We present a semi-supervised learning technique to significantly reduce this ambiguity by using the logical information present in the input data to guide the synthesis algorithm. We develop a data structure InputDataGraph to succinctly represent a large set of logical patterns that are shared across the input data, and use this graph to effciently learn substring expressions in a new PBE system BlinkFill. We evaluate BlinkFill on 207 real-world benchmarks and show that BlinkFill is significantly faster (on average 41×) and requires fewer input-output examples (1.27 vs 1.53) to learn the desired transformations in comparison to FlashFill. © 2016 VLDB Endowment.","doi":"10.14778/2977797.2977807","bibtex":"@ARTICLE{Gulwani20169,\nauthor={Gulwani, S.},\ntitle={Programming by examples: Applications, algorithms, and ambiguity resolution},\njournal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},\nyear={2016},\nvolume={9706},\npages={9-14},\ndoi={10.1007/978-3-319-40229-1_2},\nnote={cited By 3},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976614459&doi=10.1007%2f978-3-319-40229-1_2&partnerID=40&md5=cdb4b6460ace33a89696533595840a75},\nabstract={99% of computer end users do not know programming, and struggle with repetitive tasks. Programming by Examples (PBE) can revolutionize this landscape by enabling users to synthesize intended programs from example based specifications. A key technical challenge in PBE is to search for programs that are consistent with the examples provided by the user. Our efficient search methodology is based on two key ideas: (i) Restriction of the search space to an appropriate domainspecific language that offers balanced expressivity and readability (ii) A divide-and-conquer based deductive search paradigm that inductively reduces the problem of synthesizing a program of a certain kind that satisfies a given specification into sub-problems that refer to sub-programs or sub-specifications. Another challenge in PBE is to resolve the ambiguity in the example based specification. We will discuss two complementary approaches: (a) machine learning based ranking techniques that can pick an intended program from among those that satisfy the specification, and (b) active-learning based user interaction models. The above concepts will be illustrated using FlashFill, FlashExtract, and FlashRelate— PBE technologies for data manipulation domains. These technologies, which have been released inside various Microsoft products, are useful for data scientists who spend 80% of their time wrangling with data. The Microsoft PROSE SDK allows easy construction of such technologies. © Springer International Publishing Switzerland 2016.},\nkeywords={Algorithms;  Artificial intelligence;  Learning systems;  Specifications;  Web browsers, Ambiguity resolution;  Data manipulations;  Divide and conquer;  Domain specific languages;  Programming by Example;  Ranking technique;  Technical challenges;  User interaction, Computer programming},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral76","name":"Recording and replaying system specific, source code transformations","authors":"Santos, G. and Etien, A. and Anquetil, N. and Ducasse, S. and Valente, M.T.","year":2015,"base":["Geral"],"abstract":"During its lifetime, a software system is under continuous maintenance to remain useful. Maintenance can be achieved in activities such as adding new features, fixing bugs, improving the system's structure, or adapting to new APIs. In such cases, developers sometimes perform sequences of code changes in a systematic way. These sequences consist of small code changes (e.g., create a class, then extract a method to this class), which are applied to groups of related code entities (e.g., some of the methods of a class). This paper presents the design and proof-of-concept implementation of a tool called MacroRecorder. This tool records a sequence of code changes, then it allows the developer to generalize this sequence in order to apply it in other code locations. In this paper, we discuss MACRORECORDER's approach that is independent of both development and transformation tools. The evaluation is based on previous work on repetitive code changes related to rearchitecting. MacroRecorder was able to replay 92% of the examples, which consisted in up to seven code entities modified up to 66 times. The generation of a customizable, large-scale transformation operator has the potential to efficiently assist code maintenance. © 2015 IEEE.","doi":"10.1109/SCAM.2015.7335418","bibtex":"@CONFERENCE{Singh2016816,\nauthor={Singh, R.},\ntitle={BlinkFill: Semisupervised programming by example for syntactic string transformations},\njournal={Proceedings of the VLDB Endowment},\nyear={2016},\nvolume={9},\nnumber={10},\npages={816-827},\ndoi={10.14778/2977797.2977807},\nnote={cited By 13},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979581533&doi=10.14778%2f2977797.2977807&partnerID=40&md5=66f5e5e705c159427fc948b4fa02c8b6},\nabstract={The recent Programming By Example (PBE) techniques such as FlashFill have shown great promise for enabling end-users to perform data transformation tasks using inputoutput examples. Since examples are inherently an underspecification, there are typically a large number of hypotheses conforming to the examples, and the PBE techniques suffer from scalability issues for finding the intended program amongst the large space. We present a semi-supervised learning technique to significantly reduce this ambiguity by using the logical information present in the input data to guide the synthesis algorithm. We develop a data structure InputDataGraph to succinctly represent a large set of logical patterns that are shared across the input data, and use this graph to effciently learn substring expressions in a new PBE system BlinkFill. We evaluate BlinkFill on 207 real-world benchmarks and show that BlinkFill is significantly faster (on average 41×) and requires fewer input-output examples (1.27 vs 1.53) to learn the desired transformations in comparison to FlashFill. © 2016 VLDB Endowment.},\nkeywords={Human computer interaction;  Input output programs;  Supervised learning, Data transformation;  Programming by Example;  Scalability issue;  Semi-supervised;  Semi-supervised learning techniques;  String transformation;  Synthesis algorithms;  Underspecification, Metadata},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral77","name":"System specific, source code transformations","authors":"Santos, G. and Anquetil, N. and Etien, A. and Ducasse, S. and Valente, M.T.","year":2015,"base":["Geral"],"abstract":"During its lifetime, a software system might undergo a major transformation effort in its structure, for example to migrate to a new architecture or bring some drastic improvements to the system. Particularly in this context, we found evidences that some sequences of code changes are made in a systematic way. These sequences are composed of small code transformations (e.g., create a class, move a method) which are repeatedly applied to groups of related entities (e.g., a class and some of its methods). A typical example consists in the systematic introduction of a Factory design pattern on the classes of a package. We define these sequences as transformation patterns. In this paper, we identify examples of transformation patterns in real world software systems and study their properties: (i) they are specific to a system; (ii) they were applied manually; (iii) they were not always applied to all the software entities which could have been transformed; (iv) they were sometimes complex; and (v) they were not always applied in one shot but over several releases. These results suggest that transformation patterns could benefit from automated support in their application. From this study, we propose as future work to develop a macro recorder, a tool with which a developer records a sequence of code transformations and then automatically applies them in other parts of the system as a customizable, large-scale transformation operator. © 2015 IEEE.","doi":"10.1109/ICSM.2015.7332468","bibtex":"@CONFERENCE{Santos2015221,\nauthor={Santos, G. and Etien, A. and Anquetil, N. and Ducasse, S. and Valente, M.T.},\ntitle={Recording and replaying system specific, source code transformations},\njournal={2015 IEEE 15th International Working Conference on Source Code Analysis and Manipulation, SCAM 2015 - Proceedings},\nyear={2015},\npages={221-230},\ndoi={10.1109/SCAM.2015.7335418},\nart_number={7335418},\nnote={cited By 1},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963579544&doi=10.1109%2fSCAM.2015.7335418&partnerID=40&md5=caab743909e7534391f231ce998796bd},\nabstract={During its lifetime, a software system is under continuous maintenance to remain useful. Maintenance can be achieved in activities such as adding new features, fixing bugs, improving the system's structure, or adapting to new APIs. In such cases, developers sometimes perform sequences of code changes in a systematic way. These sequences consist of small code changes (e.g., create a class, then extract a method to this class), which are applied to groups of related code entities (e.g., some of the methods of a class). This paper presents the design and proof-of-concept implementation of a tool called MacroRecorder. This tool records a sequence of code changes, then it allows the developer to generalize this sequence in order to apply it in other code locations. In this paper, we discuss MACRORECORDER's approach that is independent of both development and transformation tools. The evaluation is based on previous work on repetitive code changes related to rearchitecting. MacroRecorder was able to replay 92% of the examples, which consisted in up to seven code entities modified up to 66 times. The generation of a customizable, large-scale transformation operator has the potential to efficiently assist code maintenance. © 2015 IEEE.},\nkeywords={Computer programming languages;  Computer software maintenance;  Cosine transforms;  Maintenance;  Program debugging, Automated code;  Continuous maintenance;  Programming by demon-stration;  Refactorings;  Scale transformation;  Software Evolution;  Source code transformation;  Transformation tools, Codes (symbols)},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral78","name":"Clone-based and interactive recommendation for modifying pasted code","authors":"Lin, Y. and Peng, X. and Xing, Z. and Zheng, D. and Zhao, W.","year":2015,"base":["Geral"],"abstract":"Developers often need to modify pasted code when programming with copy-and-paste practice. Some modifications on pasted code could involve lots of editing efforts, and any missing or wrong edit could incur bugs. In this paper, we propose a clone-based and interactive approach to recommending where and how to modify the pasted code. In our approach, we regard clones of the pasted code as the results of historical copy-and-paste operations and their differences as historical modifications on the same piece of code. Our approach first retrieves clones of the pasted code from a clone repository and detects syntactically complete differences among them. Then our approach transfers each clone difference into a modification slot on the pasted code, suggests options for each slot, and further mines modifying regulations from the clone differences. Based on the mined modifying regulations, our approach dynamically updates the suggested options and their ranking in each slot according to developer's modifications on the pasted code. We implement a proof-of-concept tool CCDemon based on our approach and evaluate its effectiveness based on code clones detected from five open source projects. The results show that our approach can identify 96.9% of the to-be-modified positions in pasted code and suggest 75.0% of the required modifications. Our human study further confirms that CCDemon can help developers to accomplish their modifications of pasted code more efficiently. © 2015 ACM.","doi":"10.1145/2786805.2786871","bibtex":"@CONFERENCE{Santos2015221,\nauthor={Santos, G. and Anquetil, N. and Etien, A. and Ducasse, S. and Valente, M.T.},\ntitle={System specific, source code transformations},\njournal={2015 IEEE 31st International Conference on Software Maintenance and Evolution, ICSME 2015 - Proceedings},\nyear={2015},\npages={221-230},\ndoi={10.1109/ICSM.2015.7332468},\nart_number={7332468},\nnote={cited By 5},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961588842&doi=10.1109%2fICSM.2015.7332468&partnerID=40&md5=c7da4702e7797779c2fe047863c830a2},\nabstract={During its lifetime, a software system might undergo a major transformation effort in its structure, for example to migrate to a new architecture or bring some drastic improvements to the system. Particularly in this context, we found evidences that some sequences of code changes are made in a systematic way. These sequences are composed of small code transformations (e.g., create a class, move a method) which are repeatedly applied to groups of related entities (e.g., a class and some of its methods). A typical example consists in the systematic introduction of a Factory design pattern on the classes of a package. We define these sequences as transformation patterns. In this paper, we identify examples of transformation patterns in real world software systems and study their properties: (i) they are specific to a system; (ii) they were applied manually; (iii) they were not always applied to all the software entities which could have been transformed; (iv) they were sometimes complex; and (v) they were not always applied in one shot but over several releases. These results suggest that transformation patterns could benefit from automated support in their application. From this study, we propose as future work to develop a macro recorder, a tool with which a developer records a sequence of code transformations and then automatically applies them in other parts of the system as a customizable, large-scale transformation operator. © 2015 IEEE.},\nkeywords={Codes (symbols);  Computer software;  Cosine transforms;  Maintenance, Code transformation;  Rearchitecting;  Refactoring tools;  Restructuring;  Scale transformation;  Source code transformation;  Transformation effort;  Transformation patterns, Computer software maintenance},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral79","name":"Crowd debugging","authors":"Chen, F. and Kim, S.","year":2015,"base":["Geral"],"abstract":"Research shows that, in general, many people turn to QA sites to solicit answers to their problems. We observe in Stack Overflow a huge number of recurring questions, 1,632,590, despite mechanisms having been put into place to prevent these recurring questions. Recurring questions imply developers are facing similar issues in their source code. However, limitations exist in the QA sites. Developers need to visit them frequently and/or should be familiar with all the content to take advantage of the crowd's knowledge. Due to the large and rapid growth of QA data, it is difficult, if not impossible for developers to catch up. To address these limitations, we propose mining the QA site, Stack Overflow, to leverage the huge mass of crowd knowledge to help developers debug their code. Our approach reveals 189 warnings and 171 (90.5%) of them are confirmed by developers from eight high-quality and well-maintained projects. Developers appreciate these findings because the crowd provides solutions and comprehensive explanations to the issues. We compared the confirmed bugs with three popular static analysis tools (FindBugs, JLint and PMD). Of the 171 bugs identified by our approach, only FindBugs detected six of them whereas JLint and PMD detected none. © 2015 ACM.","doi":"10.1145/2786805.2786819","bibtex":"@CONFERENCE{Lin2015520,\nauthor={Lin, Y. and Peng, X. and Xing, Z. and Zheng, D. and Zhao, W.},\ntitle={Clone-based and interactive recommendation for modifying pasted code},\njournal={2015 10th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, ESEC/FSE 2015 - Proceedings},\nyear={2015},\npages={520-531},\ndoi={10.1145/2786805.2786871},\nnote={cited By 5},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960403747&doi=10.1145%2f2786805.2786871&partnerID=40&md5=3deb374eda483a956e83cfea41b7cb04},\nabstract={Developers often need to modify pasted code when programming with copy-and-paste practice. Some modifications on pasted code could involve lots of editing efforts, and any missing or wrong edit could incur bugs. In this paper, we propose a clone-based and interactive approach to recommending where and how to modify the pasted code. In our approach, we regard clones of the pasted code as the results of historical copy-and-paste operations and their differences as historical modifications on the same piece of code. Our approach first retrieves clones of the pasted code from a clone repository and detects syntactically complete differences among them. Then our approach transfers each clone difference into a modification slot on the pasted code, suggests options for each slot, and further mines modifying regulations from the clone differences. Based on the mined modifying regulations, our approach dynamically updates the suggested options and their ranking in each slot according to developer's modifications on the pasted code. We implement a proof-of-concept tool CCDemon based on our approach and evaluate its effectiveness based on code clones detected from five open source projects. The results show that our approach can identify 96.9% of the to-be-modified positions in pasted code and suggest 75.0% of the required modifications. Our human study further confirms that CCDemon can help developers to accomplish their modifications of pasted code more efficiently. © 2015 ACM.},\nkeywords={Cloning;  Codes (symbols);  Copying;  Open source software;  Software engineering, Code clone;  Copy-and-paste;  Differencing;  Recommendation;  Reuse, Open systems},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral80","name":"Interactive code review for systematic changes","authors":"Zhang, T. and Song, M. and Pinedo, J. and Kim, M.","year":2015,"base":["Geral"],"abstract":"Developers often inspect a diff patch during peer code reviews. Diff patches show low-level program differences per file without summarizing systematic changes-similar, related changes to multiple contexts. We present CRITICS, an interactive approach for inspecting systematic changes. When a developer specifies code change within a diff patch, CRITICS allows developers to customize the change template by iteratively generalizing change content and context. By matching a generalized template against the codebase, it summarizes similar changes and detects potential mistakes. We evaluated CRITICS using two methods. First, we conducted a user study at Salesforce.com, where professional engineers used CRITICS to investigate diff patches authored by their own team. After using CRITICS, all six participants indicated that they would like CRITICS to be integrated into their current code review environment. This also attests to the fact that CRITICS scales to an industry-scale project and can be easily adopted by professional engineers. Second, we conducted a user study where twelve participants reviewed diff patches using CRITICS and Eclipse diff. The results show that human subjects using CRITICS answer questions about systematic changes 47.3% more correctly with 31.9% saving in time during code review tasks, in comparison to the baseline use of Eclipse diff. These results show that CRITICS should improve developer productivity in inspecting systematic changes during peer code reviews. © 2015 IEEE.","doi":"10.1109/ICSE.2015.33","bibtex":"@CONFERENCE{Chen2015320,\nauthor={Chen, F. and Kim, S.},\ntitle={Crowd debugging},\njournal={2015 10th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, ESEC/FSE 2015 - Proceedings},\nyear={2015},\npages={320-332},\ndoi={10.1145/2786805.2786819},\nnote={cited By 14},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960393115&doi=10.1145%2f2786805.2786819&partnerID=40&md5=b989d9a2a1cb36798ea196dae432e413},\nabstract={Research shows that, in general, many people turn to QA sites to solicit answers to their problems. We observe in Stack Overflow a huge number of recurring questions, 1,632,590, despite mechanisms having been put into place to prevent these recurring questions. Recurring questions imply developers are facing similar issues in their source code. However, limitations exist in the QA sites. Developers need to visit them frequently and/or should be familiar with all the content to take advantage of the crowd's knowledge. Due to the large and rapid growth of QA data, it is difficult, if not impossible for developers to catch up. To address these limitations, we propose mining the QA site, Stack Overflow, to leverage the huge mass of crowd knowledge to help developers debug their code. Our approach reveals 189 warnings and 171 (90.5%) of them are confirmed by developers from eight high-quality and well-maintained projects. Developers appreciate these findings because the crowd provides solutions and comprehensive explanations to the issues. We compared the confirmed bugs with three popular static analysis tools (FindBugs, JLint and PMD). Of the 171 bugs identified by our approach, only FindBugs detected six of them whereas JLint and PMD detected none. © 2015 ACM.},\nkeywords={Computer debugging;  Knowledge management;  Software engineering, High quality;  Rapid growth;  Source codes;  Stack overflow, Static analysis},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral81","name":"CARAMEL: Detecting and fixing performance problems that have non-intrusive fixes","authors":"Nistor, A. and Chang, P.-C. and Radoi, C. and Lu, S.","year":2015,"base":["Geral"],"abstract":"Performance bugs are programming errors that slow down program execution. While existing techniques can detect various types of performance bugs, a crucial and practical aspect of performance bugs has not received the attention it deserves: how likely are developers to fix a performance bug? In practice, fixing a performance bug can have both benefits and drawbacks, and developers fix a performance bug only when the benefits outweigh the drawbacks. Unfortunately, for many performance bugs, the benefits and drawbacks are difficult to assess accurately. This paper presents CARAMEL, a novel static technique that detects and fixes performance bugs that have non-intrusive fixes likely to be adopted by developers. Each performance bug detected by CARAMEL is associated with a loop and a condition. When the condition becomes true during the loop execution, all the remaining computation performed by the loop is wasted. Developers typically fix such performance bugs because these bugs waste computation in loops and have nonintrusive fixes: when some condition becomes true dynamically, just break out of the loop. Given a program, CARAMEL detects such bugs statically and gives developers a potential sourcelevel fix for each bug. We evaluate CARAMEL on real-world applications, including 11 popular Java applications (e.g., Groovy, Log4J, Lucene, Struts, Tomcat, etc) and 4 widely used C/C++ applications (Chromium, GCC, Mozilla, and MySQL). CARAMEL finds 61 new performance bugs in the Java applications and 89 new performance bugs in the C/C++ applications. Based on our bug reports, developers so far have fixed 51 and 65 performance bugs in the Java and C/C++ applications, respectively. Most of the remaining bugs are still under consideration by developers. © 2015 IEEE.","doi":"10.1109/ICSE.2015.100","bibtex":"@CONFERENCE{Zhang2015111,\nauthor={Zhang, T. and Song, M. and Pinedo, J. and Kim, M.},\ntitle={Interactive code review for systematic changes},\njournal={Proceedings - International Conference on Software Engineering},\nyear={2015},\nvolume={1},\npages={111-122},\ndoi={10.1109/ICSE.2015.33},\nart_number={7194566},\nnote={cited By 25},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951825931&doi=10.1109%2fICSE.2015.33&partnerID=40&md5=0d25c8e6c882b9d18bad5a2a67b001e6},\nabstract={Developers often inspect a diff patch during peer code reviews. Diff patches show low-level program differences per file without summarizing systematic changes-similar, related changes to multiple contexts. We present CRITICS, an interactive approach for inspecting systematic changes. When a developer specifies code change within a diff patch, CRITICS allows developers to customize the change template by iteratively generalizing change content and context. By matching a generalized template against the codebase, it summarizes similar changes and detects potential mistakes. We evaluated CRITICS using two methods. First, we conducted a user study at Salesforce.com, where professional engineers used CRITICS to investigate diff patches authored by their own team. After using CRITICS, all six participants indicated that they would like CRITICS to be integrated into their current code review environment. This also attests to the fact that CRITICS scales to an industry-scale project and can be easily adopted by professional engineers. Second, we conducted a user study where twelve participants reviewed diff patches using CRITICS and Eclipse diff. The results show that human subjects using CRITICS answer questions about systematic changes 47.3% more correctly with 31.9% saving in time during code review tasks, in comparison to the baseline use of Eclipse diff. These results show that CRITICS should improve developer productivity in inspecting systematic changes during peer code reviews. © 2015 IEEE.},\nkeywords={Inspection;  Iterative methods;  Software engineering, Current codes;  Human subjects;  Interactive approach;  Low-level programs;  Multiple contexts;  Peer code review;  Professional engineer;  Systematic changes, Codes (symbols)},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral82","name":"Automatic error elimination by horizontal code transfer across multiple applications","authors":"Sidiroglou-Douskos, S. and Lahtinen, E. and Long, F. and Rinard, M.","year":2015,"base":["Geral"],"abstract":"We present Code Phage (CP), a system for automatically transferring correct code from donor applications into recipient applications that process the same inputs to successfully eliminate errors in the recipient. Experimental results using seven donor applications to eliminate ten errors in seven recipient applications highlight the ability of CP to transfer code across applications to eliminate out of bounds access, integer overflow, and divide by zero errors. Because CP works with binary donors with no need for source code or symbolic information, it supports a wide range of use cases. To the best of our knowledge, CP is the first system to automatically transfer code across multiple applications. © 2015 ACM.","doi":"10.1145/2737924.2737988","bibtex":"@CONFERENCE{Nistor2015902,\nauthor={Nistor, A. and Chang, P.-C. and Radoi, C. and Lu, S.},\ntitle={CARAMEL: Detecting and fixing performance problems that have non-intrusive fixes},\njournal={Proceedings - International Conference on Software Engineering},\nyear={2015},\nvolume={1},\npages={902-912},\ndoi={10.1109/ICSE.2015.100},\nart_number={7194636},\nnote={cited By 45},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951840695&doi=10.1109%2fICSE.2015.100&partnerID=40&md5=9c0a889a5a1ffb4b6395f7ed47280ea6},\nabstract={Performance bugs are programming errors that slow down program execution. While existing techniques can detect various types of performance bugs, a crucial and practical aspect of performance bugs has not received the attention it deserves: how likely are developers to fix a performance bug? In practice, fixing a performance bug can have both benefits and drawbacks, and developers fix a performance bug only when the benefits outweigh the drawbacks. Unfortunately, for many performance bugs, the benefits and drawbacks are difficult to assess accurately. This paper presents CARAMEL, a novel static technique that detects and fixes performance bugs that have non-intrusive fixes likely to be adopted by developers. Each performance bug detected by CARAMEL is associated with a loop and a condition. When the condition becomes true during the loop execution, all the remaining computation performed by the loop is wasted. Developers typically fix such performance bugs because these bugs waste computation in loops and have nonintrusive fixes: when some condition becomes true dynamically, just break out of the loop. Given a program, CARAMEL detects such bugs statically and gives developers a potential sourcelevel fix for each bug. We evaluate CARAMEL on real-world applications, including 11 popular Java applications (e.g., Groovy, Log4J, Lucene, Struts, Tomcat, etc) and 4 widely used C/C++ applications (Chromium, GCC, Mozilla, and MySQL). CARAMEL finds 61 new performance bugs in the Java applications and 89 new performance bugs in the C/C++ applications. Based on our bug reports, developers so far have fixed 51 and 65 performance bugs in the Java and C/C++ applications, respectively. Most of the remaining bugs are still under consideration by developers. © 2015 IEEE.},\nkeywords={C++ (programming language);  Costs;  Java programming language;  Software engineering, Bug reports;  Java applications;  Non-intrusive;  Performance bugs;  Performance problems;  Program execution;  Programming errors;  Static techniques, Program debugging},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral83","name":"Critics: An interactive code review tool for searching and inspecting systematic changes","authors":"Zhang, T. and Song, M. and Kim, M.","year":2014,"base":["Geral"],"abstract":"During peer code reviews, developers often examine program differences. When using existing program differencing tools, it is difficult for developers to inspect systematic changes|similar, related changes that are scattered across multiple files. Developers cannot easily answer questions such as \"what other code locations changed similar to this change?\" and \"are there any other locations that are similar to this code but are not updated?\" In this paper, we demonstrate CRITICS, an Eclipse plug-in that assists developers in inspecting systematic changes. It (1) allows developers to customize a context-aware change template, (2) searches for systematic changes using the template, and (3) detects missing or inconsistent edits. Developers can interactively refine the customized change template to see corresponding search results. CRITICS has potential to improve developer productivity in inspecting large, scattered edits during code reviews. The tool's demonstration video is available at https://www.youtube.com/watch?v=F2D7t-Z5rhk. Copyright 2014 ACM.","doi":"10.1145/2635868.2661675","bibtex":"@ARTICLE{Sidiroglou-Douskos201543,\nauthor={Sidiroglou-Douskos, S. and Lahtinen, E. and Long, F. and Rinard, M.},\ntitle={Automatic error elimination by horizontal code transfer across multiple applications},\njournal={ACM SIGPLAN Notices},\nyear={2015},\nvolume={50},\nnumber={6},\npages={43-54},\ndoi={10.1145/2737924.2737988},\nnote={cited By 17},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951014939&doi=10.1145%2f2737924.2737988&partnerID=40&md5=2cedfff3535e3140739b6fbd4db4d84c},\nabstract={We present Code Phage (CP), a system for automatically transferring correct code from donor applications into recipient applications that process the same inputs to successfully eliminate errors in the recipient. Experimental results using seven donor applications to eliminate ten errors in seven recipient applications highlight the ability of CP to transfer code across applications to eliminate out of bounds access, integer overflow, and divide by zero errors. Because CP works with binary donors with no need for source code or symbolic information, it supports a wide range of use cases. To the best of our knowledge, CP is the first system to automatically transfer code across multiple applications. © 2015 ACM.},\nkeywords={Errors;  Program translators, Automatic codes;  Correct code;  Error elimination;  First systems;  Integer overflow;  Multiple applications;  Source codes;  Zero errors, Codes (symbols)},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral84","name":"Selection and presentation practices for code example summarization","authors":"Ying, A.T.T. and Robillard, M.P.","year":2014,"base":["Geral"],"abstract":"Code examples are an important source for answering questions about software libraries and applications. Many usage contexts for code examples require them to be distilled to their essence: e.g., when serving as cues to longer documents, or for reminding developers of a previously known idiom. We conducted a study to discover how code can be summarized and why. As part of the study, we collected 156 pairs of code examples and their summaries from 16 participants, along with over 26 hours of think-aloud verbalizations detailing the decisions of the participants during their summarization activities. Based on a qualitative analysis of this data we elicited a list of practices followed by the participants to summarize code examples and propose empirically-supported hypotheses justifying the use of specific practices. One main finding was that none of the participants exclusively extracted code verbatim for the summaries, motivating abstractive summarization. The results provide a grounded basis for the development of code example summarization and presentation technology. Copyright 2014 ACM.","doi":"10.1145/2635868.2635877","bibtex":"@CONFERENCE{Zhang2014755,\nauthor={Zhang, T. and Song, M. and Kim, M.},\ntitle={Critics: An interactive code review tool for searching and inspecting systematic changes},\njournal={Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},\nyear={2014},\nvolume={16-21-November-2014},\npages={755-758},\ndoi={10.1145/2635868.2661675},\nnote={cited By 3},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986910764&doi=10.1145%2f2635868.2661675&partnerID=40&md5=48f9f2fd8365c31b51688fc097c0635a},\nabstract={During peer code reviews, developers often examine program differences. When using existing program differencing tools, it is difficult for developers to inspect systematic changes|similar, related changes that are scattered across multiple files. Developers cannot easily answer questions such as \"what other code locations changed similar to this change?\" and \"are there any other locations that are similar to this code but are not updated?\" In this paper, we demonstrate CRITICS, an Eclipse plug-in that assists developers in inspecting systematic changes. It (1) allows developers to customize a context-aware change template, (2) searches for systematic changes using the template, and (3) detects missing or inconsistent edits. Developers can interactively refine the customized change template to see corresponding search results. CRITICS has potential to improve developer productivity in inspecting large, scattered edits during code reviews. The tool's demonstration video is available at https://www.youtube.com/watch?v=F2D7t-Z5rhk. Copyright 2014 ACM.},\nkeywords={Inspection;  Software engineering, Code review;  Context-Aware;  Peer code review;  Plug-ins;  Program differencing;  Software Evolution;  Systematic changes, Codes (symbols)},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral85","name":"A tool to suggest similar program element modifications","authors":"Yang, Y. and Washizaki, H. and Fukazawa, Y.","year":2014,"base":["Geral"],"abstract":"Many program tasks require continuous modification of similar program elements, which is burdensome on programmers because continuous modifications are time consuming and some modifications are easily overlooked. To resolve this issue, we developed a tool, named SimilarHighlight, which extracted all possible matching elements via similarity patterns from recently modified elements using a sub syntax tree comparison. SimilarHighlight suggests similar program elements that may be modified during the next modification. Potential elements are highlighted and their text can be immediately selected by shortcut keys. Evaluations indicate that SimilarHighlight can improve programming productivity. Currently, SimilarHighlight supports C, C#, JAVA, JavaScript, and PHP, but in the future we will expand it to other languages. © 2014 IEEE.","doi":"10.1109/APSEC.2014.54","bibtex":"@CONFERENCE{Ying2014460,\nauthor={Ying, A.T.T. and Robillard, M.P.},\ntitle={Selection and presentation practices for code example summarization},\njournal={Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},\nyear={2014},\nvolume={16-21-November-2014},\npages={460-471},\ndoi={10.1145/2635868.2635877},\nnote={cited By 17},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986893030&doi=10.1145%2f2635868.2635877&partnerID=40&md5=465229eebfd86615ec13bb389f8ae142},\nabstract={Code examples are an important source for answering questions about software libraries and applications. Many usage contexts for code examples require them to be distilled to their essence: e.g., when serving as cues to longer documents, or for reminding developers of a previously known idiom. We conducted a study to discover how code can be summarized and why. As part of the study, we collected 156 pairs of code examples and their summaries from 16 participants, along with over 26 hours of think-aloud verbalizations detailing the decisions of the participants during their summarization activities. Based on a qualitative analysis of this data we elicited a list of practices followed by the participants to summarize code examples and propose empirically-supported hypotheses justifying the use of specific practices. One main finding was that none of the participants exclusively extracted code verbatim for the summaries, motivating abstractive summarization. The results provide a grounded basis for the development of code example summarization and presentation technology. Copyright 2014 ACM.},\nkeywords={Application programs;  Software engineering, Code examples;  Qualitative analysis;  Software libraries;  Summarization;  Think aloud;  Usage context, Codes (symbols)},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral86","name":"Scalable detection of missed cross-function refactorings","authors":"Milea, N.A. and Jiang, L. and Khoo, S.-C.","year":2014,"base":["Geral"],"abstract":"Refactoring is an important way to improve the design of existing code. Identifying refactoring opportunities (i.e., code fragments that can be refactored) in large code bases is a challenging task. In this paper, we propose a novel, automated and scalable technique for identifying cross-function refactoring opportunities that span more than one function (e.g., Extract Method and Inline Method). The key of our technique is the design of efficient vector inlining operations that emulate the effect of method inlining among code fragments, so that the problem of identifying cross-function refactoring can be reduced to the problem of finding similar vectors before and after inlining. We have implemented our technique in a prototype tool named ReDex which encodes Java programs to particular vectors. We have applied the tool to a large code base, 4.5 million lines of code, comprising of 200 bundle projects in the Eclipse ecosystem (e.g., Eclipse JDT, Eclipse PDE, Apache Commons, Hamcrest, etc.). Also, different from many other studies on detecting refactoring, ReDex only searches for code fragments that can be, but have not yet been, refactored in a way similar to some refactoring that happened in the code base. Our results show that ReDex can find 277 cross-function refactoring opportunities in 2 minutes, and 223 cases were labelled as true opportunities by users, and cover many categories of cross-function refactoring operations in classical refactoring books, such as Self Encapsulate Field, Decompose Conditional Expression, Hide Delegate, Preserve Whole Object, etc. Copyright 2014 ACM.","bibtex":"@CONFERENCE{Yang2014311,\nauthor={Yang, Y. and Washizaki, H. and Fukazawa, Y.},\ntitle={A tool to suggest similar program element modifications},\njournal={Proceedings - Asia-Pacific Software Engineering Conference, APSEC},\nyear={2014},\nvolume={1},\npages={311-318},\ndoi={10.1109/APSEC.2014.54},\nart_number={7091325},\nnote={cited By 0},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951280534&doi=10.1109%2fAPSEC.2014.54&partnerID=40&md5=5b480adc08ddd54e7e53ff7563d062b9},\nabstract={Many program tasks require continuous modification of similar program elements, which is burdensome on programmers because continuous modifications are time consuming and some modifications are easily overlooked. To resolve this issue, we developed a tool, named SimilarHighlight, which extracted all possible matching elements via similarity patterns from recently modified elements using a sub syntax tree comparison. SimilarHighlight suggests similar program elements that may be modified during the next modification. Potential elements are highlighted and their text can be immediately selected by shortcut keys. Evaluations indicate that SimilarHighlight can improve programming productivity. Currently, SimilarHighlight supports C, C#, JAVA, JavaScript, and PHP, but in the future we will expand it to other languages. © 2014 IEEE.},\nkeywords={Productivity;  Software engineering;  Syntactics;  Trees (mathematics), Continuous modification;  Matching elements;  Minimal Keystrokes;  Modification;  Program elements;  Similar elements;  Similarity patterns;  Syntax tree, Java programming language},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral87","name":"Detecting and characterizing semantic inconsistencies in ported code","authors":"Ray, B. and Kim, M. and Person, S. and Rungta, N.","year":2013,"base":["Geral"],"abstract":"Adding similar features and bug fixes often requires porting program patches from reference implementations and adapting them to target implementations. Porting errors may result from faulty adaptations or inconsistent updates. This paper investigates (1) the types of porting errors found in practice, and (2) how to detect and characterize potential porting errors. Analyzing version histories, we define five categories of porting errors, including incorrect control- and data-flow, code redundancy, inconsistent identifier renamings, etc. Leveraging this categorization, we design a static control- and data-dependence analysis technique, SPA, to detect and characterize porting inconsistencies. Our evaluation on code from four open-source projects shows that SPA can detect porting inconsistencies with 65% to 73% precision and 90% recall, and identify inconsistency types with 58% to 63% precision and 92% to 100% recall. In a comparison with two existing error detection tools, SPA improves precision by 14 to 17 percentage points. © 2013 IEEE.","doi":"10.1109/ASE.2013.6693095","bibtex":"@CONFERENCE{Milea2014138,\nauthor={Milea, N.A. and Jiang, L. and Khoo, S.-C.},\ntitle={Scalable detection of missed cross-function refactorings},\njournal={2014 International Symposium on Software Testing and Analysis, ISSTA 2014 - Proceedings},\nyear={2014},\npages={138-148},\nnote={cited By 4},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942772713&partnerID=40&md5=9ab2cbfb7609e69b67c329607b80eab7},\nabstract={Refactoring is an important way to improve the design of existing code. Identifying refactoring opportunities (i.e., code fragments that can be refactored) in large code bases is a challenging task. In this paper, we propose a novel, automated and scalable technique for identifying cross-function refactoring opportunities that span more than one function (e.g., Extract Method and Inline Method). The key of our technique is the design of efficient vector inlining operations that emulate the effect of method inlining among code fragments, so that the problem of identifying cross-function refactoring can be reduced to the problem of finding similar vectors before and after inlining. We have implemented our technique in a prototype tool named ReDex which encodes Java programs to particular vectors. We have applied the tool to a large code base, 4.5 million lines of code, comprising of 200 bundle projects in the Eclipse ecosystem (e.g., Eclipse JDT, Eclipse PDE, Apache Commons, Hamcrest, etc.). Also, different from many other studies on detecting refactoring, ReDex only searches for code fragments that can be, but have not yet been, refactored in a way similar to some refactoring that happened in the code base. Our results show that ReDex can find 277 cross-function refactoring opportunities in 2 minutes, and 223 cases were labelled as true opportunities by users, and cover many categories of cross-function refactoring operations in classical refactoring books, such as Self Encapsulate Field, Decompose Conditional Expression, Hide Delegate, Preserve Whole Object, etc. Copyright 2014 ACM.},\nkeywords={Computer software;  Java programming language;  Software testing;  Vectors, Code fragments;  Conditional expressions;  Large code basis;  Lines of code;  Prototype tools;  Refactorings;  Software Evolution;  Vector-based representations, Codes (symbols)},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral88","name":"Automatic patch generation learned from human-written patches","authors":"Kim, D. and Nam, J. and Song, J. and Kim, S.","year":2013,"base":["Geral"],"abstract":"Patch generation is an essential software maintenance task because most software systems inevitably have bugs that need to be fixed. Unfortunately, human resources are often insufficient to fix all reported and known bugs. To address this issue, several automated patch generation techniques have been proposed. In particular, a genetic-programming-based patch generation technique, GenProg, proposed by Weimer et al., has shown promising results. However, these techniques can generate nonsensical patches due to the randomness of their mutation operations. To address this limitation, we propose a novel patch generation approach, Pattern-based Automatic program Repair (Par), using fix patterns learned from existing human-written patches. We manually inspected more than 60,000 human-written patches and found there are several common fix patterns. Our approach leverages these fix patterns to generate program patches automatically. We experimentally evaluated Par on 119 real bugs. In addition, a user study involving 89 students and 164 developers confirmed that patches generated by our approach are more acceptable than those generated by GenProg. Par successfully generated patches for 27 out of 119 bugs, while GenProg was successful for only 16 bugs. © 2013 IEEE.","doi":"10.1109/ICSE.2013.6606626","bibtex":"@CONFERENCE{Ray2013367,\nauthor={Ray, B. and Kim, M. and Person, S. and Rungta, N.},\ntitle={Detecting and characterizing semantic inconsistencies in ported code},\njournal={2013 28th IEEE/ACM International Conference on Automated Software Engineering, ASE 2013 - Proceedings},\nyear={2013},\npages={367-377},\ndoi={10.1109/ASE.2013.6693095},\nart_number={6693095},\nnote={cited By 15},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893600713&doi=10.1109%2fASE.2013.6693095&partnerID=40&md5=7404c91fba52752da40871d741d0151f},\nabstract={Adding similar features and bug fixes often requires porting program patches from reference implementations and adapting them to target implementations. Porting errors may result from faulty adaptations or inconsistent updates. This paper investigates (1) the types of porting errors found in practice, and (2) how to detect and characterize potential porting errors. Analyzing version histories, we define five categories of porting errors, including incorrect control- and data-flow, code redundancy, inconsistent identifier renamings, etc. Leveraging this categorization, we design a static control- and data-dependence analysis technique, SPA, to detect and characterize porting inconsistencies. Our evaluation on code from four open-source projects shows that SPA can detect porting inconsistencies with 65% to 73% precision and 90% recall, and identify inconsistency types with 58% to 63% precision and 92% to 100% recall. In a comparison with two existing error detection tools, SPA improves precision by 14 to 17 percentage points. © 2013 IEEE.},\nkeywords={Analysis techniques;  Code redundancy;  Error-detection tools;  Open source projects;  Percentage points;  Reference implementation;  Semantic inconsistencies;  Static control, Errors;  Semantics, Software engineering},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral89","name":"A Survey of Refactoring Detection Tools","authors":"Tan, L. and Bockisch, C.","year":2019,"base":["Geral"],"abstract":"Several tools for detecting refactorings in the code exist and have been evaluated in the literature. However, we found that the benchmarks used for the evaluation so far are incomplete and therefore, the validity of the previous evaluations is at stake. While our completed benchmark largely confirmed the previous results, in particular confirming that RefactoringMiner generally outperforms its competitors, we also identified a weak spot of RefactoringMiner that was not noted before: Refactorings of the type Move Class and Rename Package are frequently classified falsely. In this paper we discuss the reasons for this wrong classification and outline a possible fix, which potentially boosts the overall precision and recall of RefactoringMiner to over 95%. © 2019 CEUR-WS.","bibtex":"@CONFERENCE{Kim2013802,\nauthor={Kim, D. and Nam, J. and Song, J. and Kim, S.},\ntitle={Automatic patch generation learned from human-written patches},\njournal={Proceedings - International Conference on Software Engineering},\nyear={2013},\npages={802-811},\ndoi={10.1109/ICSE.2013.6606626},\nart_number={6606626},\nnote={cited By 203},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886385527&doi=10.1109%2fICSE.2013.6606626&partnerID=40&md5=685b153c2c003f703ede67ec8ccdf3a6},\nabstract={Patch generation is an essential software maintenance task because most software systems inevitably have bugs that need to be fixed. Unfortunately, human resources are often insufficient to fix all reported and known bugs. To address this issue, several automated patch generation techniques have been proposed. In particular, a genetic-programming-based patch generation technique, GenProg, proposed by Weimer et al., has shown promising results. However, these techniques can generate nonsensical patches due to the randomness of their mutation operations. To address this limitation, we propose a novel patch generation approach, Pattern-based Automatic program Repair (Par), using fix patterns learned from existing human-written patches. We manually inspected more than 60,000 human-written patches and found there are several common fix patterns. Our approach leverages these fix patterns to generate program patches automatically. We experimentally evaluated Par on 119 real bugs. In addition, a user study involving 89 students and 164 developers confirmed that patches generated by our approach are more acceptable than those generated by GenProg. Par successfully generated patches for 27 out of 119 bugs, while GenProg was successful for only 16 bugs. © 2013 IEEE.},\nkeywords={Automatic programs;  Generation techniques;  Mutation operations;  Software systems;  Software-maintenance tasks;  User study, Genetic programming;  Repair;  Software engineering, Program debugging},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral90","name":"Using Code Evolution Information to Improve the Quality of Labels in Code Smell Datasets","authors":"Wang, Y. and Hu, S. and Yin, L. and Zhou, X.","year":2018,"base":["Geral"],"abstract":"Several approaches are proposed to detect code smells. A set of important approaches are based on machine learning algorithms, which require the code smells have been labeled in source codes as training data firstly. The common labeling approaches are based on manual or tools, but it is difficult for current approaches to get reliable large-scale datasets. In this paper, an approach using the evolution information of source codes is proposed to get large-scale and more reliable training datasets for detecting code smells based on machine learning algorithms. Our approach analyzes the evolving of the source code smells firstly labeled by a tool from the baseline version into the contrastive version of a software system, and then constructs training datasets based on those 'changed smells'. Experiments conducted on three open source software projects for detecting four types of code smells(which are Data Class, God Class, Brain Class and Brain Method) show that the models obtained by changed smells datasets have better performance on code smell detection than those obtained by unchanged smells datasets (with an average improvement rate of 7.8% and a maximum increase of 30%). The experiments results indicate that using the evolution information of source codes can construct more reliable training datasets for detecting code smells based on machine learning algorithms. © 2018 IEEE.","doi":"10.1109/COMPSAC.2018.00015","bibtex":"@CONFERENCE{Tan2019100,\nauthor={Tan, L. and Bockisch, C.},\ntitle={A Survey of Refactoring Detection Tools},\njournal={CEUR Workshop Proceedings},\nyear={2019},\nvolume={2308},\npages={100-105},\nnote={cited By 0},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061841732&partnerID=40&md5=612dba5f487183e718d91ddb10f440d4},\nabstract={Several tools for detecting refactorings in the code exist and have been evaluated in the literature. However, we found that the benchmarks used for the evaluation so far are incomplete and therefore, the validity of the previous evaluations is at stake. While our completed benchmark largely confirmed the previous results, in particular confirming that RefactoringMiner generally outperforms its competitors, we also identified a weak spot of RefactoringMiner that was not noted before: Refactorings of the type Move Class and Rename Package are frequently classified falsely. In this paper we discuss the reasons for this wrong classification and outline a possible fix, which potentially boosts the overall precision and recall of RefactoringMiner to over 95%. © 2019 CEUR-WS.},\nkeywords={Detection tools;  Move method;  Precision and recall;  Refactoringminer;  Refactorings;  Reproduction study, Software engineering},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral91","name":"A systematic mapping of literature on software refactoring tools [Um Mapeamento Sistemático da Literatura sobre Ferramentas de Refatoração de Software]","authors":"Tavares, C.S. and Ferreira, F. and Figueiredo, E.","year":2018,"base":["Geral"],"abstract":"Refactoring consists of improving the internal structure of the code without changing the external behavior of a software system. However, the task of refactoring is very costly in the development of an information system. Thus, many tools have been proposed to support refactoring the source code. In order to find tools cited in the literature, this work presents a Systematic Literature Mapping about refactoring. As a result, this paper summarizes the refactoring tools that have been published in the last 5 years in terms of the tool profiles developed, which programming languages have support for refactoring and which are the main refactoring strategies that are handled by tools. It has been identified that publications on refactoring have remained constant over the past 5 years. Also, most of the refactoring works describe tools, being they for systems written in the Java language, that perform code refactoring automatically and the main refactorings are: Move Method, Pull Up Method, Extract Class and Code Clone. Finally, we performed an analysis of the data returned by the DBLP library. As a result, it was observed that the papers returned by the DBLP have a high level of similarity with the other research bases studied. © 2018 Association for Computing Machinery.","doi":"10.1145/3229345.3229357","bibtex":"@CONFERENCE{Wang201848,\nauthor={Wang, Y. and Hu, S. and Yin, L. and Zhou, X.},\ntitle={Using Code Evolution Information to Improve the Quality of Labels in Code Smell Datasets},\njournal={Proceedings - International Computer Software and Applications Conference},\nyear={2018},\nvolume={1},\npages={48-53},\ndoi={10.1109/COMPSAC.2018.00015},\nart_number={8377639},\nnote={cited By 0},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055446660&doi=10.1109%2fCOMPSAC.2018.00015&partnerID=40&md5=1f214cb0dd32cce58b9061f6e84b8e89},\nabstract={Several approaches are proposed to detect code smells. A set of important approaches are based on machine learning algorithms, which require the code smells have been labeled in source codes as training data firstly. The common labeling approaches are based on manual or tools, but it is difficult for current approaches to get reliable large-scale datasets. In this paper, an approach using the evolution information of source codes is proposed to get large-scale and more reliable training datasets for detecting code smells based on machine learning algorithms. Our approach analyzes the evolving of the source code smells firstly labeled by a tool from the baseline version into the contrastive version of a software system, and then constructs training datasets based on those 'changed smells'. Experiments conducted on three open source software projects for detecting four types of code smells(which are Data Class, God Class, Brain Class and Brain Method) show that the models obtained by changed smells datasets have better performance on code smell detection than those obtained by unchanged smells datasets (with an average improvement rate of 7.8% and a maximum increase of 30%). The experiments results indicate that using the evolution information of source codes can construct more reliable training datasets for detecting code smells based on machine learning algorithms. © 2018 IEEE.},\nkeywords={Application programs;  Artificial intelligence;  Codes (symbols);  Computer programming languages;  Information use;  Learning systems;  Odors;  Open source software;  Open systems, Code smell;  Large-scale datasets;  Open source software projects;  Refactorings;  Software systems;  Training data;  Training data sets;  Training dataset, Learning algorithms},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral92","name":"APIDiff: Detecting API breaking changes","authors":"Brito, A. and Xavier, L. and Hora, A. and Valente, M.T.","year":2018,"base":["Geral"],"abstract":"Libraries are commonly used to increase productivity. As most software systems, they evolve over time and changes are required. However, this process may involve breaking compatibility with previous versions, leading clients to fail. In this context, it is important that libraries creators and clients frequently assess API stability in order to better support their maintenance practices. In this paper, we introduce APIDIFF, a tool to identify API breaking and non-breaking changes between two versions of a Java library. The tool detects changes on three API elements: types, methods, and fields. We also report usage scenarios of APIDIFF with four real-world Java libraries. © 2018 IEEE.","doi":"10.1109/SANER.2018.8330249","bibtex":"@CONFERENCE{Tavares201881,\nauthor={Tavares, C.S. and Ferreira, F. and Figueiredo, E.},\ntitle={A systematic mapping of literature on software refactoring tools [Um Mapeamento Sistemático da Literatura sobre Ferramentas de Refatoração de Software]},\njournal={ACM International Conference Proceeding Series},\nyear={2018},\npages={81-88},\ndoi={10.1145/3229345.3229357},\nnote={cited By 0},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060041285&doi=10.1145%2f3229345.3229357&partnerID=40&md5=5bc897f0e4336f4df00a3a2cbc4112f4},\nabstract={Refactoring consists of improving the internal structure of the code without changing the external behavior of a software system. However, the task of refactoring is very costly in the development of an information system. Thus, many tools have been proposed to support refactoring the source code. In order to find tools cited in the literature, this work presents a Systematic Literature Mapping about refactoring. As a result, this paper summarizes the refactoring tools that have been published in the last 5 years in terms of the tool profiles developed, which programming languages have support for refactoring and which are the main refactoring strategies that are handled by tools. It has been identified that publications on refactoring have remained constant over the past 5 years. Also, most of the refactoring works describe tools, being they for systems written in the Java language, that perform code refactoring automatically and the main refactorings are: Move Method, Pull Up Method, Extract Class and Code Clone. Finally, we performed an analysis of the data returned by the DBLP library. As a result, it was observed that the papers returned by the DBLP have a high level of similarity with the other research bases studied. © 2018 Association for Computing Machinery.},\nkeywords={Information systems;  Information use;  Mapping, Code re-factoring;  External behavior;  Internal structure;  Java language;  Refactoring tools;  Software refactoring;  Software systems;  Systematic mapping, Codes (symbols)},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral93","name":"Why and how Java developers break APIs","authors":"Brito, A. and Xavier, L. and Hora, A. and Valente, M.T.","year":2018,"base":["Geral"],"abstract":"Modern software development depends on APIs to reuse code and increase productivity. As most software systems, these libraries and frameworks also evolve, which may break existing clients. However, the main reasons to introduce breaking changes in APIs are unclear. Therefore, in this paper, we report the results of an almost 4-month long field study with the developers of 400 popular Java libraries and frameworks. We configured an infrastructure to observe all changes in these libraries and to detect breaking changes shortly after their introduction in the code. After identifying breaking changes, we asked the developers to explain the reasons behind their decision to change the APIs. During the study, we identified 59 breaking changes, confirmed by the developers of 19 projects. By analyzing the developers' answers, we report that breaking changes are mostly motivated by the need to implement new features, by the desire to make the APIs simpler and with fewer elements, and to improve maintainability. We conclude by providing suggestions to language designers, tool builders, software engineering researchers and API developers. © 2018 IEEE.","doi":"10.1109/SANER.2018.8330214","bibtex":"@CONFERENCE{Brito2018507,\nauthor={Brito, A. and Xavier, L. and Hora, A. and Valente, M.T.},\ntitle={APIDiff: Detecting API breaking changes},\njournal={25th IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2018 - Proceedings},\nyear={2018},\nvolume={2018-March},\npages={507-511},\ndoi={10.1109/SANER.2018.8330249},\nnote={cited By 1},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050975257&doi=10.1109%2fSANER.2018.8330249&partnerID=40&md5=83d5929cd83f6627dea22a5e51149fae},\nabstract={Libraries are commonly used to increase productivity. As most software systems, they evolve over time and changes are required. However, this process may involve breaking compatibility with previous versions, leading clients to fail. In this context, it is important that libraries creators and clients frequently assess API stability in order to better support their maintenance practices. In this paper, we introduce APIDIFF, a tool to identify API breaking and non-breaking changes between two versions of a Java library. The tool detects changes on three API elements: types, methods, and fields. We also report usage scenarios of APIDIFF with four real-world Java libraries. © 2018 IEEE.},\nkeywords={Java programming language;  Libraries;  Reengineering, API Evolution;  Breaking Changes;  Java library;  Maintenance practices;  Mining software repositories;  Real-world;  Software systems;  Usage scenarios, Application programming interfaces (API)},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral94","name":"Simulating software refactorings based on graph transformations","authors":"Honsel, D. and Fiekas, N. and Herbold, V. and Welter, M. and Ahlbrecht, T. and Waack, S. and Dix, J. and Grabowski, J.","year":2018,"base":["Geral"],"abstract":"We aim to simulate software processes in order to predict the structural evolution of software graphs and assure higher software quality. To make our simulation and therefore the results more accurate, we need to model real world practices. In this paper, we consider the specific problem of including software refactorings in our simulation. We describe these refactorings as graph transformations and apply parameters we collected from open source projects. © Springer Nature Switzerland AG 2018.","doi":"10.1007/978-3-319-96271-9_10","bibtex":"@CONFERENCE{Brito2018255,\nauthor={Brito, A. and Xavier, L. and Hora, A. and Valente, M.T.},\ntitle={Why and how Java developers break APIs},\njournal={25th IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2018 - Proceedings},\nyear={2018},\nvolume={2018-March},\npages={255-265},\ndoi={10.1109/SANER.2018.8330214},\nart_number={8330214},\nnote={cited By 2},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050964632&doi=10.1109%2fSANER.2018.8330214&partnerID=40&md5=3b5660a7a94702544d95b57772bc2b57},\nabstract={Modern software development depends on APIs to reuse code and increase productivity. As most software systems, these libraries and frameworks also evolve, which may break existing clients. However, the main reasons to introduce breaking changes in APIs are unclear. Therefore, in this paper, we report the results of an almost 4-month long field study with the developers of 400 popular Java libraries and frameworks. We configured an infrastructure to observe all changes in these libraries and to detect breaking changes shortly after their introduction in the code. After identifying breaking changes, we asked the developers to explain the reasons behind their decision to change the APIs. During the study, we identified 59 breaking changes, confirmed by the developers of 19 projects. By analyzing the developers' answers, we report that breaking changes are mostly motivated by the need to implement new features, by the desire to make the APIs simpler and with fewer elements, and to improve maintainability. We conclude by providing suggestions to language designers, tool builders, software engineering researchers and API developers. © 2018 IEEE.},\nkeywords={Computer software reusability;  Libraries;  Reengineering;  Software design, API Evolution;  Breaking Changes;  Field studies;  Java developers;  Java library;  Software systems, Java programming language},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral95","name":"Dynamic Ranking of Refactoring Menu Items for Integrated Development Environment","authors":"Oo, T. and Liu, H. and Nyirongo, B.","year":2018,"base":["Geral"],"abstract":"Software refactoring is popular and thus most mainstream IDEs, e.g., Eclipse, provide a top level menu, especially for refactoring activities. The refactoring menu is designed to facilitate refactorings, and it has become one of the most commonly used menus. However, to support a large number of refactoring types, the refactoring menu contains a long list of menu items. As a result, it is tedious to select the intended menu item from the lengthy menu. To facilitate the menu selection, in this paper, we propose an approach to dynamic ranking of refactoring menu items for IDE. We put the most likely refactoring menu item on the top of the refactoring menu according to developers' source code selection and code smells associated with the selected source code. The ranking is dynamic because it changes frequently according to the context. First, we collect the refactoring history of the open source applications and detect the code smells. Based on the refactoring history, we design questionnaires and analyze the responses from developers to discover the source code selection patterns for different refactoring types. Subsequently, we analyze the relationship between code smells associated with the refactoring software entities and the corresponding refactoring types. Finally, based on the preceding analysis, we calculate the likelihood of different refactoring types to be applied when a specific part of source code is selected, and rank the menu items according to the resulting likelihood. We conduct a case study to evaluate the proposed approach. Evaluation results suggest that the proposed approach is accurate, and in most cases (95.69%), it can put the intended refactoring menu item on the top of the menu. © 2013 IEEE.","doi":"10.1109/ACCESS.2018.2883769","bibtex":"@ARTICLE{Honsel2018161,\nauthor={Honsel, D. and Fiekas, N. and Herbold, V. and Welter, M. and Ahlbrecht, T. and Waack, S. and Dix, J. and Grabowski, J.},\ntitle={Simulating software refactorings based on graph transformations},\njournal={Communications in Computer and Information Science},\nyear={2018},\nvolume={889},\npages={161-175},\ndoi={10.1007/978-3-319-96271-9_10},\nnote={cited By 0},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052070893&doi=10.1007%2f978-3-319-96271-9_10&partnerID=40&md5=a7c2e2098b93048b455a2aaf8c0bf1e9},\nabstract={We aim to simulate software processes in order to predict the structural evolution of software graphs and assure higher software quality. To make our simulation and therefore the results more accurate, we need to model real world practices. In this paper, we consider the specific problem of including software refactorings in our simulation. We describe these refactorings as graph transformations and apply parameters we collected from open source projects. © Springer Nature Switzerland AG 2018.},\nkeywords={Computer software selection and evaluation;  Graph theory, Graph Transformation;  Open source projects;  Real-world practice;  Refactorings;  Software process;  Software Quality;  Specific problems;  Structural evolution, Open source software},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral96","name":"Do null-type mutation operators help prevent null-type faults?","authors":"Parsai, A. and Demeyer, S.","year":2019,"base":["Geral"],"abstract":"The null-type is a major source of faults in Java programs, and its overuse has a severe impact on software maintenance. Unfortunately traditional mutation testing operators do not cover null-type faults by default, hence cannot be used as a preventive measure. We address this problem by designing four new mutation operators which model null-type faults explicitly. We show how these mutation operators are capable of revealing the missing tests, and we demonstrate that these mutation operators are useful in practice. For the latter, we analyze the test suites of 15 open-source projects to describe the trade-offs related to the adoption of these operators to strengthen the test suite. © 2019, Springer Nature Switzerland AG.","doi":"10.1007/978-3-030-10801-4_33","bibtex":"@ARTICLE{Oo201876025,\nauthor={Oo, T. and Liu, H. and Nyirongo, B.},\ntitle={Dynamic Ranking of Refactoring Menu Items for Integrated Development Environment},\njournal={IEEE Access},\nyear={2018},\nvolume={6},\npages={76025-76035},\ndoi={10.1109/ACCESS.2018.2883769},\nart_number={8552339},\nnote={cited By 0},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057887721&doi=10.1109%2fACCESS.2018.2883769&partnerID=40&md5=ab8d1015283043b0ce46f6f453d4d8c0},\nabstract={Software refactoring is popular and thus most mainstream IDEs, e.g., Eclipse, provide a top level menu, especially for refactoring activities. The refactoring menu is designed to facilitate refactorings, and it has become one of the most commonly used menus. However, to support a large number of refactoring types, the refactoring menu contains a long list of menu items. As a result, it is tedious to select the intended menu item from the lengthy menu. To facilitate the menu selection, in this paper, we propose an approach to dynamic ranking of refactoring menu items for IDE. We put the most likely refactoring menu item on the top of the refactoring menu according to developers' source code selection and code smells associated with the selected source code. The ranking is dynamic because it changes frequently according to the context. First, we collect the refactoring history of the open source applications and detect the code smells. Based on the refactoring history, we design questionnaires and analyze the responses from developers to discover the source code selection patterns for different refactoring types. Subsequently, we analyze the relationship between code smells associated with the refactoring software entities and the corresponding refactoring types. Finally, based on the preceding analysis, we calculate the likelihood of different refactoring types to be applied when a specific part of source code is selected, and rank the menu items according to the resulting likelihood. We conduct a case study to evaluate the proposed approach. Evaluation results suggest that the proposed approach is accurate, and in most cases (95.69%), it can put the intended refactoring menu item on the top of the menu. © 2013 IEEE.},\nkeywords={Codes (symbols);  Computer programming languages;  History;  Integrodifferential equations;  Learning systems;  Odors;  Open systems;  Software design;  Software engineering;  Surveys;  Tools;  User interfaces, Crawlers;  Evaluation results;  Integrated development environment;  Java;  Menu Ranking;  Open source application;  Software entities;  Software refactoring, Open source software},\ndocument_type={Article},\nsource={Scopus},\n}\n\n"},{"id":"Geral97","name":"On the characteristics of buggy code clones: A code quality perspective","authors":"Islam, M.R. and Zibran, M.F.","year":2018,"base":["Geral"],"abstract":"Code clone is an immensely studied code smell. Not all the clones in a software system are equally harmful. Earlier work studied various traits of clones including their stability and relationships with program faults against non-cloned code. This paper presents a comparative study on the characteristics of buggy and non-buggy clones from a code quality perspective. In the light of 29 code quality metrics, we study buggy and non-buggy clones in 2,077 revisions of three software systems written in Java. The findings from this work add to the characterization of buggy clones. Such a characterization will be useful in cost-effective clone management and clone-aware software development. © 2018 IEEE.","doi":"10.1109/IWSC.2018.8327315","bibtex":"@ARTICLE{Parsai2019419,\nauthor={Parsai, A. and Demeyer, S.},\ntitle={Do null-type mutation operators help prevent null-type faults?},\njournal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},\nyear={2019},\nvolume={11376 LNCS},\npages={419-434},\ndoi={10.1007/978-3-030-10801-4_33},\nnote={cited By 0},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062322203&doi=10.1007%2f978-3-030-10801-4_33&partnerID=40&md5=0fd6c2a43cf9ed71388bf04ec228b189},\nabstract={The null-type is a major source of faults in Java programs, and its overuse has a severe impact on software maintenance. Unfortunately traditional mutation testing operators do not cover null-type faults by default, hence cannot be used as a preventive measure. We address this problem by designing four new mutation operators which model null-type faults explicitly. We show how these mutation operators are capable of revealing the missing tests, and we demonstrate that these mutation operators are useful in practice. For the latter, we analyze the test suites of 15 open-source projects to describe the trade-offs related to the adoption of these operators to strengthen the test suite. © 2019, Springer Nature Switzerland AG.},\nkeywords={Computer software maintenance;  Economic and social effects;  Open source software, Java program;  Mutation operators;  Mutation testing;  Null type;  Open source projects;  Preventive measures;  Test quality;  Trade off, Software testing},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral98","name":"Towards API-specific automatic program repair","authors":"Nielebock, S.","year":2017,"base":["Geral"],"abstract":"The domain of Automatic Program Repair (APR) had many research contributions in recent years. So far, most approaches target fixing generic bugs in programs (e.g., off-by-one errors). Nevertheless, recent studies reveal that about 50% of real bugs require API-specific fixes (e.g., adding missing API method calls or correcting method ordering), for which existing APR approaches are not designed. In this paper, we address this problem and introduce the notion of an API-specific program repair mechanism. This mechanism detects erroneous code in a similar way to existing APR approaches. However, to fix such bugs, it uses API-specific information from the erroneous code to search for API usage patterns in other software, with which we could fix the bug. We provide first insights on the applicability of this mechanism and discuss upcoming research challenges. © 2017 IEEE.","doi":"10.1109/ASE.2017.8115721","bibtex":"@CONFERENCE{Islam201823,\nauthor={Islam, M.R. and Zibran, M.F.},\ntitle={On the characteristics of buggy code clones: A code quality perspective},\njournal={2018 IEEE 12th International Workshop on Software Clones, IWSC 2018 - Proceedings},\nyear={2018},\nvolume={2018-January},\npages={23-29},\ndoi={10.1109/IWSC.2018.8327315},\nnote={cited By 0},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049689512&doi=10.1109%2fIWSC.2018.8327315&partnerID=40&md5=6a1aa84c107419a2b856199395e2ae17},\nabstract={Code clone is an immensely studied code smell. Not all the clones in a software system are equally harmful. Earlier work studied various traits of clones including their stability and relationships with program faults against non-cloned code. This paper presents a comparative study on the characteristics of buggy and non-buggy clones from a code quality perspective. In the light of 29 code quality metrics, we study buggy and non-buggy clones in 2,077 revisions of three software systems written in Java. The findings from this work add to the characterization of buggy clones. Such a characterization will be useful in cost-effective clone management and clone-aware software development. © 2018 IEEE.},\nkeywords={Codes (symbols);  Computer software;  Cost effectiveness;  Software design, Clone management;  Code clone;  Code quality;  Code smell;  Comparative studies;  Cost effective;  Software systems, Cloning},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral99","name":"Harvesting the Wisdom of the Crowd to Infer Method Nullness in Java","authors":"Leuenberger, M. and Osman, H. and Ghafari, M. and Nierstrasz, O.","year":2017,"base":["Geral"],"abstract":"Null pointer exceptions are common bugs in Java projects. Previous research has shown that dereferencing the results of method calls is the main source of these bugs, as developers do not anticipate that some methods return null. To make matters worse, we find that whether a method returns null or not (nullness), is rarely documented. We argue that method nullness is a vital piece of information that can help developers avoid this category of bugs. This is especially important for external APIs where developers may not even have access to the code.,In this paper, we study the method nullness of Apache Lucene, the de facto standard library for text processing in Java. Particularly, we investigate how often the result of each Lucene method is checked against null in Lucene clients. We call this measure method nullability, which can serve as a proxy for method nullness. Analyzing Lucene internal and external usage, we find that most methods are never checked for null. External clients check more methods than Lucene checks internally. Manually inspecting our dataset reveals that some null checks are unnecessary. We present an IDE plugin that complements existing documentation and makes up for missing documentation regarding method nullness and generates nullness annotations, so that static analysis can pinpoint potentially missing or unnecessary null checks. © 2017 IEEE.","doi":"10.1109/SCAM.2017.22","bibtex":"@CONFERENCE{Nielebock20171010,\nauthor={Nielebock, S.},\ntitle={Towards API-specific automatic program repair},\njournal={ASE 2017 - Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering},\nyear={2017},\npages={1010-1013},\ndoi={10.1109/ASE.2017.8115721},\nart_number={8115721},\nnote={cited By 0},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041446664&doi=10.1109%2fASE.2017.8115721&partnerID=40&md5=e0a1bc4e46a709a9cf9b2e1534377cb7},\nabstract={The domain of Automatic Program Repair (APR) had many research contributions in recent years. So far, most approaches target fixing generic bugs in programs (e.g., off-by-one errors). Nevertheless, recent studies reveal that about 50% of real bugs require API-specific fixes (e.g., adding missing API method calls or correcting method ordering), for which existing APR approaches are not designed. In this paper, we address this problem and introduce the notion of an API-specific program repair mechanism. This mechanism detects erroneous code in a similar way to existing APR approaches. However, to fix such bugs, it uses API-specific information from the erroneous code to search for API usage patterns in other software, with which we could fix the bug. We provide first insights on the applicability of this mechanism and discuss upcoming research challenges. © 2017 IEEE.},\nkeywords={Application programming interfaces (API);  Repair;  Software engineering, API-specific Bugs;  Automatic programs;  Repair mechanism;  Research challenges;  Specific information;  Specification mining;  Usage patterns, Program debugging},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral100","name":"LittleDarwin: A Feature-Rich and Extensible Mutation Testing Framework for Large and Complex Java Systems","authors":"Parsai, A. and Murgia, A. and Demeyer, S.","year":2017,"base":["Geral"],"abstract":"Mutation testing is a well-studied method for increasing the quality of a test suite. We designed LittleDarwin as a mutation testing framework able to cope with large and complex Java software systems, while still being easily extensible with new experimental components. LittleDarwin addresses two existing problems in the domain of mutation testing: having a tool able to work within an industrial setting, and yet, be open to extension for cutting edge techniques provided by academia. LittleDarwin already offers higher-order mutation, null type mutants, mutant sampling, manual mutation, and mutant subsumption analysis. There is no tool today available with all these features that is able to work with typical industrial software systems. © 2017, IFIP International Federation for Information Processing.","doi":"10.1007/978-3-319-68972-2_10","bibtex":"@CONFERENCE{Leuenberger201771,\nauthor={Leuenberger, M. and Osman, H. and Ghafari, M. and Nierstrasz, O.},\ntitle={Harvesting the Wisdom of the Crowd to Infer Method Nullness in Java},\njournal={Proceedings - 2017 IEEE 17th International Working Conference on Source Code Analysis and Manipulation, SCAM 2017},\nyear={2017},\nvolume={2017-October},\npages={71-80},\ndoi={10.1109/SCAM.2017.22},\nnote={cited By 1},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040622329&doi=10.1109%2fSCAM.2017.22&partnerID=40&md5=ee3e32b92c05b0c439c4553f2aec5160},\nabstract={Null pointer exceptions are common bugs in Java projects. Previous research has shown that dereferencing the results of method calls is the main source of these bugs, as developers do not anticipate that some methods return null. To make matters worse, we find that whether a method returns null or not (nullness), is rarely documented. We argue that method nullness is a vital piece of information that can help developers avoid this category of bugs. This is especially important for external APIs where developers may not even have access to the code.,In this paper, we study the method nullness of Apache Lucene, the de facto standard library for text processing in Java. Particularly, we investigate how often the result of each Lucene method is checked against null in Lucene clients. We call this measure method nullability, which can serve as a proxy for method nullness. Analyzing Lucene internal and external usage, we find that most methods are never checked for null. External clients check more methods than Lucene checks internally. Manually inspecting our dataset reveals that some null checks are unnecessary. We present an IDE plugin that complements existing documentation and makes up for missing documentation regarding method nullness and generates nullness annotations, so that static analysis can pinpoint potentially missing or unnecessary null checks. © 2017 IEEE.},\nkeywords={Application programming interfaces (API);  Codes (symbols);  Static analysis;  Text processing, Apache Lucene;  De facto standard;  Management systems;  null pointer exceptions;  Plug-ins;  Usage analysis;  Wisdom of the crowds, Java programming language},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral101","name":"Identifying the Exact Bug Fixing Actions","authors":"Oumarou, H. and Anquetil, N. and Etien, A. and Ducasse, S. and Taiwe, K.D.","year":2016,"base":["Geral"],"abstract":"In-spite of the principle of good programming practice which stipulates that a commit should include only modifications belonging to one task, programmers submit tangled commits consisting of modifications related to two or several distinct tasks. Some researches show that between 11 and 39% of bug fix commits are tangled and at least 16.6% of all the commits are incorrectly associated to bug reports. Tangled commits make historical analysis of the project less reliable, and impede the extraction of bug fix pattern because most mining software repository techniques are designed with the assumption that each commit includes only modifications for a single task. In this paper, we are proposing a method that identifies precisely the modifications that are related to a bug fix. We validate our solution on a real world system with real bugs and bug-fixes. © 2016 IEEE.","doi":"10.1109/IWESEP.2016.13","bibtex":"@ARTICLE{Parsai2017148,\nauthor={Parsai, A. and Murgia, A. and Demeyer, S.},\ntitle={LittleDarwin: A Feature-Rich and Extensible Mutation Testing Framework for Large and Complex Java Systems},\njournal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},\nyear={2017},\nvolume={10522 LNCS},\npages={148-163},\ndoi={10.1007/978-3-319-68972-2_10},\nnote={cited By 4},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032862315&doi=10.1007%2f978-3-319-68972-2_10&partnerID=40&md5=806a7b502a1d5e2d0e5847f8465040d4},\nabstract={Mutation testing is a well-studied method for increasing the quality of a test suite. We designed LittleDarwin as a mutation testing framework able to cope with large and complex Java software systems, while still being easily extensible with new experimental components. LittleDarwin addresses two existing problems in the domain of mutation testing: having a tool able to work within an industrial setting, and yet, be open to extension for cutting edge techniques provided by academia. LittleDarwin already offers higher-order mutation, null type mutants, mutant sampling, manual mutation, and mutant subsumption analysis. There is no tool today available with all these features that is able to work with typical industrial software systems. © 2017, IFIP International Federation for Information Processing.},\nkeywords={Computer software;  Java programming language;  Software engineering, Cutting edges;  Existing problems;  Higher-order;  Industrial settings;  Industrial software;  Java software;  Java system;  Mutation testing, Software testing},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral102","name":"Collect, decompile, extract, stats, and diff: Mining design pattern changes in android apps","authors":"Alharbi, K. and Yeh, T.","year":2015,"base":["Geral"],"abstract":"Mobile user interface design patterns have been widely used across different mobile platforms. UI design patterns have evolved and changed significantly as new trends emerge and fade at different times. This paper presents a data-mining approach to analyzing design pattern changes in Android apps. Over a period of 18 months, we tracked 24,436 apps and collected their versions. In total, our sample consists of 56,349 unique app versions, more than 5 million source files, and more than 25 million UI elements. We developed a dedicated infrastructure based on modern big data technologies to support our differential analyses regarding design pattern changes. Some highlights of our findings include a) some apps would switch to a design pattern even after it was deprecated, b) the adoption rate of newly introduced design patterns (e.g., Fragment) is relatively low, c) some apps would update their listing details to reflect changes in design patterns. © 2015 ACM.","doi":"10.1145/2785830.2785892","bibtex":"@CONFERENCE{Oumarou201651,\nauthor={Oumarou, H. and Anquetil, N. and Etien, A. and Ducasse, S. and Taiwe, K.D.},\ntitle={Identifying the Exact Bug Fixing Actions},\njournal={Proceedings - 7th International Workshop on Empirical Software Engineering in Practice, IWESEP 2016},\nyear={2016},\npages={51-56},\ndoi={10.1109/IWESEP.2016.13},\nart_number={7464553},\nnote={cited By 0},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971472391&doi=10.1109%2fIWESEP.2016.13&partnerID=40&md5=1598198c00a2a9c4f6b0c190cedd31c8},\nabstract={In-spite of the principle of good programming practice which stipulates that a commit should include only modifications belonging to one task, programmers submit tangled commits consisting of modifications related to two or several distinct tasks. Some researches show that between 11 and 39% of bug fix commits are tangled and at least 16.6% of all the commits are incorrectly associated to bug reports. Tangled commits make historical analysis of the project less reliable, and impede the extraction of bug fix pattern because most mining software repository techniques are designed with the assumption that each commit includes only modifications for a single task. In this paper, we are proposing a method that identifies precisely the modifications that are related to a bug fix. We validate our solution on a real world system with real bugs and bug-fixes. © 2016 IEEE.},\nkeywords={Costs;  Software engineering, Bug fixes;  Bug reports;  Bug-fixing;  Historical analysis;  Mining software repositories;  Programming practices;  Real-world system, Program debugging},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral103","name":"Understanding the Software Fault Introduction Process","authors":"Inozemtseva, L.","year":2015,"base":["Geral"],"abstract":"Testing and debugging research revolves around faults, yet we have a limited understanding of the processes by which faults are introduced and removed. Previous work in this area has focused on describing faults rather than explaining the introduction and removal processes, meaning that a great deal of testing and debugging research depends on assumptions that have not been empirically validated. We propose a three-phase project to develop an explanatory theory of the fault introduction process and describe how the project will be completed. © 2015 IEEE.","doi":"10.1109/ICSE.2015.274","bibtex":"@CONFERENCE{Alharbi2015515,\nauthor={Alharbi, K. and Yeh, T.},\ntitle={Collect, decompile, extract, stats, and diff: Mining design pattern changes in android apps},\njournal={MobileHCI 2015 - Proceedings of the 17th International Conference on Human-Computer Interaction with Mobile Devices and Services},\nyear={2015},\npages={515-524},\ndoi={10.1145/2785830.2785892},\nnote={cited By 10},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959419451&doi=10.1145%2f2785830.2785892&partnerID=40&md5=df2828957d8450e1c6a7c91a9a5ca925},\nabstract={Mobile user interface design patterns have been widely used across different mobile platforms. UI design patterns have evolved and changed significantly as new trends emerge and fade at different times. This paper presents a data-mining approach to analyzing design pattern changes in Android apps. Over a period of 18 months, we tracked 24,436 apps and collected their versions. In total, our sample consists of 56,349 unique app versions, more than 5 million source files, and more than 25 million UI elements. We developed a dedicated infrastructure based on modern big data technologies to support our differential analyses regarding design pattern changes. Some highlights of our findings include a) some apps would switch to a design pattern even after it was deprecated, b) the adoption rate of newly introduced design patterns (e.g., Fragment) is relatively low, c) some apps would update their listing details to reflect changes in design patterns. © 2015 ACM.},\nkeywords={Application programs;  Big data;  Data mining;  Design;  Human computer interaction;  Mobile devices;  User interfaces, Analysis;  Android;  Data technologies;  Design Patterns;  Differential analysis;  Mobile platform;  Mobile user interface;  Pattern, Android (operating system)},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral104","name":"Recurrent bug fixing: Keshmesh and Naive Bayes","authors":"Chauhan, P.","year":2015,"base":["Geral"],"abstract":"The bug-free programming has unendingly been a dream to chase for programmers and developers thus inescapable is the presence of bugs which advances with development of the software. Problems being faced by programmers are analyzed by characterizing the matter followed by frequency analysis of bugs being similar over different frameworks which is another significant issue to unwind. Only then will bugs be positioned in hierarchal order of commonest to least common. We worked on recurring Concurrent Bug patterns. Concurrency bug patterns (CBPs) are a classification of bug examples specific to concurrent software. More and more software engineers are developing concurrent programming software to acquire effectiveness of multi core architecture. However another obstacle in creating concurrent programming is the presence of practically identical bugs(aka recurrent bugs) showing up over and over. In spite of the fact that frequency of a bugs does not generally indicate errors in programming but rather it leads to future bugs as programming grows. This build up a need to develop tools and techniques to automatically extract and use the already available information within the software revision histories and code to make future bug correction predictions. In this paper we propose the Naïve Bayes classifier to extend the functionality provided by Keshmesh, a tool to detect and fix the concurrent bugs. Naïve Bayes plays a major role in identifying more number of bugs as found by Keshmesh alone. Finding and repairing bugs in software is extremely crucial for the software to keep it stable and least vulnerable to future bugs. © Research India Publications.","bibtex":"@CONFERENCE{Inozemtseva2015843,\nauthor={Inozemtseva, L.},\ntitle={Understanding the Software Fault Introduction Process},\njournal={Proceedings - International Conference on Software Engineering},\nyear={2015},\nvolume={2},\npages={843-846},\ndoi={10.1109/ICSE.2015.274},\nart_number={7203095},\nnote={cited By 0},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951777712&doi=10.1109%2fICSE.2015.274&partnerID=40&md5=2c7fdeb4baf234ad6b4f8ab8756b76a3},\nabstract={Testing and debugging research revolves around faults, yet we have a limited understanding of the processes by which faults are introduced and removed. Previous work in this area has focused on describing faults rather than explaining the introduction and removal processes, meaning that a great deal of testing and debugging research depends on assumptions that have not been empirically validated. We propose a three-phase project to develop an explanatory theory of the fault introduction process and describe how the project will be completed. © 2015 IEEE.},\nkeywords={Computer debugging;  Computer software;  Engineering research;  Software engineering;  Software testing;  Testing, Fault introduction;  Removal process;  Software fault;  Testing and debugging;  Three-phase projects, Program debugging},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral105","name":"Modern code reviews in open-source projects: Which problems do they fix?","authors":"Beller, M. and Bacchelli, A. and Zaidman, A. and Juergens, E.","year":2014,"base":["Geral"],"abstract":"Code review is the manual assessment of source code by humans, mainly intended to identify defects and quality problems. Modern Code Review (MCR), a lightweight variant of the code inspections investigated since the 1970s, prevails today both in industry and open-source software (OSS) systems. The objective of this paper is to increase our understanding of the practical benefits that the MCR process produces on reviewed source code. To that end, we empirically explore the problems fixed through MCR in OSS systems. We manually classified over 1,400 changes taking place in reviewed code from two OSS projects into a validated categorization scheme. Surprisingly, results show that the types of changes due to the MCR process in OSS are strikingly similar to those in the industry and academic systems from literature, featuring the similar 75:25 ratio of maintainability-related to functional problems. We also reveal that 7-35% of review comments are discarded and that 10-22% of the changes are not triggered by an explicit review comment. Patterns emerged in the review data; we investigated them revealing the technical factors that influence the number of changes due to the MCR process. We found that bug-fixing tasks lead to fewer changes and tasks with more altered files and a higher code churn have more changes. Contrary to intuition, the person of the reviewer had no impact on the number of changes. Copyright is held by the author/owner(s). Publication rights licensed to ACM.","doi":"10.1145/2597073.2597082","bibtex":"@ARTICLE{Chauhan201535203,\nauthor={Chauhan, P.},\ntitle={Recurrent bug fixing: Keshmesh and Naive Bayes},\njournal={International Journal of Applied Engineering Research},\nyear={2015},\nvolume={10},\nnumber={15},\npages={35203-35208},\nnote={cited By 0},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941131005&partnerID=40&md5=5cde4b0996ad145482ff10a112cf925c},\nabstract={The bug-free programming has unendingly been a dream to chase for programmers and developers thus inescapable is the presence of bugs which advances with development of the software. Problems being faced by programmers are analyzed by characterizing the matter followed by frequency analysis of bugs being similar over different frameworks which is another significant issue to unwind. Only then will bugs be positioned in hierarchal order of commonest to least common. We worked on recurring Concurrent Bug patterns. Concurrency bug patterns (CBPs) are a classification of bug examples specific to concurrent software. More and more software engineers are developing concurrent programming software to acquire effectiveness of multi core architecture. However another obstacle in creating concurrent programming is the presence of practically identical bugs(aka recurrent bugs) showing up over and over. In spite of the fact that frequency of a bugs does not generally indicate errors in programming but rather it leads to future bugs as programming grows. This build up a need to develop tools and techniques to automatically extract and use the already available information within the software revision histories and code to make future bug correction predictions. In this paper we propose the Naïve Bayes classifier to extend the functionality provided by Keshmesh, a tool to detect and fix the concurrent bugs. Naïve Bayes plays a major role in identifying more number of bugs as found by Keshmesh alone. Finding and repairing bugs in software is extremely crucial for the software to keep it stable and least vulnerable to future bugs. © Research India Publications.},\ndocument_type={Article},\nsource={Scopus},\n}\n\n"},{"id":"Geral106","name":"Test-equivalence analysis for automatic patch generation","authors":"Mechtaev, S. and Gao, X. and Tan, S.H. and Roychoudhury, A.","year":2018,"base":["Geral"],"abstract":"Automated program repair is a problem of finding a transformation (called a patch) of a given incorrect program that eliminates the observable failures. It has important applications such as providing debugging aids, automatically grading student assignments, and patching security vulnerabilities. A common challenge faced by existing repair techniques is scalability to large patch spaces, since there are many candidate patches that these techniques explicitly or implicitly consider. The correctness criteria for program repair is often given as a suite of tests. Current repair techniques do not scale due to the large number of test executions performed by the underlying search algorithms. In this work, we address this problem by introducing a methodology of patch generation based on a test-equivalence relation (if two programs are “test-equivalent” for a given test, they produce indistinguishable results on this test). We propose two test-equivalence relations based on runtime values and dependencies, respectively, and present an algorithm that performs on-the-fly partitioning of patches into test-equivalence classes. Our experiments on real-world programs reveal that the proposed methodology drastically reduces the number of test executions and therefore provides an order of magnitude efficiency improvement over existing repair techniques, without sacrificing patch quality. © 2018 Association for Computing Machinery.","doi":"10.1145/3241980","bibtex":"@CONFERENCE{Beller2014202,\nauthor={Beller, M. and Bacchelli, A. and Zaidman, A. and Juergens, E.},\ntitle={Modern code reviews in open-source projects: Which problems do they fix?},\njournal={11th Working Conference on Mining Software Repositories, MSR 2014 - Proceedings},\nyear={2014},\npages={202-211},\ndoi={10.1145/2597073.2597082},\nnote={cited By 57},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928665241&doi=10.1145%2f2597073.2597082&partnerID=40&md5=d794b26c0bc19222b32a9d28449d8db2},\nabstract={Code review is the manual assessment of source code by humans, mainly intended to identify defects and quality problems. Modern Code Review (MCR), a lightweight variant of the code inspections investigated since the 1970s, prevails today both in industry and open-source software (OSS) systems. The objective of this paper is to increase our understanding of the practical benefits that the MCR process produces on reviewed source code. To that end, we empirically explore the problems fixed through MCR in OSS systems. We manually classified over 1,400 changes taking place in reviewed code from two OSS projects into a validated categorization scheme. Surprisingly, results show that the types of changes due to the MCR process in OSS are strikingly similar to those in the industry and academic systems from literature, featuring the similar 75:25 ratio of maintainability-related to functional problems. We also reveal that 7-35% of review comments are discarded and that 10-22% of the changes are not triggered by an explicit review comment. Patterns emerged in the review data; we investigated them revealing the technical factors that influence the number of changes due to the MCR process. We found that bug-fixing tasks lead to fewer changes and tasks with more altered files and a higher code churn have more changes. Contrary to intuition, the person of the reviewer had no impact on the number of changes. Copyright is held by the author/owner(s). Publication rights licensed to ACM.},\nkeywords={Codes (symbols);  Computer software;  Defects;  Information dissemination;  Open source software;  Software engineering, Academic system;  Code inspections;  Code review;  Functional problems;  Lightweight variants;  Open source projects;  Quality problems;  Technical factors, Open systems},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral107","name":"Search, align, and repair: Data-driven feedback generation for introductory programming exercises","authors":"Wang, K. and Singh, R. and Su, Z.","year":2018,"base":["Geral"],"abstract":"This paper introduces the 'Search, Align, and Repair' data-driven program repair framework to automate feedback generation for introductory programming exercises. Distinct from existing techniques, our goal is to develop an efficient, fully automated, and problem-agnostic technique for large or MOOC-scale introductory programming courses. We leverage the large amount of available student submissions in such settings and develop new algorithms for identifying similar programs, aligning correct and incorrect programs, and repairing incorrect programs by finding minimal fixes. We have implemented our technique in the Sarfgen system and evaluated it on thousands of real student attempts from the Microsoft-DEV204.1x edX course and the Microsoft CodeHunt platform. Our results show that Sarfgen can, within two seconds on average, generate concise, useful feedback for 89.7% of the incorrect student submissions. It has been integrated with the Microsoft-DEV204.1X edX class and deployed for production use. © 2018 Association for Computing Machinery.","doi":"10.1145/3192366.3192384","bibtex":"@ARTICLE{Mechtaev2018,\nauthor={Mechtaev, S. and Gao, X. and Tan, S.H. and Roychoudhury, A.},\ntitle={Test-equivalence analysis for automatic patch generation},\njournal={ACM Transactions on Software Engineering and Methodology},\nyear={2018},\nvolume={27},\nnumber={4},\ndoi={10.1145/3241980},\nart_number={15},\nnote={cited By 2},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055834590&doi=10.1145%2f3241980&partnerID=40&md5=8b652172ab80df3bad5917d0e9e5e6c8},\nabstract={Automated program repair is a problem of finding a transformation (called a patch) of a given incorrect program that eliminates the observable failures. It has important applications such as providing debugging aids, automatically grading student assignments, and patching security vulnerabilities. A common challenge faced by existing repair techniques is scalability to large patch spaces, since there are many candidate patches that these techniques explicitly or implicitly consider. The correctness criteria for program repair is often given as a suite of tests. Current repair techniques do not scale due to the large number of test executions performed by the underlying search algorithms. In this work, we address this problem by introducing a methodology of patch generation based on a test-equivalence relation (if two programs are “test-equivalent” for a given test, they produce indistinguishable results on this test). We propose two test-equivalence relations based on runtime values and dependencies, respectively, and present an algorithm that performs on-the-fly partitioning of patches into test-equivalence classes. Our experiments on real-world programs reveal that the proposed methodology drastically reduces the number of test executions and therefore provides an order of magnitude efficiency improvement over existing repair techniques, without sacrificing patch quality. © 2018 Association for Computing Machinery.},\nkeywords={Equivalence classes;  Grading;  Program debugging;  Repair;  Set theory, Correctness criterion;  Dynamic program analysis;  Efficiency improvement;  Equivalence relations;  Program synthesis;  Real world projects;  Security vulnerabilities;  Student assignments, Software testing},\ndocument_type={Article},\nsource={Scopus},\n}\n\n"},{"id":"Geral108","name":"Automated clustering and program repair for introductory programming assignments","authors":"Gulwani, S. and Radiček, I. and Zuleger, F.","year":2018,"base":["Geral"],"abstract":"Providing feedback on programming assignments is a tedious task for the instructor, and even impossible in large Massive Open Online Courses with thousands of students. Previous research has suggested that program repair techniques can be used to generate feedback in programming education. In this paper, we present a novel fully automated program repair algorithm for introductory programming assignments. The key idea of the technique, which enables automation and scalability, is to use the existing correct student solutions to repair the incorrect attempts. We evaluate the approach in two experiments: (I) We evaluate the number, size and quality of the generated repairs on 4,293 incorrect student attempts from an existing MOOC. We find that our approach can repair 97% of student attempts, while 81% of those are small repairs of good quality. (II) We conduct a preliminary user study on performance and repair usefulness in an interactive teaching setting. We obtain promising initial results (the average usefulness grade 3.4 on a scale from 1 to 5), and conclude that our approach can be used in an interactive setting. © 2018 Copyright held by the owner/author(s).","doi":"10.1145/3192366.3192387","bibtex":"@CONFERENCE{Wang2018481,\nauthor={Wang, K. and Singh, R. and Su, Z.},\ntitle={Search, align, and repair: Data-driven feedback generation for introductory programming exercises},\njournal={Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)},\nyear={2018},\npages={481-495},\ndoi={10.1145/3192366.3192384},\nnote={cited By 0},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049555511&doi=10.1145%2f3192366.3192384&partnerID=40&md5=a16d350c7cd9b1be04456c59cf7ca95d},\nabstract={This paper introduces the 'Search, Align, and Repair' data-driven program repair framework to automate feedback generation for introductory programming exercises. Distinct from existing techniques, our goal is to develop an efficient, fully automated, and problem-agnostic technique for large or MOOC-scale introductory programming courses. We leverage the large amount of available student submissions in such settings and develop new algorithms for identifying similar programs, aligning correct and incorrect programs, and repairing incorrect programs by finding minimal fixes. We have implemented our technique in the Sarfgen system and evaluated it on thousands of real student attempts from the Microsoft-DEV204.1x edX course and the Microsoft CodeHunt platform. Our results show that Sarfgen can, within two seconds on average, generate concise, useful feedback for 89.7% of the incorrect student submissions. It has been integrated with the Microsoft-DEV204.1X edX class and deployed for production use. © 2018 Association for Computing Machinery.},\nkeywords={Computer aided analysis;  Computer programming languages;  Grading;  Repair;  Students, Automatic grading;  Computer-aided education;  Data driven;  Fully automated;  Introductory programming;  Introductory programming course;  Large amounts;  Program analysis, Computer aided instruction},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral109","name":"Automatic diagnosis of students' misconceptions in K-8 mathematics","authors":"Feldman, M.Q. and Cho, J.Y. and Ong, M. and Gulwani, S. and Popovic, Z. and Andersen, E.","year":2018,"base":["Geral"],"abstract":"K-8 mathematics students must learn many procedures, such as addition and subtraction. Students frequently learn \"buggy\" variations of these procedures, which we ideally could identify automatically. This is challenging because there are many possible variations that reflect deep compositions of procedural thought. Existing approaches for K-8 math use manually specified variations which do not scale to new math algorithms or previously unseen misconceptions. Our system examines students' answers and infers how they incorrectly combine basic skills into complex procedures. We evaluate this approach on data from approximately 300 students. Our system replicates 86% of the answers that contain clear systematic mistakes (13%). Investigating further, we found 77% at least partially replicate a known misconception, with 53% matching exactly. We also present data from 29 participants showing that our system can demonstrate inferred incorrect procedures to an educator as successfully as a human expert. © 2018 Association for Computing Machinery.","doi":"10.1145/3173574.3173838","bibtex":"@CONFERENCE{Gulwani2018465,\nauthor={Gulwani, S. and Radiček, I. and Zuleger, F.},\ntitle={Automated clustering and program repair for introductory programming assignments},\njournal={Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)},\nyear={2018},\npages={465-480},\ndoi={10.1145/3192366.3192387},\nnote={cited By 0},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049565185&doi=10.1145%2f3192366.3192387&partnerID=40&md5=1ff2247f025dd3d8e783ea1e5f38ddd4},\nabstract={Providing feedback on programming assignments is a tedious task for the instructor, and even impossible in large Massive Open Online Courses with thousands of students. Previous research has suggested that program repair techniques can be used to generate feedback in programming education. In this paper, we present a novel fully automated program repair algorithm for introductory programming assignments. The key idea of the technique, which enables automation and scalability, is to use the existing correct student solutions to repair the incorrect attempts. We evaluate the approach in two experiments: (I) We evaluate the number, size and quality of the generated repairs on 4,293 incorrect student attempts from an existing MOOC. We find that our approach can repair 97% of student attempts, while 81% of those are small repairs of good quality. (II) We conduct a preliminary user study on performance and repair usefulness in an interactive teaching setting. We obtain promising initial results (the average usefulness grade 3.4 on a scale from 1 to 5), and conclude that our approach can be used in an interactive setting. © 2018 Copyright held by the owner/author(s).},\nkeywords={Automation;  Computer programming languages;  Dynamic analysis;  Quality control;  Repair;  Teaching, Automated clustering;  Clustering;  Introductory programming;  Massive open online course;  MOOC;  Programming assignments;  Programming education;  Teaching settings, Students},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral110","name":"Mining stackoverflow for program repair","authors":"Liu, X. and Zhong, H.","year":2018,"base":["Geral"],"abstract":"In recent years, automatic program repair has been a hot research topic in the software engineering community, and many approaches have been proposed. Although these approaches produce promising results, some researchers criticize that existing approaches are still limited in their repair capability, due to their limited repair templates. Indeed, it is quite difficult to design effective repair templates. An award-wining paper analyzes thousands of manual bug fixes, but summarizes only ten repair templates. Although more bugs are thus repaired, recent studies show such repair templates are still insufficient. We notice that programmers often refer to Stack Overflow, when they repair bugs. With years of accumulation, Stack Overflow has millions of posts that are potentially useful to repair many bugs. The observation motives our work towards mining repair templates from Stack Overflow. In this paper, we propose a novel approach, called SOFix, that extracts code samples from Stack Overflow, and mines repair patterns from extracted code samples. Based on our mined repair patterns, we derived 13 repair templates. We implemented these repair templates in SOFix, and conducted evaluations on the widely used benchmark, Defects4J. Our results show that SOFix repaired 23 bugs, which are more than existing approaches. After comparing repaired bugs and templates, we find that SOFix repaired more bugs, since it has more repair templates. In addition, our results also reveal the urgent need for better fault localization techniques. © 2018 IEEE.","doi":"10.1109/SANER.2018.8330202","bibtex":"@CONFERENCE{Feldman2018,\nauthor={Feldman, M.Q. and Cho, J.Y. and Ong, M. and Gulwani, S. and Popovic, Z. and Andersen, E.},\ntitle={Automatic diagnosis of students' misconceptions in K-8 mathematics},\njournal={Conference on Human Factors in Computing Systems - Proceedings},\nyear={2018},\nvolume={2018-April},\ndoi={10.1145/3173574.3173838},\nnote={cited By 0},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046976960&doi=10.1145%2f3173574.3173838&partnerID=40&md5=8202873f8061d2f3b0859ec330907431},\nabstract={K-8 mathematics students must learn many procedures, such as addition and subtraction. Students frequently learn \"buggy\" variations of these procedures, which we ideally could identify automatically. This is challenging because there are many possible variations that reflect deep compositions of procedural thought. Existing approaches for K-8 math use manually specified variations which do not scale to new math algorithms or previously unseen misconceptions. Our system examines students' answers and infers how they incorrectly combine basic skills into complex procedures. We evaluate this approach on data from approximately 300 students. Our system replicates 86% of the answers that contain clear systematic mistakes (13%). Investigating further, we found 77% at least partially replicate a known misconception, with 53% matching exactly. We also present data from 29 participants showing that our system can demonstrate inferred incorrect procedures to an educator as successfully as a human expert. © 2018 Association for Computing Machinery.},\nkeywords={Human computer interaction;  Human engineering, Automatic diagnosis;  Complex procedure;  Elementary education;  Human expert;  Programming by demon-stration, Students},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral111","name":"Automated data-driven hint generation in intelligent tutoring systems for code-writing: On the road of future research","authors":"Hieu, B.T. and Mustapha, S.M.F.D.S.","year":2018,"base":["Geral"],"abstract":"Introductory programming is an essential part of the curriculum in any engineering discipline in universities. However, for many beginning students, it is very difficult to learn. In particular, these students often get stuck and frustrated when attempting to solve programming exercises. One way to assist beginning programmers to overcome difficulties in learning to program is to use intelligent tutoring systems (ITSs) for programming, which can provide students with personalized hints of students' solving process in programming exercises. Currently, mostly these systems manually construct the domain models. They take much time to construct, especially for exercises with very large solution spaces. One of the major challenges associated with handling ITSs for programming comes from the diversity of possible code solutions that a student can write. The use of data-driven approaches to develop these ITSs is just starting to be explored in the field. Given that this is still a relatively new research field, many challenges are still remained unsolved. Our goal in this paper is to review and classify analysis techniques that are requested to generate datadriven hints in ITSs for programming. This work also aims equally to identify the possible future directions in this research field. © 2018, Kassel University Press GmbH.","doi":"10.3991/ijet.v13i09.8023","bibtex":"@CONFERENCE{Liu2018118,\nauthor={Liu, X. and Zhong, H.},\ntitle={Mining stackoverflow for program repair},\njournal={25th IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2018 - Proceedings},\nyear={2018},\nvolume={2018-March},\npages={118-129},\ndoi={10.1109/SANER.2018.8330202},\nart_number={8330202},\nnote={cited By 4},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044591182&doi=10.1109%2fSANER.2018.8330202&partnerID=40&md5=7754c6864ba2ccc911d697af4efe0457},\nabstract={In recent years, automatic program repair has been a hot research topic in the software engineering community, and many approaches have been proposed. Although these approaches produce promising results, some researchers criticize that existing approaches are still limited in their repair capability, due to their limited repair templates. Indeed, it is quite difficult to design effective repair templates. An award-wining paper analyzes thousands of manual bug fixes, but summarizes only ten repair templates. Although more bugs are thus repaired, recent studies show such repair templates are still insufficient. We notice that programmers often refer to Stack Overflow, when they repair bugs. With years of accumulation, Stack Overflow has millions of posts that are potentially useful to repair many bugs. The observation motives our work towards mining repair templates from Stack Overflow. In this paper, we propose a novel approach, called SOFix, that extracts code samples from Stack Overflow, and mines repair patterns from extracted code samples. Based on our mined repair patterns, we derived 13 repair templates. We implemented these repair templates in SOFix, and conducted evaluations on the widely used benchmark, Defects4J. Our results show that SOFix repaired 23 bugs, which are more than existing approaches. After comparing repaired bugs and templates, we find that SOFix repaired more bugs, since it has more repair templates. In addition, our results also reveal the urgent need for better fault localization techniques. © 2018 IEEE.},\nkeywords={Reengineering;  Software engineering, Automatic programs;  Bug fixes;  Engineering community;  Fault localization;  Hot research topics;  Limited repairs;  Stack overflow, Repair},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral112","name":"TraceDiff: Debugging unexpected code behavior using trace divergences","authors":"Suzuki, R. and Soares, G. and Head, A. and Glassman, E. and Reis, R. and Mongiovi, M. and D'Antoni, L. and Hartmann, B.","year":2017,"base":["Geral"],"abstract":"Recent advances in program synthesis offer means to automatically debug student submissions and generate personalized feedback in massive programming classrooms. When automatically generating feedback for programming assignments, a key challenge is designing pedagogically useful hints that are as effective as the manual feedback given by teachers. Through an analysis of teachers' hint-giving practices in 132 online Q&A posts, we establish three design guidelines that an effective feedback design should follow. Based on these guidelines, we develop a feedback system that leverages both program synthesis and visualization techniques. Our system compares the dynamic code execution of both incorrect and fixed code and highlights how the error leads to a difference in behavior and where the incorrect code trace diverges from the expected solution. Results from our study suggest that our system enables students to detect and fix bugs that are not caught by students using another existing visual debugging tool. © 2017 IEEE.","doi":"10.1109/VLHCC.2017.8103457","bibtex":"@ARTICLE{Hieu2018174,\nauthor={Hieu, B.T. and Mustapha, S.M.F.D.S.},\ntitle={Automated data-driven hint generation in intelligent tutoring systems for code-writing: On the road of future research},\njournal={International Journal of Emerging Technologies in Learning},\nyear={2018},\nvolume={13},\nnumber={9},\npages={174-189},\ndoi={10.3991/ijet.v13i09.8023},\nnote={cited By 1},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057574543&doi=10.3991%2fijet.v13i09.8023&partnerID=40&md5=349e90c4f60f6f19a18e4cd048a7246b},\nabstract={Introductory programming is an essential part of the curriculum in any engineering discipline in universities. However, for many beginning students, it is very difficult to learn. In particular, these students often get stuck and frustrated when attempting to solve programming exercises. One way to assist beginning programmers to overcome difficulties in learning to program is to use intelligent tutoring systems (ITSs) for programming, which can provide students with personalized hints of students' solving process in programming exercises. Currently, mostly these systems manually construct the domain models. They take much time to construct, especially for exercises with very large solution spaces. One of the major challenges associated with handling ITSs for programming comes from the diversity of possible code solutions that a student can write. The use of data-driven approaches to develop these ITSs is just starting to be explored in the field. Given that this is still a relatively new research field, many challenges are still remained unsolved. Our goal in this paper is to review and classify analysis techniques that are requested to generate datadriven hints in ITSs for programming. This work also aims equally to identify the possible future directions in this research field. © 2018, Kassel University Press GmbH.},\nkeywords={Computer aided instruction;  Students, Analysis techniques;  Data driven;  Data-driven approach;  Engineering disciplines;  Intelligent tutoring system;  Intelligent tutoring system (ITSs);  Introductory programming;  Programming exercise, Engineering research},\ndocument_type={Article},\nsource={Scopus},\n}\n\n"},{"id":"Geral113","name":"Parser Generation by Example for Legacy Pattern Languages","authors":"Zaytsev, V.","year":2017,"base":["Geral"],"abstract":"Most modern software languages enjoy relatively free and relaxed concrete syntax, with significant flexibility of formatting of the program/model/sheet text. Yet, in the dark legacy corners of software engineering there are still languages with a strict fixed column-based structure—the compromises of times long gone, attempting to combine some human readability with some ease of machine processing. In this paper, we consider an industrial case study for retirement of a legacy domain-specific language, completed under extreme circumstances: absolute lack of documentation, varying line structure, hierarchical blocks within one file, scalability demands for millions of lines of code, performance demands for manipulating tens of thousands multi-megabyte files, etc. However, the regularity of the language allowed to infer its structure from the available examples, automatically, and produce highly efficient parsers for it. © 2017 Copyright held by the owner/author(s). Publication rights licensed to Association for Computing Machinery.","doi":"10.1145/3136040.3136058","bibtex":"@CONFERENCE{Suzuki2017107,\nauthor={Suzuki, R. and Soares, G. and Head, A. and Glassman, E. and Reis, R. and Mongiovi, M. and D'Antoni, L. and Hartmann, B.},\ntitle={TraceDiff: Debugging unexpected code behavior using trace divergences},\njournal={Proceedings of IEEE Symposium on Visual Languages and Human-Centric Computing, VL/HCC},\nyear={2017},\nvolume={2017-October},\npages={107-115},\ndoi={10.1109/VLHCC.2017.8103457},\nart_number={8103457},\nnote={cited By 1},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041002633&doi=10.1109%2fVLHCC.2017.8103457&partnerID=40&md5=28f50d6d73e7bb7f88973ff590bfa785},\nabstract={Recent advances in program synthesis offer means to automatically debug student submissions and generate personalized feedback in massive programming classrooms. When automatically generating feedback for programming assignments, a key challenge is designing pedagogically useful hints that are as effective as the manual feedback given by teachers. Through an analysis of teachers' hint-giving practices in 132 online Q&A posts, we establish three design guidelines that an effective feedback design should follow. Based on these guidelines, we develop a feedback system that leverages both program synthesis and visualization techniques. Our system compares the dynamic code execution of both incorrect and fixed code and highlights how the error leads to a difference in behavior and where the incorrect code trace diverges from the expected solution. Results from our study suggest that our system enables students to detect and fix bugs that are not caught by students using another existing visual debugging tool. © 2017 IEEE.},\nkeywords={Codes (symbols);  Students;  Teaching;  Visual languages, Code execution;  Feed-back designs;  Feedback systems;  Personalized feedback;  Program synthesis;  Programming assignments;  Visual debugging;  Visualization technique, Program debugging},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral114","name":"NoFAQ: Synthesizing command repairs from examples","authors":"D'Antoni, L. and Singh, R. and Vaughn, M.","year":2017,"base":["Geral"],"abstract":"Command-line tools are confusing and hard to use due to their cryptic error messages and lack of documentation. Novice users often resort to online help-forums for finding corrections to their buggy commands, but have a hard time in searching precisely for posts that are relevant to their problem and then applying the suggested solutions to their buggy command. We present NoFAQ, a tool that uses a set of rules to suggest possible fixes when users write buggy commands that trigger commonly occurring errors. The rules are expressed in a language called Fixit and each rule pattern-matches against the user's buggy command and corresponding error message, and uses these inputs to produce a possible fixed command. NoFAQ automatically learns Fixit rules from examples of buggy and repaired commands. We evaluate NoFAQ on two fronts. First, we use 92 benchmark problems drawn from an existing tool and show that NoFAQ is able to synthesize rules for 81 benchmark problems in real time using just 2 to 5 input-output examples for each rule. Second, we run our learning algorithm on the examples obtained through a crowd-sourcing interface and show that the learning algorithm scales to large sets of examples. © 2017 Copyright held by the owner/author(s).","doi":"10.1145/3106237.3106241","bibtex":"@CONFERENCE{Zaytsev2017212,\nauthor={Zaytsev, V.},\ntitle={Parser Generation by Example for Legacy Pattern Languages},\njournal={GPCE 2017 - Proceedings of the 16th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences, co-located with SPLASH 2017},\nyear={2017},\npages={212-218},\ndoi={10.1145/3136040.3136058},\nnote={cited By 2},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041182217&doi=10.1145%2f3136040.3136058&partnerID=40&md5=8d8111e2ad9dfc69526caaa4f4ec8ce7},\nabstract={Most modern software languages enjoy relatively free and relaxed concrete syntax, with significant flexibility of formatting of the program/model/sheet text. Yet, in the dark legacy corners of software engineering there are still languages with a strict fixed column-based structure—the compromises of times long gone, attempting to combine some human readability with some ease of machine processing. In this paper, we consider an industrial case study for retirement of a legacy domain-specific language, completed under extreme circumstances: absolute lack of documentation, varying line structure, hierarchical blocks within one file, scalability demands for millions of lines of code, performance demands for manipulating tens of thousands multi-megabyte files, etc. However, the regularity of the language allowed to infer its structure from the available examples, automatically, and produce highly efficient parsers for it. © 2017 Copyright held by the owner/author(s). Publication rights licensed to Association for Computing Machinery.},\nkeywords={Computer programming languages;  Problem oriented languages;  Software engineering;  Syntactics, Grammar inference;  Language acquisition;  Legacy software;  Parser generation;  Pattern languages, Computational linguistics},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral115","name":"New trends on exploratory methods for data analytics","authors":"Mottin, D. and Lissandrini, M. and Velegrakis, Y. and Palpanas, T.","year":2017,"base":["Geral"],"abstract":"Data usually comes in a plethora of formats and dimensions, rendering the exploration and information extraction processes cumbersome. Thus, being able to cast exploratory queries in the data with the intent of having an immediate glimpse on some of the data properties is becoming crucial. An exploratory query should be simple enough to avoid complicate declarative languages (such as SQL) and mechanisms, and at the same time retain the exibility and expressiveness of such languages. Recently, we have witnessed a rediscovery of the so called example-based methods, in which the user, or the analyst circumvent query languages by using examples as input. An example is a representative of the intended results, or in other words, an item from the result set. Example-based methods exploit inherent characteristics of the data to infer the results that the user has in mind, but may not able to (easily) express. They can be useful both in cases where a user is looking for information in an unfamiliar dataset, or simply when she is exploring the data without knowing what to find in there. In this tutorial, we present an excursus over the main methods for exploratory analysis, with a particular focus on examplebased methods. We show how different data types require different techniques, and present algorithms that are specifically designed for relational, textual, and graph data. © 2017 VLDB.","doi":"10.14778/3137765.3137824","bibtex":"@CONFERENCE{D'Antoni2017582,\nauthor={D'Antoni, L. and Singh, R. and Vaughn, M.},\ntitle={NoFAQ: Synthesizing command repairs from examples},\njournal={Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},\nyear={2017},\nvolume={Part F130154},\npages={582-592},\ndoi={10.1145/3106237.3106241},\nnote={cited By 2},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030779381&doi=10.1145%2f3106237.3106241&partnerID=40&md5=d88c0cefc995e65243cedba1951f22a5},\nabstract={Command-line tools are confusing and hard to use due to their cryptic error messages and lack of documentation. Novice users often resort to online help-forums for finding corrections to their buggy commands, but have a hard time in searching precisely for posts that are relevant to their problem and then applying the suggested solutions to their buggy command. We present NoFAQ, a tool that uses a set of rules to suggest possible fixes when users write buggy commands that trigger commonly occurring errors. The rules are expressed in a language called Fixit and each rule pattern-matches against the user's buggy command and corresponding error message, and uses these inputs to produce a possible fixed command. NoFAQ automatically learns Fixit rules from examples of buggy and repaired commands. We evaluate NoFAQ on two fronts. First, we use 92 benchmark problems drawn from an existing tool and show that NoFAQ is able to synthesize rules for 81 benchmark problems in real time using just 2 to 5 input-output examples for each rule. Second, we run our learning algorithm on the examples obtained through a crowd-sourcing interface and show that the learning algorithm scales to large sets of examples. © 2017 Copyright held by the owner/author(s).},\nkeywords={Benchmarking;  Computer programming languages;  Errors;  Learning algorithms;  Problem oriented languages, Bench-mark problems;  Command line interface;  Domain specific languages;  Error messages;  Input-output;  Program synthesis;  Programming by Example;  Set of rules, Software engineering},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral116","name":"High-coverage hint generation for massive courses: Do automated hints help CS1 students?","authors":"Phothilimthana, P.M. and Sridhara, S.","year":2017,"base":["Geral"],"abstract":"In massive programming courses, automated hint generation offers the promise of zero-cost, zero-latency assistance for students who are struggling to make progress on solving a program. While a more robust hint generation approach based on path construction requires tremendous engineering effort to build, another easier-to-build approach based on program mutations suffers from low coverage. This paper describes a robust hint generation system that extends the coverage of the mutation-based approach using two complementary techniques. A syntax checker detects common syntax misconception errors in individual subexpressions to guide students to partial solutions that can be evaluated for the semantic correctness. A mutation-based approach is then used to generate hints for almost-correct programs. If the mutation-based approach fails, a case analyzer detects missing program branches to guide students to partial solutions with reasonable structures. After analyzing over 75, 000 program submissions and 8, 789 hint requests, we found that using all three techniques together could offer hints for any program, no matter how far it was from a correct solution. Furthermore, our analysis shows that hints contributed to students' progress while still encouraging the students to solve problems by themselves. © 2017 ACM.","doi":"10.1145/3059009.3059058","bibtex":"@CONFERENCE{Mottin20171977,\nauthor={Mottin, D. and Lissandrini, M. and Velegrakis, Y. and Palpanas, T.},\ntitle={New trends on exploratory methods for data analytics},\njournal={Proceedings of the VLDB Endowment},\nyear={2017},\nvolume={10},\nnumber={12},\npages={1977-1980},\ndoi={10.14778/3137765.3137824},\nnote={cited By 7},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85036654672&doi=10.14778%2f3137765.3137824&partnerID=40&md5=90d86b7ee7fb1259483ac733a3a9c3aa},\nabstract={Data usually comes in a plethora of formats and dimensions, rendering the exploration and information extraction processes cumbersome. Thus, being able to cast exploratory queries in the data with the intent of having an immediate glimpse on some of the data properties is becoming crucial. An exploratory query should be simple enough to avoid complicate declarative languages (such as SQL) and mechanisms, and at the same time retain the exibility and expressiveness of such languages. Recently, we have witnessed a rediscovery of the so called example-based methods, in which the user, or the analyst circumvent query languages by using examples as input. An example is a representative of the intended results, or in other words, an item from the result set. Example-based methods exploit inherent characteristics of the data to infer the results that the user has in mind, but may not able to (easily) express. They can be useful both in cases where a user is looking for information in an unfamiliar dataset, or simply when she is exploring the data without knowing what to find in there. In this tutorial, we present an excursus over the main methods for exploratory analysis, with a particular focus on examplebased methods. We show how different data types require different techniques, and present algorithms that are specifically designed for relational, textual, and graph data. © 2017 VLDB.},\nkeywords={Query languages, Data analytics;  Data properties;  Data type;  Declarative Languages;  Example-based methods;  Exploratory analysis;  Graph data;  Inherent characteristics, Data mining},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral117","name":"Exploring the design space of automatically synthesized hints for introductory programming assignments","authors":"Suzuki, R. and Soares, G. and Glassman, E. and Head, A. and D'Antoni, L. and Hartmann, B.","year":2017,"base":["Geral"],"abstract":"For massive programming classrooms, recent advances in program synthesis offer means to automatically grade and debug student submissions, and generate feedback at scale. A key challenge for synthesis-based autograders is how to design personalized feedback for students that is as effective as manual feedback given by teachers today. To understand the state of hint-giving practice, we analyzed 132 online Q&A posts and conducted a semi-structured interview with a teacher from a local massive programming class. We identified five types of teacher hints that can also be generated by program synthesis. These hints describe transformations, locations, data, behavior, and examples. We describe our implementation of three of these hint types. This work paves the way for future deployments of automatic, pedagogically-useful programming hints driven by program synthesis. Copyright © 2017 by the Association for Computing Machinery, Inc. (ACM).","doi":"10.1145/3027063.3053187","bibtex":"@CONFERENCE{Phothilimthana2017182,\nauthor={Phothilimthana, P.M. and Sridhara, S.},\ntitle={High-coverage hint generation for massive courses: Do automated hints help CS1 students?},\njournal={Annual Conference on Innovation and Technology in Computer Science Education, ITiCSE},\nyear={2017},\nvolume={Part F128680},\npages={182-187},\ndoi={10.1145/3059009.3059058},\nnote={cited By 2},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029501992&doi=10.1145%2f3059009.3059058&partnerID=40&md5=4b7156222608027b2be15013517361e5},\nabstract={In massive programming courses, automated hint generation offers the promise of zero-cost, zero-latency assistance for students who are struggling to make progress on solving a program. While a more robust hint generation approach based on path construction requires tremendous engineering effort to build, another easier-to-build approach based on program mutations suffers from low coverage. This paper describes a robust hint generation system that extends the coverage of the mutation-based approach using two complementary techniques. A syntax checker detects common syntax misconception errors in individual subexpressions to guide students to partial solutions that can be evaluated for the semantic correctness. A mutation-based approach is then used to generate hints for almost-correct programs. If the mutation-based approach fails, a case analyzer detects missing program branches to guide students to partial solutions with reasonable structures. After analyzing over 75, 000 program submissions and 8, 789 hint requests, we found that using all three techniques together could offer hints for any program, no matter how far it was from a correct solution. Furthermore, our analysis shows that hints contributed to students' progress while still encouraging the students to solve problems by themselves. © 2017 ACM.},\nkeywords={Automation;  Computer aided analysis;  Computer aided instruction;  Education;  Education computing;  Engineering education;  Engineering research;  Semantics;  Syntactics;  Teaching, Automated tutor;  Complementary techniques;  Computer-aided education;  Generation systems;  Path construction;  Program analysis;  Program synthesis;  Programming course, Students},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral118","name":"Programming by examples: PL Meets ML","authors":"Gulwani, S. and Jain, P.","year":2017,"base":["Geral"],"abstract":"Programming by Examples (PBE) involves synthesizing intended programs in an underlying domain-specific language from example-based specifications. PBE systems are already revolutionizing the application domain of data wrangling and are set to significantly impact several other domains including code refactoring. There are three key components in a PBE system. (i) A search algorithm that can efficiently search for programs that are consistent with the examples provided by the user. We leverage a divide-and-conquer-based deductive search paradigm that inductively reduces the problem of synthesizing a program expression of a certain kind that satisfies a given specification into sub-problems that refer to sub-expressions or sub-specifications. (ii) Program ranking techniques to pick an intended program from among the many that satisfy the examples provided by the user. We leverage features of the program structure as well of the outputs generated by the program on test inputs. (iii) User interaction models to facilitate usability and debuggability. We leverage active-learning techniques based on clustering inputs and synthesizing multiple programs. Each of these PBE components leverage both symbolic reasoning and heuristics. We make the case for synthesizing these heuristics from training data using appropriate machine learning methods. This can not only lead to better heuristics, but can also enable easier development, maintenance, and even personalization of a PBE system. © Springer International Publishing AG 2017.","doi":"10.1007/978-3-319-71237-6_1","bibtex":"@CONFERENCE{Suzuki20172951,\nauthor={Suzuki, R. and Soares, G. and Glassman, E. and Head, A. and D'Antoni, L. and Hartmann, B.},\ntitle={Exploring the design space of automatically synthesized hints for introductory programming assignments},\njournal={Conference on Human Factors in Computing Systems - Proceedings},\nyear={2017},\nvolume={Part F127655},\npages={2951-2958},\ndoi={10.1145/3027063.3053187},\nnote={cited By 6},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019558441&doi=10.1145%2f3027063.3053187&partnerID=40&md5=4939483d8199b0c855668f61a42e3821},\nabstract={For massive programming classrooms, recent advances in program synthesis offer means to automatically grade and debug student submissions, and generate feedback at scale. A key challenge for synthesis-based autograders is how to design personalized feedback for students that is as effective as manual feedback given by teachers today. To understand the state of hint-giving practice, we analyzed 132 online Q&A posts and conducted a semi-structured interview with a teacher from a local massive programming class. We identified five types of teacher hints that can also be generated by program synthesis. These hints describe transformations, locations, data, behavior, and examples. We describe our implementation of three of these hint types. This work paves the way for future deployments of automatic, pedagogically-useful programming hints driven by program synthesis. Copyright © 2017 by the Association for Computing Machinery, Inc. (ACM).},\nkeywords={Education;  Human engineering;  Program debugging;  Students, Automated feedback;  Design spaces;  Introductory programming;  Personalized feedback;  Program synthesis;  Programming class;  Programming education;  Semi structured interviews, Teaching},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral119","name":"How many versions does a bug live in? an empirical study on text features for bug lifecycle prediction","authors":"Wang, C. and Li, Y. and Xu, B.","year":2018,"base":["Geral"],"abstract":"During the software system's maintenance and evolution, finding and removing software bugs is a very important part that consumes a large amount of money and effort. To analyze different bugs' character, it is very essential to know how long or which period of versions does the bug live in. In this study, we define version-based bug lifecycle and propose a text features based classification model to predict the versionlength of bug lifecycle. We collect 57000+ bugs from 10 well-know Apache Software Foundation projects to construct our dataset, and use the tf-idf method to collect our text features from bug report's summary and description. Our experimental results show that the text feature based method performs better than other baseline methods on 10 projects. The text feature based Naive Bayes classifiers outperforms all other methods with different features and classifiers. © 2018 Universitat zu Koln. All rights reserved.","doi":"10.18293/SEKE2018-176","bibtex":"@ARTICLE{Gulwani20173,\nauthor={Gulwani, S. and Jain, P.},\ntitle={Programming by examples: PL Meets ML},\njournal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},\nyear={2017},\nvolume={10695 LNCS},\npages={3-20},\ndoi={10.1007/978-3-319-71237-6_1},\nnote={cited By 0},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035036817&doi=10.1007%2f978-3-319-71237-6_1&partnerID=40&md5=d7a830a9231a66768fc21ac7a34d3cad},\nabstract={Programming by Examples (PBE) involves synthesizing intended programs in an underlying domain-specific language from example-based specifications. PBE systems are already revolutionizing the application domain of data wrangling and are set to significantly impact several other domains including code refactoring. There are three key components in a PBE system. (i) A search algorithm that can efficiently search for programs that are consistent with the examples provided by the user. We leverage a divide-and-conquer-based deductive search paradigm that inductively reduces the problem of synthesizing a program expression of a certain kind that satisfies a given specification into sub-problems that refer to sub-expressions or sub-specifications. (ii) Program ranking techniques to pick an intended program from among the many that satisfy the examples provided by the user. We leverage features of the program structure as well of the outputs generated by the program on test inputs. (iii) User interaction models to facilitate usability and debuggability. We leverage active-learning techniques based on clustering inputs and synthesizing multiple programs. Each of these PBE components leverage both symbolic reasoning and heuristics. We make the case for synthesizing these heuristics from training data using appropriate machine learning methods. This can not only lead to better heuristics, but can also enable easier development, maintenance, and even personalization of a PBE system. © Springer International Publishing AG 2017.},\nkeywords={Artificial intelligence;  Computer programming languages;  Graphical user interfaces;  Heuristic methods;  Learning algorithms;  Learning systems;  Object oriented programming;  Problem oriented languages;  Specifications, Code re-factoring;  Divide and conquer;  Domain specific languages;  Machine learning methods;  Program structures;  Programming by Example;  Search Algorithms;  Symbolic reasoning, Software testing},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n\n\n"},{"id":"Geral120","name":"A Simple, Efficient, Context-sensitive Approach for Code Completion","authors":"Asaduzzaman, M. and Roy, C.K. and Schneider, K.A. and Hou, D.","year":2016,"base":["Geral"],"abstract":"Code completion helps developers use application programming interfaces (APIs) and frees them from remembering every detail. In this paper, we first describe a novel technique called Context-sensitive Code Completion (CSCC) for improving the performance of API method call completion. CSCC is context sensitive in that it uses new sources of information as the context of a target method call. CSCC indexes method calls in code examples by their context. To recommend completion proposals, CSCC ranks candidate methods by the similarities between their contexts and the context of the target call. Evaluation using a set of subject systems and five popular state-of-the-art techniques suggests that CSCC performs better than existing type or example-based code completion systems. We conduct experiments to find how different contextual elements of the target call benefit CSCC. Next, we investigate the adaptability of the technique to support another form of code completion, i.e., field completion. Evaluation with eight different subject systems suggests that CSCC can easily support field completion with high accuracy. Finally, we compare CSCC with four popular statistical language models that support code completion. Results of statistical tests from our study suggest that CSCC not only outperforms those techniques that are based on token level language models, but also in most cases performs better or equally well with GraLan, the state-of-the-art graph-based language model. © 2016 John Wiley & Sons, Ltd.","doi":"10.1002/smr.1791","bibtex":"@CONFERENCE{Wang2018415,\nauthor={Wang, C. and Li, Y. and Xu, B.},\ntitle={How many versions does a bug live in? an empirical study on text features for bug lifecycle prediction},\njournal={Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE},\nyear={2018},\nvolume={2018-July},\npages={415-420},\ndoi={10.18293/SEKE2018-176},\nnote={cited By 0},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056862953&doi=10.18293%2fSEKE2018-176&partnerID=40&md5=a7272695d95a2df2f5c8a4617e84d198},\nabstract={During the software system's maintenance and evolution, finding and removing software bugs is a very important part that consumes a large amount of money and effort. To analyze different bugs' character, it is very essential to know how long or which period of versions does the bug live in. In this study, we define version-based bug lifecycle and propose a text features based classification model to predict the versionlength of bug lifecycle. We collect 57000+ bugs from 10 well-know Apache Software Foundation projects to construct our dataset, and use the tf-idf method to collect our text features from bug report's summary and description. Our experimental results show that the text feature based method performs better than other baseline methods on 10 projects. The text feature based Naive Bayes classifiers outperforms all other methods with different features and classifiers. © 2018 Universitat zu Koln. All rights reserved.},\nkeywords={Knowledge engineering;  Life cycle;  Program debugging;  Software engineering;  Technology transfer;  Text processing, Apache software foundations;  Baseline methods;  Classification models;  Empirical studies;  Large amounts;  Naive Bayes classifiers;  Software bug;  Software systems, Classification (of information)},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral121","name":"A survey on text-based modeling in model evolution and management","authors":"Somogyi, F.A. and Asztalos, M.","year":2019,"base":["Geral"],"abstract":"Model-driven software engineering methodologies like model-driven engineering aim to improve the productivity of software development by using graph-based models as the main artifacts during development, and generating the source code from these models. The models are usually displayed and edited using a graphical notation. However, they can also be described using a textual notation. This has some advantages and disadvantages compared to the graphical approach. For example, while editing the model, we can better focus on the details instead of a broad overview. Similarly to source code, models evolve rapidly during development. Handling and managing the evolution of models is an important task in model-driven methodologies and is an active research area today. However, there exist few research on text-based modeling approaches, compared to graph-based ones. This paper introduces the text-based modeling research field based on existing literature, and presents the state-of-the-art of the field related to model evolution and management. Our goal is to identify challenges and directions for future research in this field. The main topics covered are model differencing and merging, and the synchronization of the textual and graphical notations. © 2018 Budapest University of Technology and Economics. All Rights Reserved.","doi":"10.3311/PPee.12305","bibtex":"@ARTICLE{Asaduzzaman2016512,\nauthor={Asaduzzaman, M. and Roy, C.K. and Schneider, K.A. and Hou, D.},\ntitle={A Simple, Efficient, Context-sensitive Approach for Code Completion},\njournal={Journal of Software: Evolution and Process},\nyear={2016},\nvolume={28},\nnumber={7},\npages={512-541},\ndoi={10.1002/smr.1791},\nnote={cited By 2},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977540582&doi=10.1002%2fsmr.1791&partnerID=40&md5=d385d6c1e31bc5f299d78135d8608574},\nabstract={Code completion helps developers use application programming interfaces (APIs) and frees them from remembering every detail. In this paper, we first describe a novel technique called Context-sensitive Code Completion (CSCC) for improving the performance of API method call completion. CSCC is context sensitive in that it uses new sources of information as the context of a target method call. CSCC indexes method calls in code examples by their context. To recommend completion proposals, CSCC ranks candidate methods by the similarities between their contexts and the context of the target call. Evaluation using a set of subject systems and five popular state-of-the-art techniques suggests that CSCC performs better than existing type or example-based code completion systems. We conduct experiments to find how different contextual elements of the target call benefit CSCC. Next, we investigate the adaptability of the technique to support another form of code completion, i.e., field completion. Evaluation with eight different subject systems suggests that CSCC can easily support field completion with high accuracy. Finally, we compare CSCC with four popular statistical language models that support code completion. Results of statistical tests from our study suggest that CSCC not only outperforms those techniques that are based on token level language models, but also in most cases performs better or equally well with GraLan, the state-of-the-art graph-based language model. © 2016 John Wiley & Sons, Ltd.},\nkeywords={Application programming interfaces (API);  Codes (symbols);  Computational linguistics;  Graphic methods;  Natural language processing systems, API methods;  Code completions;  Context sensitive;  Language model;  Simhash, Context sensitive languages},\ndocument_type={Article},\nsource={Scopus},\n}\n\n"},{"id":"Geral122","name":"A taxonomy of code changes occurring within a statement or a signature","authors":"Yang, C. and James WhiteHead, E.","year":2018,"base":["Geral"],"abstract":"We propose a taxonomy of code changes at a granularity finer than the statement level. It classifies changes that occur within a statement or signature. We firstly define the changes based on the proposed operations on the tree of the statement or signature. Then, we classify the changes according to action type, entity kind, element kind and pattern kind. Based on the current implementation, we applied it to four open Java projects. Through the case study, we validated that the taxonomy can classify changes in all the modified statements and signatures. And, we checked the proportions of change patterns. Furthermore, we demonstrated that it is easy to find out the rename-induced statement updates with the help of the taxonomy. As a result, this taxonomy can be used for further change understanding and change analysis. © 2018 IEEE.","doi":"10.1109/TASE.2018.00020","bibtex":"@ARTICLE{Somogyi201951,\nauthor={Somogyi, F.A. and Asztalos, M.},\ntitle={A survey on text-based modeling in model evolution and management},\njournal={Periodica polytechnica Electrical engineering and computer science},\nyear={2019},\nvolume={63},\nnumber={1},\npages={51-65},\ndoi={10.3311/PPee.12305},\nnote={cited By 0},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061730571&doi=10.3311%2fPPee.12305&partnerID=40&md5=28ec18c1a37328a04dba247cf0260499},\nabstract={Model-driven software engineering methodologies like model-driven engineering aim to improve the productivity of software development by using graph-based models as the main artifacts during development, and generating the source code from these models. The models are usually displayed and edited using a graphical notation. However, they can also be described using a textual notation. This has some advantages and disadvantages compared to the graphical approach. For example, while editing the model, we can better focus on the details instead of a broad overview. Similarly to source code, models evolve rapidly during development. Handling and managing the evolution of models is an important task in model-driven methodologies and is an active research area today. However, there exist few research on text-based modeling approaches, compared to graph-based ones. This paper introduces the text-based modeling research field based on existing literature, and presents the state-of-the-art of the field related to model evolution and management. Our goal is to identify challenges and directions for future research in this field. The main topics covered are model differencing and merging, and the synchronization of the textual and graphical notations. © 2018 Budapest University of Technology and Economics. All Rights Reserved.},\nkeywords={Graphic methods;  Reviews;  Surveying;  Surveys, Domain specific modeling;  Model driven development;  Model evolution;  Model management;  Model-based engineering;  Model-driven Engineering, Software design},\ndocument_type={Article},\nsource={Scopus},\n}\n\n"},{"id":"Geral123","name":"Beyond metadata: code-centric and usage-based analysis of known vulnerabilities in open-source software","authors":"Ponta, S.E. and Plate, H. and Sabetta, A.","year":2018,"base":["Geral"],"abstract":"The use of open-source software (OSS) is ever-increasing, and so is the number of open-source vulnerabilities being discovered and publicly disclosed. The gains obtained from the reuse of community-developed libraries may be offset by the cost of detecting, assessing, and mitigating their vulnerabilities in a timely manner. In this paper we present a novel method to detect, assess and mitigate OSS vulnerabilities that improves on state-of-The-Art approaches, which commonly depend on metadata to identify vulnerable OSS dependencies. Our solution instead is code-centric and combines static and dynamic analysis to determine the reachability of the vulnerable portion of libraries used (directly or transitively) by an application. Taking this usage into account, our approach then supports developers in choosing among the existing non-vulnerable library versions. Vulas, the tool implementing our code-centric and usage-based approach, is officially recommended by SAP to scan its Java software, and has been successfully used to perform more than 250000 scans of about 500 applications since December 2016. We report on our experience and on the lessons we learned when maturing the tool from a research prototype to an industrial-grade solution. © 2018 IEEE.","doi":"10.1109/ICSME.2018.00054","bibtex":"@CONFERENCE{Yang201892,\nauthor={Yang, C. and James WhiteHead, E.},\ntitle={A taxonomy of code changes occurring within a statement or a signature},\njournal={Proceedings - 2018 12th International Symposium on Theoretical Aspects of Software Engineering, TASE 2018},\nyear={2018},\nvolume={2018-January},\npages={92-99},\ndoi={10.1109/TASE.2018.00020},\nart_number={8560738},\nnote={cited By 0},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062324469&doi=10.1109%2fTASE.2018.00020&partnerID=40&md5=7ce389dad8d7f517237a49395111f7b1},\nabstract={We propose a taxonomy of code changes at a granularity finer than the statement level. It classifies changes that occur within a statement or signature. We firstly define the changes based on the proposed operations on the tree of the statement or signature. Then, we classify the changes according to action type, entity kind, element kind and pattern kind. Based on the current implementation, we applied it to four open Java projects. Through the case study, we validated that the taxonomy can classify changes in all the modified statements and signatures. And, we checked the proportions of change patterns. Furthermore, we demonstrated that it is easy to find out the rename-induced statement updates with the help of the taxonomy. As a result, this taxonomy can be used for further change understanding and change analysis. © 2018 IEEE.},\nkeywords={Codes (symbols);  Software engineering, Change analysis;  Change patterns;  Code changes;  Element kinds;  Software Evolution;  Source code changes, Taxonomies},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral124","name":"Efficient static checking of library updates","authors":"Foo, D. and Chua, H. and Yeo, J. and Ang, M.Y. and Sharma, A.","year":2018,"base":["Geral"],"abstract":"Software engineering practices have evolved to the point where a developer writing a new application today doesn't start from scratch, but reuses a number of open source libraries and components. These third-party libraries evolve independently of the applications in which they are used, and may not maintain stable interfaces as bugs and vulnerabilities in them are fixed. This in turn causes API incompatibilities in downstream applications which must be manually resolved. Oversight here may manifest in many ways, from test failures to crashes at runtime. To address this problem, we present a static analysis for automatically and efficiently checking if a library upgrade introduces an API incompatibility. Our analysis does not rely on reported version information from library developers, and instead computes the actual differences between methods in libraries across different versions. The analysis is scalable, enabling real-time diff queries involving arbitrary pairs of library versions. It supports a vulnerability remediation product which suggests library upgrades automatically and is lightweight enough to be part of a continuous integration/delivery (CI/CD) pipeline. To evaluate the effectiveness of our approach, we determine semantic versioning adherence of a corpus of open source libraries taken from Maven Central, PyPI, and RubyGems. We find that on average, 26% of library versions are in violation of semantic versioning. We also analyze a collection of popular open source projects from GitHub to determine if we can automatically update libraries in them without causing API incompatibilities. Our results indicate that we can suggest upgrades automatically for 10% of the libraries. © 2018 Association for Computing Machinery.","doi":"10.1145/3236024.3275535","bibtex":"@CONFERENCE{Ponta2018449,\nauthor={Ponta, S.E. and Plate, H. and Sabetta, A.},\ntitle={Beyond metadata: code-centric and usage-based analysis of known vulnerabilities in open-source software},\njournal={Proceedings - 2018 IEEE International Conference on Software Maintenance and Evolution, ICSME 2018},\nyear={2018},\npages={449-460},\ndoi={10.1109/ICSME.2018.00054},\nart_number={8530051},\nnote={cited By 4},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058302988&doi=10.1109%2fICSME.2018.00054&partnerID=40&md5=b4b3fb60996781520d12d6f2cf9d72fd},\nabstract={The use of open-source software (OSS) is ever-increasing, and so is the number of open-source vulnerabilities being discovered and publicly disclosed. The gains obtained from the reuse of community-developed libraries may be offset by the cost of detecting, assessing, and mitigating their vulnerabilities in a timely manner. In this paper we present a novel method to detect, assess and mitigate OSS vulnerabilities that improves on state-of-The-Art approaches, which commonly depend on metadata to identify vulnerable OSS dependencies. Our solution instead is code-centric and combines static and dynamic analysis to determine the reachability of the vulnerable portion of libraries used (directly or transitively) by an application. Taking this usage into account, our approach then supports developers in choosing among the existing non-vulnerable library versions. Vulas, the tool implementing our code-centric and usage-based approach, is officially recommended by SAP to scan its Java software, and has been successfully used to perform more than 250000 scans of about 500 applications since December 2016. We report on our experience and on the lessons we learned when maturing the tool from a research prototype to an industrial-grade solution. © 2018 IEEE.},\nkeywords={Application programs;  Codes (symbols);  Computer software maintenance;  Industrial research;  Information dissemination;  Libraries;  Metadata;  Open systems, Industrial grades;  Java software;  Known vulnerabilities;  Open sources;  Research prototype;  Static and dynamic analysis;  Update supports;  Vulnerability analysis, Open source software},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral125","name":"Exploring the Impact of Code Smells on Fine-Grained Structural Change-Proneness","authors":"Liu, H. and Li, B. and Yang, Y. and Ma, W. and Jia, R.","year":2018,"base":["Geral"],"abstract":"Code smells are used to describe the bad structures in the source code, which could hinder software maintainability, understandability and changeability. Nowadays, scholars mainly focus on the impact of smell on textual change-proneness. However, in comparison to textual changes, structural changes could better reveal the change nature. In practice, not all code change types are equally important in terms of change risk severity levels, and software developers are more interested in particular changes relevant to their current tasks. Therefore, we investigate the relationship between smells and fine-grained structural change-proneness to solve these issues. Our experiment was conducted on 11 typical open source projects. We first employed Fishers exact test and Mann-Whitney test to explore whether smelly files (affected by at least one smell type) had higher structural change-proneness than other files, and whether files with more smell instances are more likely to undergo structural changes, respectively. Multivariate logistic regression model was built to study the relation between each kind of smell and change-proneness with respect to five change categories. Our results showed that: (1) in most cases, smelly files were more prone to structural changes and files with more smell instances tend to undergo higher structural changes; (2) quite a few smell types were related to structural change-proneness, particularly, Refused Parent Bequest (RPB), Message Chains (MCH), Divergent Change (DIVC), Feature Envy (FE) and Shotgun Surgery (SS) increased structural changes for some change categories. However, when controlling the file size Lines of Code (LOC), significant change-proneness of some smells disappeared or the magnitude of significance decreased more or less. © 2018 World Scientific Publishing Company.","doi":"10.1142/S0218194018500432","bibtex":"@CONFERENCE{Foo2018791,\nauthor={Foo, D. and Chua, H. and Yeo, J. and Ang, M.Y. and Sharma, A.},\ntitle={Efficient static checking of library updates},\njournal={ESEC/FSE 2018 - Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},\nyear={2018},\npages={791-796},\ndoi={10.1145/3236024.3275535},\nnote={cited By 0},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058317239&doi=10.1145%2f3236024.3275535&partnerID=40&md5=e72811ff0b7d5dcf5026c68a24eedb85},\nabstract={Software engineering practices have evolved to the point where a developer writing a new application today doesn't start from scratch, but reuses a number of open source libraries and components. These third-party libraries evolve independently of the applications in which they are used, and may not maintain stable interfaces as bugs and vulnerabilities in them are fixed. This in turn causes API incompatibilities in downstream applications which must be manually resolved. Oversight here may manifest in many ways, from test failures to crashes at runtime. To address this problem, we present a static analysis for automatically and efficiently checking if a library upgrade introduces an API incompatibility. Our analysis does not rely on reported version information from library developers, and instead computes the actual differences between methods in libraries across different versions. The analysis is scalable, enabling real-time diff queries involving arbitrary pairs of library versions. It supports a vulnerability remediation product which suggests library upgrades automatically and is lightweight enough to be part of a continuous integration/delivery (CI/CD) pipeline. To evaluate the effectiveness of our approach, we determine semantic versioning adherence of a corpus of open source libraries taken from Maven Central, PyPI, and RubyGems. We find that on average, 26% of library versions are in violation of semantic versioning. We also analyze a collection of popular open source projects from GitHub to determine if we can automatically update libraries in them without causing API incompatibilities. Our results indicate that we can suggest upgrades automatically for 10% of the libraries. © 2018 Association for Computing Machinery.},\nkeywords={Application programs;  Libraries;  Open systems;  Program debugging;  Semantics;  Static analysis, api diffs;  Call graphs;  Continuous integrations;  Downstream applications;  Open-source libraries;  Software engineering practices;  Versioning;  Vulnerability remediations, Open source software},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral126","name":"Understanding semi-structured merge conflict characteristics in open-source Java projects","authors":"Accioly, P. and Borba, P. and Cavalcanti, G.","year":2018,"base":["Geral"],"abstract":"Empirical studies show that merge conflicts frequently occur, impairing developers’ productivity, since merging conflicting contributions might be a demanding and tedious task. However, the structure of changes that lead to conflicts has not been studied yet. Understanding the underlying structure of conflicts, and the involved syntactic language elements might shed light on how to better avoid merge conflicts. To this end, in this paper we derive a catalog of conflict patterns expressed in terms of the structure of code changes that lead to merge conflicts. We focus on conflicts reported by a semistructured merge tool that exploits knowledge about the underlying syntax of the artifacts. This way, we avoid analyzing a large number of spurious conflicts often reported by typical line based merge tools. To assess the occurrence of such patterns in different systems, we conduct an empirical study reproducing 70,047 merges from 123 GitHub Java projects. Our results show that most semistructured merge conflicts in our sample happen because developers independently edit the same or consecutive lines of the same method. However, the probability of creating a merge conflict is approximately the same when editing methods, class fields, and modifier lists. Furthermore, we noticed that most part of conflicting merge scenarios, and merge conflicts, involve more than two developers. Also, that copying and pasting pieces of code, or even entire files, across different repositories is a common practice and cause of conflicts. Finally, we discuss how our results reveal the need for new research studies and suggest potential improvements to tools supporting collaborative software development. © 2017, Springer Science+Business Media, LLC, part of Springer Nature.","doi":"10.1007/s10664-017-9586-1","bibtex":"@ARTICLE{Liu20181487,\nauthor={Liu, H. and Li, B. and Yang, Y. and Ma, W. and Jia, R.},\ntitle={Exploring the Impact of Code Smells on Fine-Grained Structural Change-Proneness},\njournal={International Journal of Software Engineering and Knowledge Engineering},\nyear={2018},\nvolume={28},\nnumber={10},\npages={1487-1516},\ndoi={10.1142/S0218194018500432},\nnote={cited By 0},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054031457&doi=10.1142%2fS0218194018500432&partnerID=40&md5=577bccdc905f34ed6bd1ac86acf7bd2a},\nabstract={Code smells are used to describe the bad structures in the source code, which could hinder software maintainability, understandability and changeability. Nowadays, scholars mainly focus on the impact of smell on textual change-proneness. However, in comparison to textual changes, structural changes could better reveal the change nature. In practice, not all code change types are equally important in terms of change risk severity levels, and software developers are more interested in particular changes relevant to their current tasks. Therefore, we investigate the relationship between smells and fine-grained structural change-proneness to solve these issues. Our experiment was conducted on 11 typical open source projects. We first employed Fishers exact test and Mann-Whitney test to explore whether smelly files (affected by at least one smell type) had higher structural change-proneness than other files, and whether files with more smell instances are more likely to undergo structural changes, respectively. Multivariate logistic regression model was built to study the relation between each kind of smell and change-proneness with respect to five change categories. Our results showed that: (1) in most cases, smelly files were more prone to structural changes and files with more smell instances tend to undergo higher structural changes; (2) quite a few smell types were related to structural change-proneness, particularly, Refused Parent Bequest (RPB), Message Chains (MCH), Divergent Change (DIVC), Feature Envy (FE) and Shotgun Surgery (SS) increased structural changes for some change categories. However, when controlling the file size Lines of Code (LOC), significant change-proneness of some smells disappeared or the magnitude of significance decreased more or less. © 2018 World Scientific Publishing Company.},\nkeywords={Odors;  Open source software;  Regression analysis, Change proneness;  Code changes;  Code smell;  Fine-grained changes;  Multivariate logistic regressions;  Open source projects;  Software developer;  Software maintainability, Codes (symbols)},\ndocument_type={Article},\nsource={Scopus},\n}\n\n"},{"id":"Geral127","name":"Discovering API usability problems at scale","authors":"Murphy-Hill, E. and Sadowski, C. and Head, A. and Daughtry, J. and Macvean, A. and Jaspan, C. and Winter, C.","year":2018,"base":["Geral"],"abstract":"Software developers' productivity can be negatively impacted by using APIs incorrectly. In this paper, we describe an analysis technique we designed to find API usability problems by comparing successive file-level changes made by individual software developers. We applied our tool, StopMotion, to the file histories of real developers doing real tasks at Google. The results reveal several API usability challenges including simple typos, conceptual API misalignments, and conflation of similar APIs. © 2018 Copyright held by the owner/author(s).","doi":"10.1145/3194793.3194795","bibtex":"@ARTICLE{Accioly20182051,\nauthor={Accioly, P. and Borba, P. and Cavalcanti, G.},\ntitle={Understanding semi-structured merge conflict characteristics in open-source Java projects},\njournal={Empirical Software Engineering},\nyear={2018},\nvolume={23},\nnumber={4},\npages={2051-2085},\ndoi={10.1007/s10664-017-9586-1},\nnote={cited By 2},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038628515&doi=10.1007%2fs10664-017-9586-1&partnerID=40&md5=0ff6c8798c2b0b941587337bc37fa534},\nabstract={Empirical studies show that merge conflicts frequently occur, impairing developers’ productivity, since merging conflicting contributions might be a demanding and tedious task. However, the structure of changes that lead to conflicts has not been studied yet. Understanding the underlying structure of conflicts, and the involved syntactic language elements might shed light on how to better avoid merge conflicts. To this end, in this paper we derive a catalog of conflict patterns expressed in terms of the structure of code changes that lead to merge conflicts. We focus on conflicts reported by a semistructured merge tool that exploits knowledge about the underlying syntax of the artifacts. This way, we avoid analyzing a large number of spurious conflicts often reported by typical line based merge tools. To assess the occurrence of such patterns in different systems, we conduct an empirical study reproducing 70,047 merges from 123 GitHub Java projects. Our results show that most semistructured merge conflicts in our sample happen because developers independently edit the same or consecutive lines of the same method. However, the probability of creating a merge conflict is approximately the same when editing methods, class fields, and modifier lists. Furthermore, we noticed that most part of conflicting merge scenarios, and merge conflicts, involve more than two developers. Also, that copying and pasting pieces of code, or even entire files, across different repositories is a common practice and cause of conflicts. Finally, we discuss how our results reveal the need for new research studies and suggest potential improvements to tools supporting collaborative software development. © 2017, Springer Science+Business Media, LLC, part of Springer Nature.},\nkeywords={Groupware;  Java programming language;  Open source software;  Software design;  Syntactics, Awareness tool;  Collaborative software development;  Common practices;  Empirical Software Engineering;  Empirical studies;  Research studies;  Semi-structured;  Syntactic languages, Mergers and acquisitions},\ndocument_type={Article},\nsource={Scopus},\n}\n\n"},{"id":"Geral128","name":"Automatically repairing dependency-related build breakage","authors":"Macho, C. and McIntosh, S. and Pinzger, M.","year":2018,"base":["Geral"],"abstract":"Build systems are widely used in today's software projects to automate integration and build processes. Similar to source code, build specifications need to be maintained to avoid outdated specifications, and build breakage as a consequence. Recent work indicates that neglected build maintenance is one of the most frequently occurring reasons why open source and proprietary builds break. In this paper, we propose BuildMedic, an approach to automatically repair Maven builds that break due to dependency-related issues. Based on a manual investigation of 37 broken Maven builds in 23 open source Java projects, we derive three repair strategies to automatically repair the build, namely Version Update, Delete Dependency, and Add Repository. We evaluate the three strategies on 84 additional broken builds from the 23 studied projects in order to demonstrate the applicability of our approach. The evaluation shows that BuildMedic can automatically repair 45 of these broken builds (54%). Furthermore, in 36% of the successfully repaired build breakages, BuildMedic outputs at least one repair candidate that is considered a correct repair. Moreover, 76% of them could be repaired with only a single dependency correction. © 2018 IEEE.","doi":"10.1109/SANER.2018.8330201","bibtex":"@CONFERENCE{Murphy-Hill201814,\nauthor={Murphy-Hill, E. and Sadowski, C. and Head, A. and Daughtry, J. and Macvean, A. and Jaspan, C. and Winter, C.},\ntitle={Discovering API usability problems at scale},\njournal={Proceedings - International Conference on Software Engineering},\nyear={2018},\npages={14-17},\ndoi={10.1145/3194793.3194795},\nnote={cited By 1},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051867089&doi=10.1145%2f3194793.3194795&partnerID=40&md5=138e1a96c0b6a68d50768026352fcfb1},\nabstract={Software developers' productivity can be negatively impacted by using APIs incorrectly. In this paper, we describe an analysis technique we designed to find API usability problems by comparing successive file-level changes made by individual software developers. We applied our tool, StopMotion, to the file histories of real developers doing real tasks at Google. The results reveal several API usability challenges including simple typos, conceptual API misalignments, and conflation of similar APIs. © 2018 Copyright held by the owner/author(s).},\nkeywords={Application programming interfaces (API), Analysis techniques;  File levels;  Software developer;  Usability problems, Software engineering},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral129","name":"Using a probabilistic model to predict bug fixes","authors":"Soto, M. and Le Goues, C.","year":2018,"base":["Geral"],"abstract":"Automatic Software Repair (APR) has significant potential to reduce software maintenance costs by reducing the human effort required to localize and fix bugs. State-of-the-art generate-and-validate APR techniques select between and instantiate various mutation operators to construct candidate patches, informed largely by heuristic probability distributions. This may reduce effectiveness in terms of both efficiency and output quality. In practice, human developers have many options in terms of how to edit code to fix bugs, some of which are far more common than others (e.g., deleting a line of code is more common than adding a new class). We mined the most recent 100 bug-fixing commits from each of the 500 most popular Java projects in GitHub (the largest dataset to date) to create a probabilistic model describing edit distributions. We categorize, compare and evaluate the different mutation operators used in state-of-the-art approaches. We find that a probabilistic modelbased APR approach patches bugs more quickly in the majority of bugs studied, and that the resulting patches are of higher quality than those produced by previous approaches. Finally, we mine association rules for multi-edit source code changes, an understudied but important problem. We validate the association rules by analyzing how much of our corpus can be built from them. Our evaluation indicates that 84.6% of the multi-edit patches from the corpus can be built from the association rules, while maintaining 90% confidence. © 2018 IEEE.","doi":"10.1109/SANER.2018.8330211","bibtex":"@CONFERENCE{Macho2018106,\nauthor={Macho, C. and McIntosh, S. and Pinzger, M.},\ntitle={Automatically repairing dependency-related build breakage},\njournal={25th IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2018 - Proceedings},\nyear={2018},\nvolume={2018-March},\npages={106-117},\ndoi={10.1109/SANER.2018.8330201},\nart_number={8330201},\nnote={cited By 5},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049722884&doi=10.1109%2fSANER.2018.8330201&partnerID=40&md5=6b1cc80af70b9364d68283821d7dcfbd},\nabstract={Build systems are widely used in today's software projects to automate integration and build processes. Similar to source code, build specifications need to be maintained to avoid outdated specifications, and build breakage as a consequence. Recent work indicates that neglected build maintenance is one of the most frequently occurring reasons why open source and proprietary builds break. In this paper, we propose BuildMedic, an approach to automatically repair Maven builds that break due to dependency-related issues. Based on a manual investigation of 37 broken Maven builds in 23 open source Java projects, we derive three repair strategies to automatically repair the build, namely Version Update, Delete Dependency, and Add Repository. We evaluate the three strategies on 84 additional broken builds from the 23 studied projects in order to demonstrate the applicability of our approach. The evaluation shows that BuildMedic can automatically repair 45 of these broken builds (54%). Furthermore, in 36% of the successfully repaired build breakages, BuildMedic outputs at least one repair candidate that is considered a correct repair. Moreover, 76% of them could be repaired with only a single dependency correction. © 2018 IEEE.},\nkeywords={Coal breakage;  Reengineering;  Repair;  Specifications, Build systems;  Open sources;  Repair strategy;  Software project;  Source codes, Open source software},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral130","name":"Semantic Slicing of Software Version Histories","authors":"Li, Y. and Zhu, C. and Rubin, J. and Chechik, M.","year":2018,"base":["Geral"],"abstract":"Software developers often need to transfer functionality, e.g., a set of commits implementing a new feature or a bug fix, from one branch of a configuration management system to another. That can be a challenging task as the existing configuration management tools lack support for matching high-level, semantic functionality with low-level version histories. The developer thus has to either manually identify the exact set of semantically-related commits implementing the functionality of interest or sequentially port a segment of the change history, 'inheriting' additional, unwanted functionality. In this paper, we tackle this problem by providing automated support for identifying the set of semantically-related commits implementing a particular functionality, which is defined by a set of tests. We formally define the semantic slicing problem, provide an algorithm for identifying a set of commits that constitute a slice, and propose techniques to minimize the produced slice. We then instantiate the overall approach, CSlicer, in a specific implementation for Java projects managed in Git and evaluate its correctness and effectiveness on a set of open-source software repositories. We show that it allows to identify subsets of change histories that maintain the functionality of interest but are substantially smaller than the original ones. © 1976-2012 IEEE.","doi":"10.1109/TSE.2017.2664824","bibtex":"@CONFERENCE{Soto2018221,\nauthor={Soto, M. and Le Goues, C.},\ntitle={Using a probabilistic model to predict bug fixes},\njournal={25th IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2018 - Proceedings},\nyear={2018},\nvolume={2018-March},\npages={221-231},\ndoi={10.1109/SANER.2018.8330211},\nart_number={8330211},\nnote={cited By 2},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049727696&doi=10.1109%2fSANER.2018.8330211&partnerID=40&md5=2ba4b267b83561bb55d53ba7024a0481},\nabstract={Automatic Software Repair (APR) has significant potential to reduce software maintenance costs by reducing the human effort required to localize and fix bugs. State-of-the-art generate-and-validate APR techniques select between and instantiate various mutation operators to construct candidate patches, informed largely by heuristic probability distributions. This may reduce effectiveness in terms of both efficiency and output quality. In practice, human developers have many options in terms of how to edit code to fix bugs, some of which are far more common than others (e.g., deleting a line of code is more common than adding a new class). We mined the most recent 100 bug-fixing commits from each of the 500 most popular Java projects in GitHub (the largest dataset to date) to create a probabilistic model describing edit distributions. We categorize, compare and evaluate the different mutation operators used in state-of-the-art approaches. We find that a probabilistic modelbased APR approach patches bugs more quickly in the majority of bugs studied, and that the resulting patches are of higher quality than those produced by previous approaches. Finally, we mine association rules for multi-edit source code changes, an understudied but important problem. We validate the association rules by analyzing how much of our corpus can be built from them. Our evaluation indicates that 84.6% of the multi-edit patches from the corpus can be built from the association rules, while maintaining 90% confidence. © 2018 IEEE.},\nkeywords={Association rules;  Codes (symbols);  Program debugging;  Reengineering;  Repair, Mine association rules;  Mutation operators;  Probabilistic modeling;  Software maintenance costs;  Software repair;  Source code changes;  State of the art;  State-of-the-art approach, Probability distributions},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral131","name":"A Delta-Oriented Approach to Support the Safe Reuse of Black-Box Code Rewriters","authors":"Benni, B. and Mosser, S. and Moha, N. and Riveill, M.","year":2018,"base":["Geral"],"abstract":"The tedious process of corrective and perfective maintenance is often automated thanks to rewriting rules using tools such as Spoon or Coccinelle. These tools consider rules as black-boxes, and compose multiple rules by giving the output of a given rewriting as input to the next one. It is up to the developer to identify the right order (if it exists) among all the different rules. In this paper, we define a formal model compatible with the black-box assumption that reifies the modifications (Δs) made by each rule. Leveraging these Δs, we propose a way to safely compose multiple rules when applied to the same program by (i) ensuring the isolated application of the different rules and (ii) yield unexpected behaviors that were silently ignored before. We assess this approach by applying rewriting rules used to fix anti-patterns existing in Android applications to external pieces of software available on GitHub. © 2018, Springer International Publishing AG, part of Springer Nature.","doi":"10.1007/978-3-319-90421-4_11","bibtex":"@ARTICLE{Li2018182,\nauthor={Li, Y. and Zhu, C. and Rubin, J. and Chechik, M.},\ntitle={Semantic Slicing of Software Version Histories},\njournal={IEEE Transactions on Software Engineering},\nyear={2018},\nvolume={44},\nnumber={2},\npages={182-201},\ndoi={10.1109/TSE.2017.2664824},\nart_number={7843626},\nnote={cited By 1},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042129677&doi=10.1109%2fTSE.2017.2664824&partnerID=40&md5=13645d66ce31b78e41975fade3fa7f89},\nabstract={Software developers often need to transfer functionality, e.g., a set of commits implementing a new feature or a bug fix, from one branch of a configuration management system to another. That can be a challenging task as the existing configuration management tools lack support for matching high-level, semantic functionality with low-level version histories. The developer thus has to either manually identify the exact set of semantically-related commits implementing the functionality of interest or sequentially port a segment of the change history, 'inheriting' additional, unwanted functionality. In this paper, we tackle this problem by providing automated support for identifying the set of semantically-related commits implementing a particular functionality, which is defined by a set of tests. We formally define the semantic slicing problem, provide an algorithm for identifying a set of commits that constitute a slice, and propose techniques to minimize the produced slice. We then instantiate the overall approach, CSlicer, in a specific implementation for Java projects managed in Git and evaluate its correctness and effectiveness on a set of open-source software repositories. We show that it allows to identify subsets of change histories that maintain the functionality of interest but are substantially smaller than the original ones. © 1976-2012 IEEE.},\nkeywords={Computer software;  Open systems;  Semantics;  Software engineering, Automated support;  Configuration management systems;  Configuration management tools;  dependency;  Program analysis;  Software change;  Software developer;  Version control, Open source software},\ndocument_type={Article},\nsource={Scopus},\n}\n\n"},{"id":"Geral132","name":"The Impact of Coverage on Bug Density in a Large Industrial Software Project","authors":"Bach, T. and Andrzejak, A. and Pannemans, R. and Lo, D.","year":2017,"base":["Geral"],"abstract":"Measuring quality of test suites is one of the major challenges of software testing. Code coverage identifies tested and untested parts of code and is frequently used to approximate test suite quality. Multiple previous studies have investigated the relationship between coverage ratio and test suite quality, without a clear consent in the results. In this work we study whether covered code contains a smaller number of future bugs than uncovered code (assuming appropriate scaling). If this correlation holds and bug density is lower in covered code, coverage can be regarded as a meaningful metric to estimate the adequacy of testing. To this end we analyse 16000 internal bug reports and bug-fixes of SAP HANA, a large industrial software project. We found that the above-mentioned relationship indeed holds, and is statistically significant. Contrary to most previous works our study uses real bugs and real bug-fixes. Furthermore, our data is derived from a complex and large industrial project. © 2017 IEEE.","doi":"10.1109/ESEM.2017.44","bibtex":"@ARTICLE{Benni2018164,\nauthor={Benni, B. and Mosser, S. and Moha, N. and Riveill, M.},\ntitle={A Delta-Oriented Approach to Support the Safe Reuse of Black-Box Code Rewriters},\njournal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},\nyear={2018},\nvolume={10826 LNCS},\npages={164-180},\ndoi={10.1007/978-3-319-90421-4_11},\nnote={cited By 0},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047130758&doi=10.1007%2f978-3-319-90421-4_11&partnerID=40&md5=25a4f8703dd96be4566d0e358c9ca946},\nabstract={The tedious process of corrective and perfective maintenance is often automated thanks to rewriting rules using tools such as Spoon or Coccinelle. These tools consider rules as black-boxes, and compose multiple rules by giving the output of a given rewriting as input to the next one. It is up to the developer to identify the right order (if it exists) among all the different rules. In this paper, we define a formal model compatible with the black-box assumption that reifies the modifications (Δs) made by each rule. Leveraging these Δs, we propose a way to safely compose multiple rules when applied to the same program by (i) ensuring the isolated application of the different rules and (ii) yield unexpected behaviors that were silently ignored before. We assess this approach by applying rewriting rules used to fix anti-patterns existing in Android applications to external pieces of software available on GitHub. © 2018, Springer International Publishing AG, part of Springer Nature.},\nkeywords={Application programs, Android applications;  Anti-patterns;  Black boxes;  Formal model;  Perfective maintenance;  Rewriting rules, Computer software reusability},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral133","name":"Change-Aware Build Prediction Model for Stall Avoidance in Continuous Integration","authors":"Hassan, F. and Wang, X.","year":2017,"base":["Geral"],"abstract":"Continuous Integration(CI) is a widely used development practice where developers integrate their work after submitting code changes at central repository. CI servers usually monitor central repository for code change submission and automatically build software with changed code, perform unit testing, integration testing and provide test summary report. If build or test fails developers fix those issues and submit the code changes. Continuous submission of code modification by developers and build latency time creates stalls at CI server build pipeline and hence developers have to wait long time to get build outcome. In this paper, we proposed build prediction model that uses TravisTorrent data set with build error log clustering and AST level code change modification data to predict whether a build will be successful or not without attempting actual build so that developer can get early build outcome result. With the proposed model we can predict build outcome with an average F-Measure over 87% on all three build systems (Ant, Maven, Gradle) under the cross-project prediction scenario. © 2017 IEEE.","doi":"10.1109/ESEM.2017.23","bibtex":"@CONFERENCE{Bach2017307,\nauthor={Bach, T. and Andrzejak, A. and Pannemans, R. and Lo, D.},\ntitle={The Impact of Coverage on Bug Density in a Large Industrial Software Project},\njournal={International Symposium on Empirical Software Engineering and Measurement},\nyear={2017},\nvolume={2017-November},\npages={307-313},\ndoi={10.1109/ESEM.2017.44},\nnote={cited By 2},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042384798&doi=10.1109%2fESEM.2017.44&partnerID=40&md5=926786df8be5dda473f3fd8671108953},\nabstract={Measuring quality of test suites is one of the major challenges of software testing. Code coverage identifies tested and untested parts of code and is frequently used to approximate test suite quality. Multiple previous studies have investigated the relationship between coverage ratio and test suite quality, without a clear consent in the results. In this work we study whether covered code contains a smaller number of future bugs than uncovered code (assuming appropriate scaling). If this correlation holds and bug density is lower in covered code, coverage can be regarded as a meaningful metric to estimate the adequacy of testing. To this end we analyse 16000 internal bug reports and bug-fixes of SAP HANA, a large industrial software project. We found that the above-mentioned relationship indeed holds, and is statistically significant. Contrary to most previous works our study uses real bugs and real bug-fixes. Furthermore, our data is derived from a complex and large industrial project. © 2017 IEEE.},\nkeywords={Codes (symbols);  Computer software selection and evaluation;  Software engineering, bug densitiy;  coverage;  Empirical research;  Industry project;  Real world projects;  Software Quality, Software testing},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral134","name":"Generating simpler AST edit scripts by considering copy-and-paste","authors":"Higo, Y. and Ohtani, A. and Kusumoto, S.","year":2017,"base":["Geral"],"abstract":"In software development, there are many situations in which developers need to understand given source code changes in detail. Until now, a variety of techniques have been proposed to support understanding source code changes. Tree-based differencing techniques are expected to have better understandability than text-based ones, which are widely used nowadays (e.g., diff in Unix). In this paper, we propose to consider copy-and-paste as a kind of editing action forming tree-based edit script, which is an editing sequence that transforms a tree to another one. Software developers often perform copy- and-paste when they are writing source code. Introducing copy- and-paste action into edit script contributes to not only making simpler (more easily understandable) edit scripts but also making edit scripts closer to developers' actual editing sequences. We conducted experiments on an open dataset. As a result, we confirmed that our technique made edit scripts shorter for 18% of the code changes with a little more computational time. For the other 82% code changes, our technique generated the same edit scripts as an existing technique. We also confirmed that our technique provided more helpful visualizations. © 2017 IEEE.","doi":"10.1109/ASE.2017.8115664","bibtex":"@CONFERENCE{Hassan2017157,\nauthor={Hassan, F. and Wang, X.},\ntitle={Change-Aware Build Prediction Model for Stall Avoidance in Continuous Integration},\njournal={International Symposium on Empirical Software Engineering and Measurement},\nyear={2017},\nvolume={2017-November},\npages={157-162},\ndoi={10.1109/ESEM.2017.23},\nnote={cited By 2},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042388336&doi=10.1109%2fESEM.2017.23&partnerID=40&md5=9f5e3568a539f3c139f23b6463f35865},\nabstract={Continuous Integration(CI) is a widely used development practice where developers integrate their work after submitting code changes at central repository. CI servers usually monitor central repository for code change submission and automatically build software with changed code, perform unit testing, integration testing and provide test summary report. If build or test fails developers fix those issues and submit the code changes. Continuous submission of code modification by developers and build latency time creates stalls at CI server build pipeline and hence developers have to wait long time to get build outcome. In this paper, we proposed build prediction model that uses TravisTorrent data set with build error log clustering and AST level code change modification data to predict whether a build will be successful or not without attempting actual build so that developer can get early build outcome result. With the proposed model we can predict build outcome with an average F-Measure over 87% on all three build systems (Ant, Maven, Gradle) under the cross-project prediction scenario. © 2017 IEEE.},\nkeywords={Codes (symbols);  Forecasting;  Integration;  Pipeline codes;  Software engineering;  Software testing, Build systems;  Code modifications;  Continuous integrations;  Continuous submission;  Development practices;  Latency time;  Outcome prediction;  Prediction model, Integration testing},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral135","name":"S3: Syntax- and semantic-guided repair synthesis via programming by examples","authors":"Le, X.-B.D. and Chu, D.-H. and Lo, D. and Le Goues, C. and Visser, W.","year":2017,"base":["Geral"],"abstract":"A notable class of techniques for automatic program repair is known as semantics-based. Such techniques, e.g., Angelix, infer semantic specifications via symbolic execution, and then use program synthesis to construct new code that satisfies those inferred specifications. However, the obtained specifications are naturally incomplete, leaving the synthesis engine with a difficult task of synthesizing a general solution from a sparse space of many possible solutions that are consistent with the provided specifications but that do not necessarily generalize. We present S3, a new repair synthesis engine that leverages programming-by-examples methodology to synthesize high-quality bug repairs. The novelty in S3 that allows it to tackle the sparse search space to create more general repairs is three-fold: (1) A systematic way to customize and constrain the syntactic search space via a domain-specific language, (2) An efficient enumerationbased search strategy over the constrained search space, and (3) A number of ranking features based on measures of the syntactic and semantic distances between candidate solutions and the original buggy program.We compare S3's repair effectiveness with state-ofthe- art synthesis engines Angelix, Enumerative, and CVC4. S3 can successfully and correctly fix at least three times more bugs than the best baseline on datasets of 52 bugs in small programs, and 100 bugs in real-world large programs. © 2017 Association for Computing Machinery.","doi":"10.1145/3106237.3106309","bibtex":"@CONFERENCE{Higo2017532,\nauthor={Higo, Y. and Ohtani, A. and Kusumoto, S.},\ntitle={Generating simpler AST edit scripts by considering copy-and-paste},\njournal={ASE 2017 - Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering},\nyear={2017},\npages={532-542},\ndoi={10.1109/ASE.2017.8115664},\nart_number={8115664},\nnote={cited By 2},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041440051&doi=10.1109%2fASE.2017.8115664&partnerID=40&md5=2866cb368a355e130c63327b292583b3},\nabstract={In software development, there are many situations in which developers need to understand given source code changes in detail. Until now, a variety of techniques have been proposed to support understanding source code changes. Tree-based differencing techniques are expected to have better understandability than text-based ones, which are widely used nowadays (e.g., diff in Unix). In this paper, we propose to consider copy-and-paste as a kind of editing action forming tree-based edit script, which is an editing sequence that transforms a tree to another one. Software developers often perform copy- and-paste when they are writing source code. Introducing copy- and-paste action into edit script contributes to not only making simpler (more easily understandable) edit scripts but also making edit scripts closer to developers' actual editing sequences. We conducted experiments on an open dataset. As a result, we confirmed that our technique made edit scripts shorter for 18% of the code changes with a little more computational time. For the other 82% code changes, our technique generated the same edit scripts as an existing technique. We also confirmed that our technique provided more helpful visualizations. © 2017 IEEE.},\nkeywords={Codes (symbols);  Computer programming languages;  Copying;  Software design, Code changes;  Computational time;  Copy-and-paste;  Software developer;  Source code changes;  Source codes;  Tree-based;  Understandability, Software engineering},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral136","name":"Computing counter-examples for privilege protection losses using security models","authors":"Laverdiere, M.-A. and Merlo, E.","year":2017,"base":["Geral"],"abstract":"Role-Based Access Control (RBAC) is commonly used in web applications to protect information and restrict operations. Code changes may affect the security of the application and need to be validated, in order to avoid security vulnerabilities, which is a major undertaking. A statement suffers from privilege protection loss in a release pair when it was definitely protected on all execution paths in the previous release and is now reachable by some execution paths with an inferior privilege protection. Because the code change and the resulting privilege protection loss may be distant (e.g. in different functions or files), developers may find it difficult to diagnose and correct the issue. We use Pattern Traversal Flow Analysis (PTFA) to statically analyze code-derived formal models. Our analysis automatically computes counter-examples of definite protection properties and privilege protection losses. We computed privilege protections and their changes for 147 release pairs of WordPress. We computed counter-examples for a total of 14,116 privilege protection losses we found spread in 31 release pairs.We present the distribution of counter-examples' lengths, as well as their spread across function and file boundaries. Our results show that counter-examples are typically short and localized. The median example spans 88 statements, crosses a single function boundary, and is contained in the same file. The 90th centile example measures 174 statements and spans 3 function boundaries over 3 files. We believe that the privilege protection counter-examples' characteristics would be helpful to focus developers' attention for security reviews. These counter-examples are also a first step toward explanations. © 2017 IEEE.","doi":"10.1109/SANER.2017.7884625","bibtex":"@CONFERENCE{Le2017593,\nauthor={Le, X.-B.D. and Chu, D.-H. and Lo, D. and Le Goues, C. and Visser, W.},\ntitle={S3: Syntax- and semantic-guided repair synthesis via programming by examples},\njournal={Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},\nyear={2017},\nvolume={Part F130154},\npages={593-604},\ndoi={10.1145/3106237.3106309},\nnote={cited By 25},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030757304&doi=10.1145%2f3106237.3106309&partnerID=40&md5=abfce350040c37286a28f28f99798ee8},\nabstract={A notable class of techniques for automatic program repair is known as semantics-based. Such techniques, e.g., Angelix, infer semantic specifications via symbolic execution, and then use program synthesis to construct new code that satisfies those inferred specifications. However, the obtained specifications are naturally incomplete, leaving the synthesis engine with a difficult task of synthesizing a general solution from a sparse space of many possible solutions that are consistent with the provided specifications but that do not necessarily generalize. We present S3, a new repair synthesis engine that leverages programming-by-examples methodology to synthesize high-quality bug repairs. The novelty in S3 that allows it to tackle the sparse search space to create more general repairs is three-fold: (1) A systematic way to customize and constrain the syntactic search space via a domain-specific language, (2) An efficient enumerationbased search strategy over the constrained search space, and (3) A number of ranking features based on measures of the syntactic and semantic distances between candidate solutions and the original buggy program.We compare S3's repair effectiveness with state-ofthe- art synthesis engines Angelix, Enumerative, and CVC4. S3 can successfully and correctly fix at least three times more bugs than the best baseline on datasets of 52 bugs in small programs, and 100 bugs in real-world large programs. © 2017 Association for Computing Machinery.},\nkeywords={Computer programming languages;  Engines;  Model checking;  Problem oriented languages;  Repair;  Semantics;  Software engineering;  Specifications;  Syntactics, Automatic programs;  Domain specific languages;  General solutions;  Programming by Example;  Repair effectiveness;  Search strategies;  Semantic specification;  Symbolic execution, Program debugging},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral137","name":"Raters’ reliability in clone benchmarks construction","authors":"Charpentier, A. and Falleri, J.-R. and Morandat, F. and Ben Hadj Yahia, E. and Réveillère, L.","year":2017,"base":["Geral"],"abstract":"Cloned code often complicates code maintenance and evolution and must therefore be effectively detected. One of the biggest challenges for clone detectors is to reduce the amount of irrelevant clones they found, called false positives. Several benchmarks of true and false positive clones have been introduced, enabling tool developers to compare, assess and fine-tune their tools. Manual inspection of clone candidates is performed by raters that do not have expertise on the underlying code. This way of building benchmarks might be unreliable when considering context-dependent clones i.e., clones valid for a specific purpose. Our goal is to investigate the reliability of rater judgments about context-dependent clones. We randomly select about 600 clones from two projects and ask several raters, including experts of the projects, to manually classify these clones. We observe that judgments of non expert raters are not always repeatable. We also observe that they seldomly agree with each others and with the expert. Finally, we find that the project and the fact that a clone is a true or false positive might have an influence on the agreement between the expert and non experts. Therefore, using non experts to produce clone benchmarks could be unreliable. © 2016, Springer Science+Business Media New York.","doi":"10.1007/s10664-015-9419-z","bibtex":"@CONFERENCE{Laverdiere2017240,\nauthor={Laverdiere, M.-A. and Merlo, E.},\ntitle={Computing counter-examples for privilege protection losses using security models},\njournal={SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering},\nyear={2017},\npages={240-249},\ndoi={10.1109/SANER.2017.7884625},\nart_number={7884625},\nnote={cited By 2},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018403590&doi=10.1109%2fSANER.2017.7884625&partnerID=40&md5=29331547ad71090979a1729384fe111b},\nabstract={Role-Based Access Control (RBAC) is commonly used in web applications to protect information and restrict operations. Code changes may affect the security of the application and need to be validated, in order to avoid security vulnerabilities, which is a major undertaking. A statement suffers from privilege protection loss in a release pair when it was definitely protected on all execution paths in the previous release and is now reachable by some execution paths with an inferior privilege protection. Because the code change and the resulting privilege protection loss may be distant (e.g. in different functions or files), developers may find it difficult to diagnose and correct the issue. We use Pattern Traversal Flow Analysis (PTFA) to statically analyze code-derived formal models. Our analysis automatically computes counter-examples of definite protection properties and privilege protection losses. We computed privilege protections and their changes for 147 release pairs of WordPress. We computed counter-examples for a total of 14,116 privilege protection losses we found spread in 31 release pairs.We present the distribution of counter-examples' lengths, as well as their spread across function and file boundaries. Our results show that counter-examples are typically short and localized. The median example spans 88 statements, crosses a single function boundary, and is contained in the same file. The 90th centile example measures 174 statements and spans 3 function boundaries over 3 files. We believe that the privilege protection counter-examples' characteristics would be helpful to focus developers' attention for security reviews. These counter-examples are also a first step toward explanations. © 2017 IEEE.},\nkeywords={Access control;  Codes (symbols);  Computer software maintenance;  Model checking;  Reengineering, Counter examples;  Evolution;  Execution paths;  Protect information;  Role-based Access Control;  Security model;  Security vulnerabilities;  WEB application, Static analysis},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral138","name":"Incrementalizing Abstract Interpretation","authors":"Van Es, N. and Vandercammen, M. and De Roover, C.","year":2017,"base":["Geral"],"abstract":"A powerful approach to design static analysers is abstract interpretation, which reasons over an approximation of the program’s behaviour. The Abstracting Abstract Machines (AAM) technique, introduced by Van Horn & Might, presents a systematic approach to derive abstract interpreters. However, it is often not useable in the development and evolution of large-scale applications, as with the current state-of-the-art, an AAM analysis can not incrementally update its results upon changes in the program. That is, whenever minor modifications occur in the program’s source code, one needs to recompute the entire AAM analysis from scratch, which can easily get time-consuming. We therefore propose an incremental approach to abstract interpretation, more precisely the AAM technique. That is, we modify the technique so that the result of an AAM analysis, the abstract state graph, can incrementally be updated following a change in the program’s source code. Our algorithm tracks dependencies between the nodes in the abstract syntax tree (AST) of the program and the transitions in the abstract state graph to invalidate and recompute new transitions in the state graph upon a change in the AST. Our experiments using a set of Scheme micro-benchmarks reveal that in practice this approach is often limited, as only states that are identical in both state graphs are reusable. We therefore introduce an improvement to the original incremental algorithm, which we refer to as state adaptation. State adaptation also enables reusing states that are not identical, but similar. Both the original and improved algorithm are integrated and evaluated in the Scala-AM framework. Our current implementations already show good results in terms of incremental efficiency, although more optimization is required to achieve actual gains in run time performance with our approaches.","bibtex":"@ARTICLE{Charpentier2017235,\nauthor={Charpentier, A. and Falleri, J.-R. and Morandat, F. and Ben Hadj Yahia, E. and Réveillère, L.},\ntitle={Raters’ reliability in clone benchmarks construction},\njournal={Empirical Software Engineering},\nyear={2017},\nvolume={22},\nnumber={1},\npages={235-258},\ndoi={10.1007/s10664-015-9419-z},\nnote={cited By 4},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957652388&doi=10.1007%2fs10664-015-9419-z&partnerID=40&md5=93b2f8d4cecdb2e421ce4d6080c577c0},\nabstract={Cloned code often complicates code maintenance and evolution and must therefore be effectively detected. One of the biggest challenges for clone detectors is to reduce the amount of irrelevant clones they found, called false positives. Several benchmarks of true and false positive clones have been introduced, enabling tool developers to compare, assess and fine-tune their tools. Manual inspection of clone candidates is performed by raters that do not have expertise on the underlying code. This way of building benchmarks might be unreliable when considering context-dependent clones i.e., clones valid for a specific purpose. Our goal is to investigate the reliability of rater judgments about context-dependent clones. We randomly select about 600 clones from two projects and ask several raters, including experts of the projects, to manually classify these clones. We observe that judgments of non expert raters are not always repeatable. We also observe that they seldomly agree with each others and with the expert. Finally, we find that the project and the fact that a clone is a true or false positive might have an influence on the agreement between the expert and non experts. Therefore, using non experts to produce clone benchmarks could be unreliable. © 2016, Springer Science+Business Media New York.},\nkeywords={Codes (symbols), Code clone;  Context dependent;  Duplication;  Empirical studies;  Enabling tools;  False positive;  Manual inspection;  Software metrics, Cloning},\ndocument_type={Article},\nsource={Scopus},\n}\n\n"},{"id":"Geral139","name":"Documentation reuse: Hot or not? an empirical study","authors":"Oumaziz, M.A. and Charpentier, A. and Falleri, J.-R. and Blanc, X.","year":2017,"base":["Geral"],"abstract":"Having available a high quality documentation is critical for software projects. This is why documentation tools such as Javadoc are so popular. As for code, documentation should be reused when possible to increase developer productivity and simplify maintenance. In this paper, we perform an empirical study of duplications in JavaDoc documentation on a corpus of seven famous Java APIs. Our results show that copy-pastes of JavaDoc documentation tags are abundant in our corpus. We also show that these copy-pastes are caused by four different kinds of relations in the underlying source code. In addition, we show that popular documentation tools do not provide any reuse mechanism to cope with these relations. Finally, we make a proposal for a simple but efficient automatic reuse mechanism. © Springer International Publishing AG 2017.","doi":"10.1007/978-3-319-56856-0_2","bibtex":"@CONFERENCE{VanEs201731,\nauthor={Van Es, N. and Vandercammen, M. and De Roover, C.},\ntitle={Incrementalizing Abstract Interpretation},\njournal={CEUR Workshop Proceedings},\nyear={2017},\nvolume={2047},\npages={31-35},\nnote={cited By 0},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041675860&partnerID=40&md5=4fb618ed6a7de67f4bf7b275934aee9b},\nabstract={A powerful approach to design static analysers is abstract interpretation, which reasons over an approximation of the program’s behaviour. The Abstracting Abstract Machines (AAM) technique, introduced by Van Horn & Might, presents a systematic approach to derive abstract interpreters. However, it is often not useable in the development and evolution of large-scale applications, as with the current state-of-the-art, an AAM analysis can not incrementally update its results upon changes in the program. That is, whenever minor modifications occur in the program’s source code, one needs to recompute the entire AAM analysis from scratch, which can easily get time-consuming. We therefore propose an incremental approach to abstract interpretation, more precisely the AAM technique. That is, we modify the technique so that the result of an AAM analysis, the abstract state graph, can incrementally be updated following a change in the program’s source code. Our algorithm tracks dependencies between the nodes in the abstract syntax tree (AST) of the program and the transitions in the abstract state graph to invalidate and recompute new transitions in the state graph upon a change in the AST. Our experiments using a set of Scheme micro-benchmarks reveal that in practice this approach is often limited, as only states that are identical in both state graphs are reusable. We therefore introduce an improvement to the original incremental algorithm, which we refer to as state adaptation. State adaptation also enables reusing states that are not identical, but similar. Both the original and improved algorithm are integrated and evaluated in the Scala-AM framework. Our current implementations already show good results in terms of incremental efficiency, although more optimization is required to achieve actual gains in run time performance with our approaches.},\nkeywords={Abstracting;  Application programs;  Model checking;  Optimization;  Static analysis, Abstract interpretations;  Abstract machines;  Abstract Syntax Trees;  Incremental algorithm;  Incremental approach;  Incremental computation;  Large-scale applications;  Run-time performance, Trees (mathematics)},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral140","name":"Precise version control of trees with line-based version control systems","authors":"Asenov, D. and Guenat, B. and Müller, P. and Otth, M.","year":2017,"base":["Geral"],"abstract":"Version control of tree structures, ubiquitous in software engineering, is typically performed on a textual encoding of the trees, rather than the trees directly. Applying standard line-based diff and merge algorithms to such encodings leads to inaccurate diffs, unnecessary conflicts, and incorrect merges. To address these problems, we propose novel algorithms for computing precise diffs between two versions of a tree and for three-way merging of trees. Unlike most other approaches for version control of structured data, our approach integrates with mainstream version control systems. Our merge algorithm can be customized for specific application domains to further improve merge results. An evaluation of our approach on abstract syntax trees from popular Java projects shows substantially improved merge results compared to Git. © Springer-Verlag GmbH Germany 2017.","doi":"10.1007/978-3-662-54494-5_9","bibtex":"@ARTICLE{Oumaziz201712,\nauthor={Oumaziz, M.A. and Charpentier, A. and Falleri, J.-R. and Blanc, X.},\ntitle={Documentation reuse: Hot or not? an empirical study},\njournal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},\nyear={2017},\nvolume={10221 LNCS},\npages={12-27},\ndoi={10.1007/978-3-319-56856-0_2},\nnote={cited By 3},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019252348&doi=10.1007%2f978-3-319-56856-0_2&partnerID=40&md5=0ed9e9ec000756d7558cec282305ab5e},\nabstract={Having available a high quality documentation is critical for software projects. This is why documentation tools such as Javadoc are so popular. As for code, documentation should be reused when possible to increase developer productivity and simplify maintenance. In this paper, we perform an empirical study of duplications in JavaDoc documentation on a corpus of seven famous Java APIs. Our results show that copy-pastes of JavaDoc documentation tags are abundant in our corpus. We also show that these copy-pastes are caused by four different kinds of relations in the underlying source code. In addition, we show that popular documentation tools do not provide any reuse mechanism to cope with these relations. Finally, we make a proposal for a simple but efficient automatic reuse mechanism. © Springer International Publishing AG 2017.},\nkeywords={Reusability;  System program documentation, Documentation tools;  Empirical studies;  High quality;  Javadoc;  Reuse;  Reuse mechanism;  Software project;  Source codes, Computer software reusability},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral141","name":"Nopol: Automatic Repair of Conditional Statement Bugs in Java Programs","authors":"Xuan, J. and Martinez, M. and DeMarco, F. and Clement, M. and Marcote, S.L. and Durieux, T. and Le Berre, D. and Monperrus, M.","year":2017,"base":["Geral"],"abstract":"We propose Nopol, an approach to automatic repair of buggy conditional statements (i.e., if-then-else statements). This approach takes a buggy program as well as a test suite as input and generates a patch with a conditional expression as output. The test suite is required to contain passing test cases to model the expected behavior of the program and at least one failing test case that reveals the bug to be repaired. The process of Nopol consists of three major phases. First, Nopol employs angelic fix localization to identify expected values of a condition during the test execution. Second, runtime trace collection is used to collect variables and their actual values, including primitive data types and objected-oriented features (e.g., nullness checks), to serve as building blocks for patch generation. Third, Nopol encodes these collected data into an instance of a Satisfiability Modulo Theory (SMT) problem; then a feasible solution to the SMT instance is translated back into a code patch. We evaluate Nopol on 22 real-world bugs (16 bugs with buggy if conditions and six bugs with missing preconditions) on two large open-source projects, namely Apache Commons Math and Apache Commons Lang. Empirical analysis on these bugs shows that our approach can effectively fix bugs with buggy if conditions and missing preconditions. We illustrate the capabilities and limitations of Nopol using case studies of real bug fixes. © 1976-2012 IEEE.","doi":"10.1109/TSE.2016.2560811","bibtex":"@ARTICLE{Asenov2017152,\nauthor={Asenov, D. and Guenat, B. and Müller, P. and Otth, M.},\ntitle={Precise version control of trees with line-based version control systems},\njournal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},\nyear={2017},\nvolume={10202 LNCS},\npages={152-169},\ndoi={10.1007/978-3-662-54494-5_9},\nnote={cited By 2},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016440185&doi=10.1007%2f978-3-662-54494-5_9&partnerID=40&md5=e6f98cec4495d91b2edfcbc02c1b17f6},\nabstract={Version control of tree structures, ubiquitous in software engineering, is typically performed on a textual encoding of the trees, rather than the trees directly. Applying standard line-based diff and merge algorithms to such encodings leads to inaccurate diffs, unnecessary conflicts, and incorrect merges. To address these problems, we propose novel algorithms for computing precise diffs between two versions of a tree and for three-way merging of trees. Unlike most other approaches for version control of structured data, our approach integrates with mainstream version control systems. Our merge algorithm can be customized for specific application domains to further improve merge results. An evaluation of our approach on abstract syntax trees from popular Java projects shows substantially improved merge results compared to Git. © Springer-Verlag GmbH Germany 2017.},\nkeywords={Computation theory;  Control systems;  Encoding (symbols);  Information management;  Software engineering;  Trees (mathematics), Abstract Syntax Trees;  Novel algorithm;  Software Evolution;  Structured data;  Structured editors;  Trees;  Version control;  Version control system, Computer control},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral142","name":"VulPecker: An automated vulnerability detection system based on code similarity analysis","authors":"Li, Z. and Zou, D. and Xu, S. and Jin, H. and Qi, H. and Hu, J.","year":2016,"base":["Geral"],"abstract":"Software vulnerabilities are the fundamental cause of many attacks. Even with rapid vulnerability patching, the problem is more complicated than it looks. One reason is that instances of the same vulnerability may exist in multiple software copies that are difficult to track in real life (e.g., different versions of libraries and applications). This calls for tools that can automatically search for vulnerable software with respect to a given vulnerability. In this paper, we move a step forward in this direction by presenting Vulnerability Pecker (VulPecker), a system for automatically detecting whether a piece of software source code contains a given vulnerability or not. The key insight underlying VulPecker is to leverage (i) a set of features that we define to characterize patches, and (ii) code-similarity algorithms that have been proposed for various purposes, while noting that no single code-similarity algorithm is effective for all kinds of vulnerabilities. Experiments show that VulPecker detects 40 vulnerabilities that are not published in the National Vulnerability Database (NVD). Among these vulnerabilities, 18 are not known for their existence and have yet to be confirmed by vendors at the time of writing (these vulnerabilities are \"anonymized\" in the present paper for ethical reasons), and the other 22 vulnerabilities have been \"silently\" patched by the vendors in the later releases of the vulnerable products. © 2016 ACM.","doi":"10.1145/2991079.2991102","bibtex":"@ARTICLE{Xuan201734,\nauthor={Xuan, J. and Martinez, M. and DeMarco, F. and Clement, M. and Marcote, S.L. and Durieux, T. and Le Berre, D. and Monperrus, M.},\ntitle={Nopol: Automatic Repair of Conditional Statement Bugs in Java Programs},\njournal={IEEE Transactions on Software Engineering},\nyear={2017},\nvolume={43},\nnumber={1},\npages={34-55},\ndoi={10.1109/TSE.2016.2560811},\nart_number={7463060},\nnote={cited By 43},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009892148&doi=10.1109%2fTSE.2016.2560811&partnerID=40&md5=af3ff1d95f717354a64369a6660f37b0},\nabstract={We propose Nopol, an approach to automatic repair of buggy conditional statements (i.e., if-then-else statements). This approach takes a buggy program as well as a test suite as input and generates a patch with a conditional expression as output. The test suite is required to contain passing test cases to model the expected behavior of the program and at least one failing test case that reveals the bug to be repaired. The process of Nopol consists of three major phases. First, Nopol employs angelic fix localization to identify expected values of a condition during the test execution. Second, runtime trace collection is used to collect variables and their actual values, including primitive data types and objected-oriented features (e.g., nullness checks), to serve as building blocks for patch generation. Third, Nopol encodes these collected data into an instance of a Satisfiability Modulo Theory (SMT) problem; then a feasible solution to the SMT instance is translated back into a code patch. We evaluate Nopol on 22 real-world bugs (16 bugs with buggy if conditions and six bugs with missing preconditions) on two large open-source projects, namely Apache Commons Math and Apache Commons Lang. Empirical analysis on these bugs shows that our approach can effectively fix bugs with buggy if conditions and missing preconditions. We illustrate the capabilities and limitations of Nopol using case studies of real bug fixes. © 1976-2012 IEEE.},\nkeywords={Computer software;  Java programming language;  Open source software;  Repair;  Software testing;  Surface mount technology, Conditional expressions;  Empirical analysis;  Fault localization;  Feasible solution;  Open source projects;  Oriented features;  patch generation;  Satisfiability modulo Theories, Program debugging},\ndocument_type={Article},\nsource={Scopus},\n}\n\n"},{"id":"Geral143","name":"Dodging Unsafe Update Points in Java Dynamic Software Updating Systems","authors":"Cazzola, W. and Jalili, M.","year":2016,"base":["Geral"],"abstract":"Dynamic Software Updating (DSU) provides mechanisms to update a program without stopping its execution. An indiscriminate update, that does not consider the current state of the computation, potentially undermines the stability of the running application. To automatically determine a safe moment when to update the running system is still an open problem often neglected from the existing DSU systems. This paper proposes a mechanism to support the choice of a safe update point by marking which point can be considered unsafe and therefore dodged during the update. The method is based on decorating the code with some specific meta-data that can be used to find the right moment to do the update. The proposed approach has been implemented as an external component that can be plugged into every DSU system. The approach is demonstrated on the evolution of the HSQLDB system from two distinct versions to their next update. © 2016 IEEE.","doi":"10.1109/ISSRE.2016.17","bibtex":"@CONFERENCE{Li2016201,\nauthor={Li, Z. and Zou, D. and Xu, S. and Jin, H. and Qi, H. and Hu, J.},\ntitle={VulPecker: An automated vulnerability detection system based on code similarity analysis},\njournal={ACM International Conference Proceeding Series},\nyear={2016},\nvolume={5-9-December-2016},\npages={201-213},\ndoi={10.1145/2991079.2991102},\nnote={cited By 8},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007566205&doi=10.1145%2f2991079.2991102&partnerID=40&md5=674029c8181f7ee6b08613f2826d7a0f},\nabstract={Software vulnerabilities are the fundamental cause of many attacks. Even with rapid vulnerability patching, the problem is more complicated than it looks. One reason is that instances of the same vulnerability may exist in multiple software copies that are difficult to track in real life (e.g., different versions of libraries and applications). This calls for tools that can automatically search for vulnerable software with respect to a given vulnerability. In this paper, we move a step forward in this direction by presenting Vulnerability Pecker (VulPecker), a system for automatically detecting whether a piece of software source code contains a given vulnerability or not. The key insight underlying VulPecker is to leverage (i) a set of features that we define to characterize patches, and (ii) code-similarity algorithms that have been proposed for various purposes, while noting that no single code-similarity algorithm is effective for all kinds of vulnerabilities. Experiments show that VulPecker detects 40 vulnerabilities that are not published in the National Vulnerability Database (NVD). Among these vulnerabilities, 18 are not known for their existence and have yet to be confirmed by vendors at the time of writing (these vulnerabilities are \"anonymized\" in the present paper for ethical reasons), and the other 22 vulnerabilities have been \"silently\" patched by the vendors in the later releases of the vulnerable products. © 2016 ACM.},\nkeywords={Codes (symbols);  Computer software;  Security of data;  Security systems, Code similarities;  National vulnerability database;  Software source codes;  Software vulnerabilities;  Vulnerability detection;  Vulnerability signature, Application programs},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral144","name":"Can testedness be effectively measured?","authors":"Ahmed, I. and Gopinath, R. and Brindescu, C. and Groce, A. and Jensen, C.","year":2016,"base":["Geral"],"abstract":"Among the major questions that a practicing tester faces are deciding where to focus additional testing effort, and decid-ing when to stop testing. Test the least-Tested code, and stop when all code is well-Tested, is a reasonable answer. Many measures of \"testedness\" have been proposed; unfortunately, we do not know whether these are truly effective. In this paper we propose a novel evaluation of two of the most important and widely-used measures of test suite qual-ity. The first measure is statement coverage, the simplest and best-known code coverage measure. The second mea-sure is mutation score, a supposedly more powerful, though expensive, measure. We evaluate these measures using the actual criteria of interest: if a program element is (by these measures) well tested at a given point in time, it should require fewer fu-ture bug-fixes than a \"poorly tested\" element. If not, then it seems likely that we are not effectively measuring tested-ness. Using a large number of open source Java programs from Github and Apache, we show that both statement cov-erage and mutation score have only a weak negative corre-lation with bug-fixes. Despite the lack of strong correlation, there are statistically and practically significant differences between program elements for various binary criteria. Pro-gram elements (other than classes) covered by any test case see about half as many bug-fixes as those not covered, and a similar line can be drawn for mutation score thresholds. Our results have important implications for both software engineering practice and research evaluation.","doi":"10.1145/2950290.2950324","bibtex":"@CONFERENCE{Cazzola2016332,\nauthor={Cazzola, W. and Jalili, M.},\ntitle={Dodging Unsafe Update Points in Java Dynamic Software Updating Systems},\njournal={Proceedings - International Symposium on Software Reliability Engineering, ISSRE},\nyear={2016},\npages={332-341},\ndoi={10.1109/ISSRE.2016.17},\nart_number={7774532},\nnote={cited By 1},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013309485&doi=10.1109%2fISSRE.2016.17&partnerID=40&md5=0c81b0442624355c1ea631ccf3123eab},\nabstract={Dynamic Software Updating (DSU) provides mechanisms to update a program without stopping its execution. An indiscriminate update, that does not consider the current state of the computation, potentially undermines the stability of the running application. To automatically determine a safe moment when to update the running system is still an open problem often neglected from the existing DSU systems. This paper proposes a mechanism to support the choice of a safe update point by marking which point can be considered unsafe and therefore dodged during the update. The method is based on decorating the code with some specific meta-data that can be used to find the right moment to do the update. The proposed approach has been implemented as an external component that can be plugged into every DSU system. The approach is demonstrated on the evolution of the HSQLDB system from two distinct versions to their next update. © 2016 IEEE.},\nkeywords={Computer software;  Dynamics, Dynamic software update;  Dynamic software updating;  Dynamic update;  External components;  JavAdaptor;  Running applications;  Running systems, Software reliability},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral145","name":"Towards the layout of things","authors":"Achten, P. and Stutterheim, J. and Lijnse, B. and Plasmeijer, R.","year":2016,"base":["Geral"],"abstract":"When writing a user interface (UI), the layout of its elements play an important role. Programmers should be able to specify the layout of UIs in an intuitive way, while being able to separate the concern of laying out the UI from the rest of the software implementation. Ideally, the same layout language can be used in multiple application domains, so the programmer only has to learn one set of layout concepts. In this paper we introduce such a general-purpose layout language. We obtain this language by abstracting from a layout language we have introduced in previous work for declaratively defining Scalable Vector Graphics (SVG). We show that this abstract layout language can be instantiated for multiple domains: the SVG library by which the language is inspired, ncurses-based text-based user interfaces, and iTasks. In all of these cases, a separation of concerns is maintained. © 2016 ACM.","doi":"10.1145/3064899.3064905","bibtex":"@CONFERENCE{Ahmed2016547,\nauthor={Ahmed, I. and Gopinath, R. and Brindescu, C. and Groce, A. and Jensen, C.},\ntitle={Can testedness be effectively measured?},\njournal={Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},\nyear={2016},\nvolume={13-18-November-2016},\npages={547-558},\ndoi={10.1145/2950290.2950324},\nnote={cited By 10},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997418602&doi=10.1145%2f2950290.2950324&partnerID=40&md5=dda5f661050c3d8d817424775170a2d3},\nabstract={Among the major questions that a practicing tester faces are deciding where to focus additional testing effort, and decid-ing when to stop testing. Test the least-Tested code, and stop when all code is well-Tested, is a reasonable answer. Many measures of \"testedness\" have been proposed; unfortunately, we do not know whether these are truly effective. In this paper we propose a novel evaluation of two of the most important and widely-used measures of test suite qual-ity. The first measure is statement coverage, the simplest and best-known code coverage measure. The second mea-sure is mutation score, a supposedly more powerful, though expensive, measure. We evaluate these measures using the actual criteria of interest: if a program element is (by these measures) well tested at a given point in time, it should require fewer fu-ture bug-fixes than a \"poorly tested\" element. If not, then it seems likely that we are not effectively measuring tested-ness. Using a large number of open source Java programs from Github and Apache, we show that both statement cov-erage and mutation score have only a weak negative corre-lation with bug-fixes. Despite the lack of strong correlation, there are statistically and practically significant differences between program elements for various binary criteria. Pro-gram elements (other than classes) covered by any test case see about half as many bug-fixes as those not covered, and a similar line can be drawn for mutation score thresholds. Our results have important implications for both software engineering practice and research evaluation.},\nkeywords={Codes (symbols);  Computer software;  Java programming language;  Open source software;  Software engineering, Coverage criteria;  Mutation testing;  Program elements;  Research evaluation;  Software engineering practices;  Sta-Tistical analysis;  Statement coverage;  Strong correlation, Software testing},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral146","name":"B-Refactoring: Automatic test code refactoring to improve dynamic analysis","authors":"Xuan, J. and Cornu, B. and Martinez, M. and Baudry, B. and Seinturier, L. and Monperrus, M.","year":2016,"base":["Geral"],"abstract":"Context: Developers design test suites to verify that software meets its expected behaviors. Many dynamic analysis techniques are performed on the exploitation of execution traces from test cases. In practice, one test case may imply various behaviors. However, the execution of a test case only yields one trace, which can hide the others. Objective: In this article, we propose a new technique of test code refactoring, called B-Refactoring. The idea behind B-Refactoring is to split a test case into small test fragments, which cover a simpler part of the control flow to provide better support for dynamic analysis. Method: For a given dynamic analysis technique, B-Refactoring monitors the execution of test cases and constructs small test cases without loss of the testability. We apply B-Refactoring to assist two existing analysis tasks: automatic repair of if-condition bugs and automatic analysis of exception contracts. Results: Experimental results show that B-Refactoring can effectively improve the execution traces of the test suite. Real-world bugs that could not be previously fixed with the original test suites are fixed after applying B-Refactoring; meanwhile, exception contracts are better verified via applying B-Refactoring to original test suites. Conclusions: We conclude that applying B-Refactoring improves the execution traces of test cases for dynamic analysis. This improvement can enhance existing dynamic analysis tasks. © 2016 Elsevier B.V. All rights reserved.","doi":"10.1016/j.infsof.2016.04.016","bibtex":"@CONFERENCE{Achten2016,\nauthor={Achten, P. and Stutterheim, J. and Lijnse, B. and Plasmeijer, R.},\ntitle={Towards the layout of things},\njournal={ACM International Conference Proceeding Series},\nyear={2016},\ndoi={10.1145/3064899.3064905},\nart_number={3},\nnote={cited By 1},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019167069&doi=10.1145%2f3064899.3064905&partnerID=40&md5=a55779ee514a1e8177a11bea684c0ff2},\nabstract={When writing a user interface (UI), the layout of its elements play an important role. Programmers should be able to specify the layout of UIs in an intuitive way, while being able to separate the concern of laying out the UI from the rest of the software implementation. Ideally, the same layout language can be used in multiple application domains, so the programmer only has to learn one set of layout concepts. In this paper we introduce such a general-purpose layout language. We obtain this language by abstracting from a layout language we have introduced in previous work for declaratively defining Scalable Vector Graphics (SVG). We show that this abstract layout language can be instantiated for multiple domains: the SVG library by which the language is inspired, ncurses-based text-based user interfaces, and iTasks. In all of these cases, a separation of concerns is maintained. © 2016 ACM.},\nkeywords={Abstracting;  Graphical user interfaces;  User interfaces, Multiple applications;  Multiple domains;  Scalable vector graphics;  Separation of concerns;  Software implementation;  Task-oriented programming, Functional programming},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral147","name":"Tree edit distance: Robust and memory-efficient","authors":"Pawlik, M. and Augsten, N.","year":2016,"base":["Geral"],"abstract":"Hierarchical data are often modelled as trees. An interesting query identifies pairs of similar trees. The standard approach to tree similarity is the tree edit distance, which has successfully been applied in a wide range of applications. In terms of runtime, the state-of-the-art algorithm for the tree edit distance is RTED, which is guaranteed to be fast independent of the tree shape. Unfortunately, this algorithm requires up to twice the memory of its competitors. The memory is quadratic in the tree size and is a bottleneck for the tree edit distance computation. In this paper we present a new, memory efficient algorithm for the tree edit distance, AP-TED (All Path Tree Edit Distance). Our algorithm runs at least as fast as RTED without trading in memory efficiency. This is achieved by releasing memory early during the first step of the algorithm, which computes a decomposition strategy for the actual distance computation. We show the correctness of our approach and prove an upper bound for the memory usage. The strategy computed by AP-TED is optimal in the class of all-path strategies, which subsumes the class of LRH strategies used in RTED. We further present the AP-TED+ algorithm, which requires less computational effort for very small subtrees and improves the runtime of the distance computation. Our experimental evaluation confirms the low memory requirements and the runtime efficiency of our approach. © 2015 Elsevier Ltd.","doi":"10.1016/j.is.2015.08.004","bibtex":"@ARTICLE{Xuan201665,\nauthor={Xuan, J. and Cornu, B. and Martinez, M. and Baudry, B. and Seinturier, L. and Monperrus, M.},\ntitle={B-Refactoring: Automatic test code refactoring to improve dynamic analysis},\njournal={Information and Software Technology},\nyear={2016},\nvolume={76},\npages={65-80},\ndoi={10.1016/j.infsof.2016.04.016},\nnote={cited By 13},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84965136564&doi=10.1016%2fj.infsof.2016.04.016&partnerID=40&md5=7f80f2bb795e4b7ba2c1f6d6e0978c89},\nabstract={Context: Developers design test suites to verify that software meets its expected behaviors. Many dynamic analysis techniques are performed on the exploitation of execution traces from test cases. In practice, one test case may imply various behaviors. However, the execution of a test case only yields one trace, which can hide the others. Objective: In this article, we propose a new technique of test code refactoring, called B-Refactoring. The idea behind B-Refactoring is to split a test case into small test fragments, which cover a simpler part of the control flow to provide better support for dynamic analysis. Method: For a given dynamic analysis technique, B-Refactoring monitors the execution of test cases and constructs small test cases without loss of the testability. We apply B-Refactoring to assist two existing analysis tasks: automatic repair of if-condition bugs and automatic analysis of exception contracts. Results: Experimental results show that B-Refactoring can effectively improve the execution traces of the test suite. Real-world bugs that could not be previously fixed with the original test suites are fixed after applying B-Refactoring; meanwhile, exception contracts are better verified via applying B-Refactoring to original test suites. Conclusions: We conclude that applying B-Refactoring improves the execution traces of test cases for dynamic analysis. This improvement can enhance existing dynamic analysis tasks. © 2016 Elsevier B.V. All rights reserved.},\nkeywords={Information systems;  Software engineering, Automatic analysis;  Control flows;  Design tests;  Dynamic analysis techniques;  Execution trace;  Real-world;  Refactorings;  Testability, Software testing},\ndocument_type={Article},\nsource={Scopus},\n}\n\n"},{"id":"Geral148","name":"An automated framework for recommending program elements to novices","authors":"Zimmerman, K. and Rupakheti, C.R.","year":2016,"base":["Geral"],"abstract":"Novice programmers often learn programming by implementing well-known algorithms. There are several challenges in the process. Recommendation systems in software currently focus on programmer productivity and ease of development. Teaching aides for such novice programmers based on recommendation systems still remain an underexplored area. In this paper, we present a general framework for recognizing the desired target for partially-written code and recommending a reliable series of edits to transform the input program into the target solution. Our code analysis is based on graph matching and tree edit algorithms. Our experimental results show that efficient graph comparison techniques can accurately match two portions of source code and produce an accurate set of source code edits. We provide details on implementation of our framework, which is developed as a plugin for Java in Eclipse IDE. © 2015 IEEE.","doi":"10.1109/ASE.2015.54","bibtex":"@ARTICLE{Pawlik2016157,\nauthor={Pawlik, M. and Augsten, N.},\ntitle={Tree edit distance: Robust and memory-efficient},\njournal={Information Systems},\nyear={2016},\nvolume={56},\npages={157-173},\ndoi={10.1016/j.is.2015.08.004},\nnote={cited By 20},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945588965&doi=10.1016%2fj.is.2015.08.004&partnerID=40&md5=5860933103c2bf5e9ff8cd90b7348d4b},\nabstract={Hierarchical data are often modelled as trees. An interesting query identifies pairs of similar trees. The standard approach to tree similarity is the tree edit distance, which has successfully been applied in a wide range of applications. In terms of runtime, the state-of-the-art algorithm for the tree edit distance is RTED, which is guaranteed to be fast independent of the tree shape. Unfortunately, this algorithm requires up to twice the memory of its competitors. The memory is quadratic in the tree size and is a bottleneck for the tree edit distance computation. In this paper we present a new, memory efficient algorithm for the tree edit distance, AP-TED (All Path Tree Edit Distance). Our algorithm runs at least as fast as RTED without trading in memory efficiency. This is achieved by releasing memory early during the first step of the algorithm, which computes a decomposition strategy for the actual distance computation. We show the correctness of our approach and prove an upper bound for the memory usage. The strategy computed by AP-TED is optimal in the class of all-path strategies, which subsumes the class of LRH strategies used in RTED. We further present the AP-TED+ algorithm, which requires less computational effort for very small subtrees and improves the runtime of the distance computation. Our experimental evaluation confirms the low memory requirements and the runtime efficiency of our approach. © 2015 Elsevier Ltd.},\nkeywords={Algorithms;  Efficiency;  Forestry, Approximate matching;  Computational effort;  Decomposition strategy;  Experimental evaluation;  Memory-efficient algorithms;  Similarity search;  State-of-the-art algorithms;  Tree edit distance, Trees (mathematics)},\ndocument_type={Article},\nsource={Scopus},\n}\n\n"},{"id":"Geral149","name":"Semantic slicing of software version histories","authors":"Li, Y. and Rubin, J. and Chechik, M.","year":2016,"base":["Geral"],"abstract":"Software developers often need to transfer func-tionality, e.g., a set of commits implementing a new feature or a bug fix, from one branch of a configuration management system to another. That can be a challenging task as the existing configuration management tools lack support for matching high-level semantic functionality with low-level version histories. The developer thus has to either manually identify the exact set of semantically-related commits implementing the functionality of interest or sequentially port a specific subset of the change history, \"inheriting\" additional, unwanted functionality. In this paper, we tackle this problem by providing automated support for identifying the set of semantically-related commits implementing a particular functionality, which is defined by a set of tests. We refer to our approach, CSLICER, as semantic slicing of version histories. We formally define the semantic slicing problem, provide an algorithm for identifying a set of commits that constitute a slice, and instantiate it in a specific implementation for Java projects managed in Git. We evaluate the correctness and effectiveness of our approach on a set of open-source software repositories. We show that it allows to identify subsets of change histories that maintain the functionality of interest but are substantially smaller than the original ones. © 2015 IEEE.","doi":"10.1109/ASE.2015.47","bibtex":"@CONFERENCE{Zimmerman2016283,\nauthor={Zimmerman, K. and Rupakheti, C.R.},\ntitle={An automated framework for recommending program elements to novices},\njournal={Proceedings - 2015 30th IEEE/ACM International Conference on Automated Software Engineering, ASE 2015},\nyear={2016},\npages={283-288},\ndoi={10.1109/ASE.2015.54},\nart_number={7372017},\nnote={cited By 3},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963811311&doi=10.1109%2fASE.2015.54&partnerID=40&md5=69e6acbdc11855fcc18984dc6c6ab7a0},\nabstract={Novice programmers often learn programming by implementing well-known algorithms. There are several challenges in the process. Recommendation systems in software currently focus on programmer productivity and ease of development. Teaching aides for such novice programmers based on recommendation systems still remain an underexplored area. In this paper, we present a general framework for recognizing the desired target for partially-written code and recommending a reliable series of edits to transform the input program into the target solution. Our code analysis is based on graph matching and tree edit algorithms. Our experimental results show that efficient graph comparison techniques can accurately match two portions of source code and produce an accurate set of source code edits. We provide details on implementation of our framework, which is developed as a plugin for Java in Eclipse IDE. © 2015 IEEE.},\nkeywords={Algorithms;  Automation;  Codes (symbols);  Computer programming;  Pattern matching;  Software engineering;  Trees (mathematics), Comparison techniques;  Graph matchings;  Input programs;  Novice programmer;  Program elements;  Programmer productivity;  Recommendation Framework;  Target solution, Recommender systems},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral150","name":"TDDViz: Using software changes to understand conformance to Test Driven Development","authors":"Hilton, M. and Nelson, N. and McDonald, H. and McDonald, S. and Metoyer, R. and Dig, D.","year":2016,"base":["Geral"],"abstract":"A bad software development process leads to wasted effort and inferior products. In order to improve a software process, it must be first understood. Our unique approach in this paper uses code and test changes to understand conformance to the Test Driven Development (TDD) process. We designed and implemented TDDViz, a tool that supports developers in better understanding how they conform to TDD. TDDViz supports this understanding by providing novel visualizations of developers’ TDD process. To enable TDDViz’s visualizations, we developed a novel automatic inferencer that identifies the phases that make up the TDD process solely based on code and test changes. We evaluate TDDViz using two complementary methods: a controlled experiment with 35 participants to evaluate the visualization, and a case study with 2601 TDD Sessions to evaluate the inference algorithm. The controlled experiment shows that, in comparison to existing visualizations, participants performed significantly better when using TDDViz to answer questions about code evolution. In addition, the case study shows that the inferencing algorithm in TDDViz infers TDD phases with an accuracy (F-measure) of 87%. © The Author(s) 2016.","doi":"10.1007/978-3-319-33515-5_5","bibtex":"@CONFERENCE{Li2016686,\nauthor={Li, Y. and Rubin, J. and Chechik, M.},\ntitle={Semantic slicing of software version histories},\njournal={Proceedings - 2015 30th IEEE/ACM International Conference on Automated Software Engineering, ASE 2015},\nyear={2016},\npages={686-696},\ndoi={10.1109/ASE.2015.47},\nart_number={7372056},\nnote={cited By 12},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963930546&doi=10.1109%2fASE.2015.47&partnerID=40&md5=1fba178ec894f35c270041b39bc4480f},\nabstract={Software developers often need to transfer func-tionality, e.g., a set of commits implementing a new feature or a bug fix, from one branch of a configuration management system to another. That can be a challenging task as the existing configuration management tools lack support for matching high-level semantic functionality with low-level version histories. The developer thus has to either manually identify the exact set of semantically-related commits implementing the functionality of interest or sequentially port a specific subset of the change history, \"inheriting\" additional, unwanted functionality. In this paper, we tackle this problem by providing automated support for identifying the set of semantically-related commits implementing a particular functionality, which is defined by a set of tests. We refer to our approach, CSLICER, as semantic slicing of version histories. We formally define the semantic slicing problem, provide an algorithm for identifying a set of commits that constitute a slice, and instantiate it in a specific implementation for Java projects managed in Git. We evaluate the correctness and effectiveness of our approach on a set of open-source software repositories. We show that it allows to identify subsets of change histories that maintain the functionality of interest but are substantially smaller than the original ones. © 2015 IEEE.},\nkeywords={Automation;  Open systems;  Semantics;  Software engineering, Automated support;  Configuration management systems;  Configuration management tools;  Dependency;  High level semantics;  Software change;  Software developer;  Software versions, Open source software},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral151","name":"Inferring student coding goals using abstract syntax trees","authors":"Freeman, P. and Watson, I. and Denny, P.","year":2016,"base":["Geral"],"abstract":"The rapidly growing demand for programming skills has driven improvements in the technologies delivering programming education to students. Intelligent tutoring systems will potentially contribute to solving this problem, but development of effective systems has been slow to take hold in this area. We present a novel alternative, abstract Syntax Tree Retrieval, which uses case-based reasoning to infer student goals from previous solutions to coding problems. Without requiring programmed expert knowledge, our system demonstrates that accurate retrieval is possible for basic problems. We expect that additional research will uncover more applications for this technology, including more effective intelligent tutoring systems. © Springer International Publishing AG 2016.","doi":"10.1007/978-3-319-47096-2_10","bibtex":"@ARTICLE{Hilton201653,\nauthor={Hilton, M. and Nelson, N. and McDonald, H. and McDonald, S. and Metoyer, R. and Dig, D.},\ntitle={TDDViz: Using software changes to understand conformance to Test Driven Development},\njournal={Lecture Notes in Business Information Processing},\nyear={2016},\nvolume={251},\npages={53-65},\ndoi={10.1007/978-3-319-33515-5_5},\nnote={cited By 2},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971575025&doi=10.1007%2f978-3-319-33515-5_5&partnerID=40&md5=68c5ff44024c0b8d7b776de4e4a20c1a},\nabstract={A bad software development process leads to wasted effort and inferior products. In order to improve a software process, it must be first understood. Our unique approach in this paper uses code and test changes to understand conformance to the Test Driven Development (TDD) process. We designed and implemented TDDViz, a tool that supports developers in better understanding how they conform to TDD. TDDViz supports this understanding by providing novel visualizations of developers’ TDD process. To enable TDDViz’s visualizations, we developed a novel automatic inferencer that identifies the phases that make up the TDD process solely based on code and test changes. We evaluate TDDViz using two complementary methods: a controlled experiment with 35 participants to evaluate the visualization, and a case study with 2601 TDD Sessions to evaluate the inference algorithm. The controlled experiment shows that, in comparison to existing visualizations, participants performed significantly better when using TDDViz to answer questions about code evolution. In addition, the case study shows that the inferencing algorithm in TDDViz infers TDD phases with an accuracy (F-measure) of 87%. © The Author(s) 2016.},\nkeywords={Codes (symbols);  Computer programming;  Inference engines;  Software engineering;  Software testing;  Visualization, Complementary methods;  Controlled experiment;  Development process;  Inference algorithm;  Novel visualizations;  Software process;  Software visualization;  Test driven development, Software design},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral152","name":"Improving pattern tracking with a language-aware tree differencing algorithm","authors":"Palix, N. and Falleri, J.-R. and Lawall, J.","year":2015,"base":["Geral"],"abstract":"Tracking code fragments of interest is important in monitoring a software project over multiple versions. Various approaches, including our previous work on Herodotos, exploit the notion of Longest Common Subsequence, as computed by readily available tools such as GNU Diff, to map corresponding code fragments. Nevertheless, the efficient code differencing algorithms are typically line-based or word-based, and thus do not report changes at the level of language constructs. Furthermore, they identify only additions and removals, but not the moving of a block of code from one part of a file to another. Code fragments of interest that fall within the added and removed regions of code have to be manually correlated across versions, which is tedious and error-prone. When studying a very large code base over a long time, the number of manual correlations can become an obstacle to the success of a study. In this paper, we investigate the effect of replacing the current line-based algorithm used by Herodotos by tree-matching, as provided by the algorithm of the differencing tool GumTree. In contrast to the line-based approach, the tree-based approach does not generate any manual correlations, but it incurs a high execution time. To address the problem, we propose a hybrid strategy that gives the best of both approaches. © 2015 IEEE.","doi":"10.1109/SANER.2015.7081814","bibtex":"@ARTICLE{Freeman2016139,\nauthor={Freeman, P. and Watson, I. and Denny, P.},\ntitle={Inferring student coding goals using abstract syntax trees},\njournal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},\nyear={2016},\nvolume={9969 LNAI},\npages={139-153},\ndoi={10.1007/978-3-319-47096-2_10},\nnote={cited By 1},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994876195&doi=10.1007%2f978-3-319-47096-2_10&partnerID=40&md5=242bcf984472bccc7cb93c5219678c03},\nabstract={The rapidly growing demand for programming skills has driven improvements in the technologies delivering programming education to students. Intelligent tutoring systems will potentially contribute to solving this problem, but development of effective systems has been slow to take hold in this area. We present a novel alternative, abstract Syntax Tree Retrieval, which uses case-based reasoning to infer student goals from previous solutions to coding problems. Without requiring programmed expert knowledge, our system demonstrates that accurate retrieval is possible for basic problems. We expect that additional research will uncover more applications for this technology, including more effective intelligent tutoring systems. © Springer International Publishing AG 2016.},\nkeywords={Computer aided instruction;  Problem solving;  Students;  Syntactics;  Trees (mathematics);  XML, Abstract Syntax Trees;  Coding problems;  Effective systems;  Expert knowledge;  Growing demand;  Intelligent tutoring system;  Programming education;  Programming skills, Case based reasoning},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral153","name":"An historical analysis of the seandroid policy evolution","authors":"Im, B. and Chen, A. and Wallach, D.S.","year":2018,"base":["Geral"],"abstract":"Android adopted SELinux's mandatory access control (MAC) mechanisms in 2013. Since then, billions of Android devices have benefited from mandatory access control security policies. These policies are expressed in a variety of rules, maintained by Google and extended by Android OEMs. Over the years, the rules have grown to be quite complex, making it challenging to properly understand or configure these policies. In this paper, we perform a measurement study on the SEAndroid repository to understand the evolution of these policies. We propose a new metric to measure the complexity of the policy by expanding policy rules, with their abstraction features such as macros and groups, into primitive “boxes”, which we then use to show that the complexity of the SEAndroid policies has been growing exponentially over time. By analyzing the Git commits, snapshot by snapshot, we are also able to analyze the “age” of policy rules, the trend of changes, and the contributor composition. We also look at hallmark events in Android's history, such as the “Stagefright” vulnerability in Android's media facilities, pointing out how these events led to changes in the MAC policies. The growing complexity of Android's mandatory policies suggests that we will eventually hit the limits of our ability to understand these policies, requiring new tools and techniques. © 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.","doi":"10.1145/3274694.3274709","bibtex":"@CONFERENCE{Palix201543,\nauthor={Palix, N. and Falleri, J.-R. and Lawall, J.},\ntitle={Improving pattern tracking with a language-aware tree differencing algorithm},\njournal={2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering, SANER 2015 - Proceedings},\nyear={2015},\npages={43-52},\ndoi={10.1109/SANER.2015.7081814},\nart_number={7081814},\nnote={cited By 5},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928689087&doi=10.1109%2fSANER.2015.7081814&partnerID=40&md5=b73a9cd4ad9daa13011bf3e039b8d2ff},\nabstract={Tracking code fragments of interest is important in monitoring a software project over multiple versions. Various approaches, including our previous work on Herodotos, exploit the notion of Longest Common Subsequence, as computed by readily available tools such as GNU Diff, to map corresponding code fragments. Nevertheless, the efficient code differencing algorithms are typically line-based or word-based, and thus do not report changes at the level of language constructs. Furthermore, they identify only additions and removals, but not the moving of a block of code from one part of a file to another. Code fragments of interest that fall within the added and removed regions of code have to be manually correlated across versions, which is tedious and error-prone. When studying a very large code base over a long time, the number of manual correlations can become an obstacle to the success of a study. In this paper, we investigate the effect of replacing the current line-based algorithm used by Herodotos by tree-matching, as provided by the algorithm of the differencing tool GumTree. In contrast to the line-based approach, the tree-based approach does not generate any manual correlations, but it incurs a high execution time. To address the problem, we propose a hybrid strategy that gives the best of both approaches. © 2015 IEEE.},\nkeywords={Algorithms;  Codes (symbols);  Computational linguistics;  Image matching;  Open source software, Code metrics;  Code tracking;  Differencing algorithm;  Differencing tools;  Language constructs;  Longest common subsequences;  Tree-based approach;  Tree-matching, Trees (mathematics)},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral154","name":"Predicting future developer behavior in the IDE using topic models","authors":"Damevski, K. and Chen, H. and Shepherd, D.C. and Kraft, N.A. and Pollock, L.","year":2018,"base":["Geral"],"abstract":"While early software command recommender systems drew negative user reaction, recent studies show that users of unusually complex applications will accept and utilize command recommendations. Given this new interest, more than a decade after first attempts, both the recommendation generation (backend) and the user experience (frontend) should be revisited. In this work, we focus on recommendation generation. One shortcoming of existing command recommenders is that algorithms focus primarily on mirroring the short-term past,-i.e., assuming that a developer who is currently debugging will continue to debug endlessly. We propose an approach to improve on the state of the art by modeling future task context to make better recommendations to developers. That is, the approach can predict that a developer who is currently debugging may continue to debug OR may edit their program. To predict future development commands, we applied Temporal Latent Dirichlet Allocation, a topic model used primarily for natural language, to software development interaction data (i.e., command streams). We evaluated this approach on two large interaction datasets for two different IDEs, Microsoft Visual Studio and ABB Robot Studio. Our evaluation shows that this is a promising approach for both predicting future IDE commands and producing empirically-interpretable observations. © 1976-2012 IEEE.","doi":"10.1109/TSE.2017.2748134","bibtex":"@CONFERENCE{Im2018629,\nauthor={Im, B. and Chen, A. and Wallach, D.S.},\ntitle={An historical analysis of the seandroid policy evolution},\njournal={ACM International Conference Proceeding Series},\nyear={2018},\npages={629-640},\ndoi={10.1145/3274694.3274709},\nnote={cited By 0},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060064966&doi=10.1145%2f3274694.3274709&partnerID=40&md5=2227f51c419a3e0dd94ea30eca4d4183},\nabstract={Android adopted SELinux's mandatory access control (MAC) mechanisms in 2013. Since then, billions of Android devices have benefited from mandatory access control security policies. These policies are expressed in a variety of rules, maintained by Google and extended by Android OEMs. Over the years, the rules have grown to be quite complex, making it challenging to properly understand or configure these policies. In this paper, we perform a measurement study on the SEAndroid repository to understand the evolution of these policies. We propose a new metric to measure the complexity of the policy by expanding policy rules, with their abstraction features such as macros and groups, into primitive “boxes”, which we then use to show that the complexity of the SEAndroid policies has been growing exponentially over time. By analyzing the Git commits, snapshot by snapshot, we are also able to analyze the “age” of policy rules, the trend of changes, and the contributor composition. We also look at hallmark events in Android's history, such as the “Stagefright” vulnerability in Android's media facilities, pointing out how these events led to changes in the MAC policies. The growing complexity of Android's mandatory policies suggests that we will eventually hit the limits of our ability to understand these policies, requiring new tools and techniques. © 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.},\nkeywords={Access control;  Android (operating system);  Security systems, Android;  Historical analysis;  Mandatory access control;  Measurement study;  SEAndroid;  Security;  SELinux;  Tools and techniques, Mobile security},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral155","name":"Do code data sharing dependencies support an early prediction of software actual change impact set?","authors":"Liu, X. and Huang, L. and Egyed, A. and Ge, J.","year":2018,"base":["Geral"],"abstract":"Existing studies have shown that structural dependencies within code are good predictors for code actual change impact set—a set of entities that repeatedly changing together to ensure a consistent and complete change. However, the result is far from ideal, particularly when insufficient historical data are available at an early stage of software development. This paper demonstrates that a better understanding of data dependencies in addition to call dependencies greatly improves actual change impact set prediction. We propose a new approach and tool (namely, CHIP) to predict software actual change impact sets leveraging both call and data sharing dependencies. For this purpose, CHIP employs novel extensions (dependency frequency filtering and shared data type idf filtering) to reduce false positives. CHIP assumes that developers know initial places where to start making changes in the source code even though they may not know all changes. This approach has been empirically evaluated on 4 large-scale open source systems. Our evaluation demonstrates that data sharing dependencies have a complementary impact on software actual change impact set prediction as compared with predictions based on call dependencies only. CHIP improves the F2-score compared with the predictors using both Program Dependence Graph and evolutionary couplings. © 2018 John Wiley & Sons, Ltd.","doi":"10.1002/smr.1960","bibtex":"@ARTICLE{Damevski20181100,\nauthor={Damevski, K. and Chen, H. and Shepherd, D.C. and Kraft, N.A. and Pollock, L.},\ntitle={Predicting future developer behavior in the IDE using topic models},\njournal={IEEE Transactions on Software Engineering},\nyear={2018},\nvolume={44},\nnumber={11},\npages={1100-1111},\ndoi={10.1109/TSE.2017.2748134},\nart_number={8024001},\nnote={cited By 0},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029172869&doi=10.1109%2fTSE.2017.2748134&partnerID=40&md5=09dd407f1e04978a96727f4f13ece9f2},\nabstract={While early software command recommender systems drew negative user reaction, recent studies show that users of unusually complex applications will accept and utilize command recommendations. Given this new interest, more than a decade after first attempts, both the recommendation generation (backend) and the user experience (frontend) should be revisited. In this work, we focus on recommendation generation. One shortcoming of existing command recommenders is that algorithms focus primarily on mirroring the short-term past,-i.e., assuming that a developer who is currently debugging will continue to debug endlessly. We propose an approach to improve on the state of the art by modeling future task context to make better recommendations to developers. That is, the approach can predict that a developer who is currently debugging may continue to debug OR may edit their program. To predict future development commands, we applied Temporal Latent Dirichlet Allocation, a topic model used primarily for natural language, to software development interaction data (i.e., command streams). We evaluated this approach on two large interaction datasets for two different IDEs, Microsoft Visual Studio and ABB Robot Studio. Our evaluation shows that this is a promising approach for both predicting future IDE commands and producing empirically-interpretable observations. © 1976-2012 IEEE.},\nkeywords={Analytical models;  Application programs;  Data reduction;  Data structures;  Flow visualization;  Integrodifferential equations;  Recommender systems, Adaptation models;  Complex applications;  Developer behavior;  IDE interaction data;  Natural languages;  Predictive models;  Software commands;  User experience, Data visualization},\ndocument_type={Article},\nsource={Scopus},\n}\n\n"},{"id":"Geral156","name":"Method Combining Structural and Semantic Features to Support Code Commenting Decision [融合结构与语义特征的代码注释决策支持方法]","authors":"Huang, Y. and Jia, N. and Zhou, Q. and Chen, X.-P. and Xiong, Y.-F. and Luo, X.-N.","year":2018,"base":["Geral"],"abstract":"Code comment is quite important to help developer review and comprehend source code. Strategic comment decision is desired to cover core code snippets of software system without incurring unintended trivial comments. However, in current practice, there is a lack of rigorous specifications for developers to make their comment decisions. Commenting has become an important yet tough decision which mostly depends on the personal experience of developers. To reduce the effort on making comment decisions, this paper investigates a unified commenting regulation from a large number of commenting instances. A method, CommentAdviser, is proposed to guide developers in placing comments in source code. Since making comment is closely related to the context information of source code themselves, the method identifies this important factor for determining where to comment and extract them as structural context feature and semantic context feature. Next, machine learning techniques are applied to identify the possible commenting locations in source code. CommentAdviser is evaluated on 10 data sets from GitHub. The experimental results, as well as a user study, demonstrate the feasibility and effectiveness of CommentAdviser. © Copyright 2018, Institute of Software, the Chinese Academy of Sciences. All rights reserved.","doi":"10.13328/j.cnki.jos.005528","bibtex":"@ARTICLE{Liu2018,\nauthor={Liu, X. and Huang, L. and Egyed, A. and Ge, J.},\ntitle={Do code data sharing dependencies support an early prediction of software actual change impact set?},\njournal={Journal of Software: Evolution and Process},\nyear={2018},\nvolume={30},\nnumber={11},\ndoi={10.1002/smr.1960},\nart_number={e1960},\nnote={cited By 0},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050919825&doi=10.1002%2fsmr.1960&partnerID=40&md5=e13c43d3695fb9fdb2516dfdfac566fc},\nabstract={Existing studies have shown that structural dependencies within code are good predictors for code actual change impact set—a set of entities that repeatedly changing together to ensure a consistent and complete change. However, the result is far from ideal, particularly when insufficient historical data are available at an early stage of software development. This paper demonstrates that a better understanding of data dependencies in addition to call dependencies greatly improves actual change impact set prediction. We propose a new approach and tool (namely, CHIP) to predict software actual change impact sets leveraging both call and data sharing dependencies. For this purpose, CHIP employs novel extensions (dependency frequency filtering and shared data type idf filtering) to reduce false positives. CHIP assumes that developers know initial places where to start making changes in the source code even though they may not know all changes. This approach has been empirically evaluated on 4 large-scale open source systems. Our evaluation demonstrates that data sharing dependencies have a complementary impact on software actual change impact set prediction as compared with predictions based on call dependencies only. CHIP improves the F2-score compared with the predictors using both Program Dependence Graph and evolutionary couplings. © 2018 John Wiley & Sons, Ltd.},\nkeywords={Codes (symbols);  Forecasting;  Open source software;  Software design, Data dependencies;  Data Sharing;  Early prediction;  Frequency filtering;  Impact analysis;  Open source system;  Program dependence graph;  Source codes, Open systems},\ndocument_type={Article},\nsource={Scopus},\n}\n\n"},{"id":"Geral157","name":"PSP-Finder: A defect detection method based on mining correlations from function call paths","authors":"Cui, Z. and Chen, X. and Mu, Y. and Pan, M. and Wang, R.","year":2018,"base":["Geral"],"abstract":"Large scale programs usually imply many programming rules, which are missing from the specification documents. However, if programmers violate these rules in the process of programming, it is possible to introduce software defects. Previous works on mining function call correlation patterns only use structural information of the program, while control flow, data flow or other semantic information of the program are not exploited in those approaches. As a result, the defect detecting ability is restricted and high false rate is caused. This paper proposes a defect detection method based on mining function call association rules from program paths, which can be provided by simple static analysis. Then, the programs are automatically checked against the function call association rules for detecting suspicious defects. Based on this approach, experiments are carried out on a group of open source projects. The experiment results show that this approach can improve the capability of detecting defects and find more bugs related to program execution path. In addition, the false positive function call patterns and the overhead for manually validating suspicious defects are reduced. © 2018 Chinese Institute of Electronics. All rights reserved.","doi":"10.1049/cje.2018.04.001","bibtex":"@ARTICLE{Huang20182226,\nauthor={Huang, Y. and Jia, N. and Zhou, Q. and Chen, X.-P. and Xiong, Y.-F. and Luo, X.-N.},\ntitle={Method Combining Structural and Semantic Features to Support Code Commenting Decision [融合结构与语义特征的代码注释决策支持方法]},\njournal={Ruan Jian Xue Bao/Journal of Software},\nyear={2018},\nvolume={29},\nnumber={8},\npages={2226-2242},\ndoi={10.13328/j.cnki.jos.005528},\nnote={cited By 0},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055558530&doi=10.13328%2fj.cnki.jos.005528&partnerID=40&md5=e6eb6abe09383934546ff21dbdc471fb},\nabstract={Code comment is quite important to help developer review and comprehend source code. Strategic comment decision is desired to cover core code snippets of software system without incurring unintended trivial comments. However, in current practice, there is a lack of rigorous specifications for developers to make their comment decisions. Commenting has become an important yet tough decision which mostly depends on the personal experience of developers. To reduce the effort on making comment decisions, this paper investigates a unified commenting regulation from a large number of commenting instances. A method, CommentAdviser, is proposed to guide developers in placing comments in source code. Since making comment is closely related to the context information of source code themselves, the method identifies this important factor for determining where to comment and extract them as structural context feature and semantic context feature. Next, machine learning techniques are applied to identify the possible commenting locations in source code. CommentAdviser is evaluated on 10 data sets from GitHub. The experimental results, as well as a user study, demonstrate the feasibility and effectiveness of CommentAdviser. © Copyright 2018, Institute of Software, the Chinese Academy of Sciences. All rights reserved.},\nkeywords={Artificial intelligence;  Computer programming languages;  Learning systems;  Semantics, Code comment;  Comment decision;  Context information;  Current practices;  Machine learning techniques;  Personal experience;  Semantic features;  Structural feature, Codes (symbols)},\ndocument_type={Article},\nsource={Scopus},\n}\n\n"},{"id":"Geral158","name":"Doppio: Tracking UI flows and code changes for app development","authors":"Chi, P.-Y. and Hu, S.-P. and Li, Y.","year":2018,"base":["Geral"],"abstract":"Developing interactive systems often involves a large set of callback functions for handling user interaction, which makes it challenging to manage UI behaviors, create descriptive documentation, and track code revisions. We developed Doppio, a tool that automatically tracks and visualizes UI flows and their changes based on source code. For each input event listener of a widget, e.g., onClick of an Android View class, Doppio captures and associates its UI output from a program execution with its code snippet from the codebase. It automatically generates a screenflow diagram organized by the callback methods and interaction flow, where developers can review the code and UI revisions interactively. Doppio, as an IDE plugin, is seamlessly integrated into a common development workflow. Our studies show that our tool is able to generate quality visual documentation and helped participants understand unfamiliar source code and track changes. © 2018 Copyright is held by the owner/author(s).","doi":"10.1145/3173574.3174029","bibtex":"@ARTICLE{Cui2018776,\nauthor={Cui, Z. and Chen, X. and Mu, Y. and Pan, M. and Wang, R.},\ntitle={PSP-Finder: A defect detection method based on mining correlations from function call paths},\njournal={Chinese Journal of Electronics},\nyear={2018},\nvolume={27},\nnumber={4},\npages={776-782},\ndoi={10.1049/cje.2018.04.001},\nnote={cited By 1},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051353929&doi=10.1049%2fcje.2018.04.001&partnerID=40&md5=d84f88abdd1d745d214486591b6fb3e7},\nabstract={Large scale programs usually imply many programming rules, which are missing from the specification documents. However, if programmers violate these rules in the process of programming, it is possible to introduce software defects. Previous works on mining function call correlation patterns only use structural information of the program, while control flow, data flow or other semantic information of the program are not exploited in those approaches. As a result, the defect detecting ability is restricted and high false rate is caused. This paper proposes a defect detection method based on mining function call association rules from program paths, which can be provided by simple static analysis. Then, the programs are automatically checked against the function call association rules for detecting suspicious defects. Based on this approach, experiments are carried out on a group of open source projects. The experiment results show that this approach can improve the capability of detecting defects and find more bugs related to program execution path. In addition, the false positive function call patterns and the overhead for manually validating suspicious defects are reduced. © 2018 Chinese Institute of Electronics. All rights reserved.},\ndocument_type={Article},\nsource={Scopus},\n}\n\n"},{"id":"Geral159","name":"Can Developers' Interaction Data Improve Change Recommendation?","authors":"Yamamori, A. and Hagward, A.M. and Kobayashi, T.","year":2017,"base":["Geral"],"abstract":"One of the most common causes of bugs is overlooking changes. To prevent bugs and improve the quality of the products, numerous studies have been undertaken on change guides based on logical couplings extracted from developers' past process histories, such as change history. While valuable change rules based on logical couplings can be gleaned found from the change history, these rules often fail to find appropriate candidates because the change histories in repositories only preserve a summary of changes between commits. We recently analyzed the interaction data produced by a developer in an integrated development environment. Such interaction data contains not only a detailed change history but also reference activities between commits. In this paper, we investigate whether logical couplings extracted from interaction data could improve change recommendation performance. We used the interaction data from actual open source development, not from the project only for this study. Experimental results obtained using the interaction data from actual open source development showed a significant improvement in the efficiency of the change recommendation process. The results also indicated improvement in the number of detected artifacts that the developer had forgot to change. © 2017 IEEE.","doi":"10.1109/COMPSAC.2017.79","bibtex":"@CONFERENCE{Chi2018,\nauthor={Chi, P.-Y. and Hu, S.-P. and Li, Y.},\ntitle={Doppio: Tracking UI flows and code changes for app development},\njournal={Conference on Human Factors in Computing Systems - Proceedings},\nyear={2018},\nvolume={2018-April},\ndoi={10.1145/3173574.3174029},\nnote={cited By 1},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046933574&doi=10.1145%2f3173574.3174029&partnerID=40&md5=6eab8da51464030181ec1111f4db713d},\nabstract={Developing interactive systems often involves a large set of callback functions for handling user interaction, which makes it challenging to manage UI behaviors, create descriptive documentation, and track code revisions. We developed Doppio, a tool that automatically tracks and visualizes UI flows and their changes based on source code. For each input event listener of a widget, e.g., onClick of an Android View class, Doppio captures and associates its UI output from a program execution with its code snippet from the codebase. It automatically generates a screenflow diagram organized by the callback methods and interaction flow, where developers can review the code and UI revisions interactively. Doppio, as an IDE plugin, is seamlessly integrated into a common development workflow. Our studies show that our tool is able to generate quality visual documentation and helped participants understand unfamiliar source code and track changes. © 2018 Copyright is held by the owner/author(s).},\nkeywords={Android (operating system);  Demonstrations;  Human computer interaction;  Human engineering, Android;  IDEs;  Mobile apps;  Screencast videos;  Screenflow diagram;  Software documentation, Codes (symbols)},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral160","name":"Mining Sequences of Developer Interactions in Visual Studio for Usage Smells","authors":"Damevski, K. and Shepherd, D.C. and Schneider, J. and Pollock, L.","year":2017,"base":["Geral"],"abstract":"In this paper, we present a semi-automatic approach for mining a large-scale dataset of IDE interactions to extract usage smells, i.e., inefficient IDE usage patterns exhibited by developers in the field. The approach outlined in this paper first mines frequent IDE usage patterns, filtered via a set of thresholds and by the authors, that are subsequently supported (or disputed) using a developer survey, in order to form usage smells. In contrast with conventional mining of IDE usage data, our approach identifies time-ordered sequences of developer actions that are exhibited by many developers in the field. This pattern mining workflow is resilient to the ample noise present in IDE datasets due to the mix of actions and events that these datasets typically contain. We identify usage patterns and smells that contribute to the understanding of the usability of Visual Studio for debugging, code search, and active file navigation, and, more broadly, to the understanding of developer behavior during these software development activities. Among our findings is the discovery that developers are reluctant to use conditional breakpoints when debugging, due to perceived IDE performance problems as well as due to the lack of error checking in specifying the conditional. © 2017 IEEE.","doi":"10.1109/TSE.2016.2592905","bibtex":"@CONFERENCE{Yamamori2017128,\nauthor={Yamamori, A. and Hagward, A.M. and Kobayashi, T.},\ntitle={Can Developers' Interaction Data Improve Change Recommendation?},\njournal={Proceedings - International Computer Software and Applications Conference},\nyear={2017},\nvolume={1},\npages={128-137},\ndoi={10.1109/COMPSAC.2017.79},\nart_number={8029600},\nnote={cited By 1},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029840337&doi=10.1109%2fCOMPSAC.2017.79&partnerID=40&md5=d71b48ed13627c82fe4a85574a521862},\nabstract={One of the most common causes of bugs is overlooking changes. To prevent bugs and improve the quality of the products, numerous studies have been undertaken on change guides based on logical couplings extracted from developers' past process histories, such as change history. While valuable change rules based on logical couplings can be gleaned found from the change history, these rules often fail to find appropriate candidates because the change histories in repositories only preserve a summary of changes between commits. We recently analyzed the interaction data produced by a developer in an integrated development environment. Such interaction data contains not only a detailed change history but also reference activities between commits. In this paper, we investigate whether logical couplings extracted from interaction data could improve change recommendation performance. We used the interaction data from actual open source development, not from the project only for this study. Experimental results obtained using the interaction data from actual open source development showed a significant improvement in the efficiency of the change recommendation process. The results also indicated improvement in the number of detected artifacts that the developer had forgot to change. © 2017 IEEE.},\nkeywords={Application programs;  Computer software maintenance;  Couplings;  Open source software, Change Guide;  Change impact analysis;  Integrated development environment;  Interaction Data;  Mining software repositories;  Open source development;  Process history;  Recommendation performance, Computer software},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral161","name":"Why we Refactor? Confessions of Github contributors","authors":"Silva, D. and Tsantalis, N. and Valente, M.T.","year":2016,"base":["Geral"],"abstract":"Refactoring is a widespread practice that helps developers to improve the maintainability and readability of their code. However, there is a limited number of studies empirically investigating the actual motivations behind specific refac-toring operations applied by developers. To fill this gap, we monitored Java projects hosted on GitHub to detect re-cently applied refactorings, and asked the developers to ex-plain the reasons behind their decision to refactor the code. By applying thematic analysis on the collected responses, we compiled a catalogue of 44 distinct motivations for 12 well-known refactoring types. We found that refactoring ac-tivity is mainly driven by changes in the requirements and much less by code smells. Extract Method is the most versatile refactoring operation serving 11 different purposes. Finally, we found evidence that the IDE used by the devel-opers affects the adoption of automated refactoring tools.","doi":"10.1145/2950290.2950305","bibtex":"@ARTICLE{Damevski2017359,\nauthor={Damevski, K. and Shepherd, D.C. and Schneider, J. and Pollock, L.},\ntitle={Mining Sequences of Developer Interactions in Visual Studio for Usage Smells},\njournal={IEEE Transactions on Software Engineering},\nyear={2017},\nvolume={43},\nnumber={4},\npages={359-371},\ndoi={10.1109/TSE.2016.2592905},\nart_number={7516714},\nnote={cited By 4},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018977446&doi=10.1109%2fTSE.2016.2592905&partnerID=40&md5=b0fbdc5052c12dba5a5b044b5ac0f506},\nabstract={In this paper, we present a semi-automatic approach for mining a large-scale dataset of IDE interactions to extract usage smells, i.e., inefficient IDE usage patterns exhibited by developers in the field. The approach outlined in this paper first mines frequent IDE usage patterns, filtered via a set of thresholds and by the authors, that are subsequently supported (or disputed) using a developer survey, in order to form usage smells. In contrast with conventional mining of IDE usage data, our approach identifies time-ordered sequences of developer actions that are exhibited by many developers in the field. This pattern mining workflow is resilient to the ample noise present in IDE datasets due to the mix of actions and events that these datasets typically contain. We identify usage patterns and smells that contribute to the understanding of the usability of Visual Studio for debugging, code search, and active file navigation, and, more broadly, to the understanding of developer behavior during these software development activities. Among our findings is the discovery that developers are reluctant to use conditional breakpoints when debugging, due to perceived IDE performance problems as well as due to the lack of error checking in specifying the conditional. © 2017 IEEE.},\nkeywords={Data mining;  Discovery wells;  Odors;  Program debugging;  Software design;  Studios, Developer behavior;  Development activity;  Large-scale dataset;  Pattern mining;  Performance problems;  Time-ordered sequences;  Usability analysis;  Usage data, Integrodifferential equations},\ndocument_type={Article},\nsource={Scopus},\n}\n\n"},{"id":"Geral162","name":"Evaluating the evaluations of code recommender systems: A reality check","authors":"Proksch, S. and Amann, S. and Nadi, S. and Mezini, M.","year":2016,"base":["Geral"],"abstract":"While researchers develop many new exciting code recommender systems, such as method-call completion, code-snippet completion, or code search, an accurate evaluation of such systems is always a challenge. We analyzed the current literature and found that most of the current evaluations rely on artificial queries extracted from released code, which begs the question: Do such evaluations reect real-life usages? To answer this question, we capture 6,189 fine-grained development histories from real IDE interactions. We use them as a ground truth and extract 7,157 real queries for a specific method-call recommender system. We compare the results of such real queries with different artificial evaluation strategies and check several assumptions that are repeatedly used in research, but never empirically evaluated. We find that an evolving context that is often observed in practice has a major effect on the prediction quality of recommender systems, but is not commonly reected in artificial evaluations. © 2016 ACM.","doi":"10.1145/2970276.2970330","bibtex":"@CONFERENCE{Silva2016858,\nauthor={Silva, D. and Tsantalis, N. and Valente, M.T.},\ntitle={Why we Refactor? Confessions of Github contributors},\njournal={Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},\nyear={2016},\nvolume={13-18-November-2016},\npages={858-870},\ndoi={10.1145/2950290.2950305},\nnote={cited By 33},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997428839&doi=10.1145%2f2950290.2950305&partnerID=40&md5=29c96c43c7fbcc798c112eee9b56c1ac},\nabstract={Refactoring is a widespread practice that helps developers to improve the maintainability and readability of their code. However, there is a limited number of studies empirically investigating the actual motivations behind specific refac-toring operations applied by developers. To fill this gap, we monitored Java projects hosted on GitHub to detect re-cently applied refactorings, and asked the developers to ex-plain the reasons behind their decision to refactor the code. By applying thematic analysis on the collected responses, we compiled a catalogue of 44 distinct motivations for 12 well-known refactoring types. We found that refactoring ac-tivity is mainly driven by changes in the requirements and much less by code smells. Extract Method is the most versatile refactoring operation serving 11 different purposes. Finally, we found evidence that the IDE used by the devel-opers affects the adoption of automated refactoring tools.},\nkeywords={Motivation;  Odors;  Software engineering, Code smell;  GitHub;  Refactoring tools;  Refactorings;  Software Evolution;  Thematic analysis, Codes (symbols)},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral163","name":"Mining high utility patterns from software executing traces","authors":"He, H. and Wang, J. and Xu, C. and Wang, H. and Ren, J.","year":2016,"base":["Geral"],"abstract":"Software behavior mining is quite meaningful work. Finding meaningful patterns can help program maintainers detect the exception and improve work efficiency. These high utility software executing patterns shed light on software behavior and capture unique characteristic of software traces. In this paper, an efficient method called SHUP- Miner (Software High Utility Pattern Mining) is proposed to mine high utility patterns from software executing traces. In the algorithm, we firstly define the utility of each event in the software executing traces according to their importance of the software behavior. Secondly, a novel structure called improved utility list shorted as IUL is put forward. It stores patterns position and utility information which contributes to pruning space and extending patterns. Moreover, pattern extending strategy and IUCS (Improved Utility Cooccurrence Structure) are incorpora, ted into SHUP-Miner to improve the efficiency. At last, vie conduct experiments on both real and synthetic datasets. The results show that SHUP-Miner incorporating the efficiency-enhanced strategies demonstrates impressive performance. © 2016, IJICIC Editorial Office. All rights reserved.","bibtex":"@CONFERENCE{Proksch2016111,\nauthor={Proksch, S. and Amann, S. and Nadi, S. and Mezini, M.},\ntitle={Evaluating the evaluations of code recommender systems: A reality check},\njournal={ASE 2016 - Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},\nyear={2016},\npages={111-121},\ndoi={10.1145/2970276.2970330},\nnote={cited By 5},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989170526&doi=10.1145%2f2970276.2970330&partnerID=40&md5=89470c873e1e038c1cd70392d1471069},\nabstract={While researchers develop many new exciting code recommender systems, such as method-call completion, code-snippet completion, or code search, an accurate evaluation of such systems is always a challenge. We analyzed the current literature and found that most of the current evaluations rely on artificial queries extracted from released code, which begs the question: Do such evaluations reect real-life usages? To answer this question, we capture 6,189 fine-grained development histories from real IDE interactions. We use them as a ground truth and extract 7,157 real queries for a specific method-call recommender system. We compare the results of such real queries with different artificial evaluation strategies and check several assumptions that are repeatedly used in research, but never empirically evaluated. We find that an evolving context that is often observed in practice has a major effect on the prediction quality of recommender systems, but is not commonly reected in artificial evaluations. © 2016 ACM.},\nkeywords={Codes (symbols);  Integrodifferential equations;  Recommender systems;  Software engineering, Artificial Evaluation;  Development history;  Empirical studies;  Evaluation strategies;  Fine grained;  Ground truth;  IDE Interaction Data;  Prediction quality, Quality control},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral164","name":"Experiences in scaling field studies of software developer behavior","authors":"Pollock, L.","year":2016,"base":["Geral"],"abstract":"Most of our current understanding of how programmers perform various software maintenance and evolution tasks is based on controlled studies or interviews, which are inherently limited in size, scope, and realism. Replicating controlled studies in the field can both explore the findings of these studies in wider contexts and study new factors that have not been previously encountered in the laboratory setting. While replicating controlled studies in the field seems like an obvious next step in scientific progress, it is a step that has rarely been attempted, in part due to its complexity, which requires not only the industrial knowhow to implement a robust, scalable system, but the academic knowledge of how to design rigorous studies. In this talk, I will describe a few examples of successfully scaled studies, contrast them with less successful cases (including our own), and provide lessons learned. I will share the importance of collecting targeted information instead of generic logs, the insight that automated data collection paired with followup surveys is a powerful tool, and the nuances around what researchers can and cannot expect working developers to tolerate for the sake of research. © 2016 ACM.","doi":"10.1145/2897022.2897838","bibtex":"@ARTICLE{He20161227,\nauthor={He, H. and Wang, J. and Xu, C. and Wang, H. and Ren, J.},\ntitle={Mining high utility patterns from software executing traces},\njournal={International Journal of Innovative Computing, Information and Control},\nyear={2016},\nvolume={12},\nnumber={4},\npages={1227-1239},\nnote={cited By 0},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84982899565&partnerID=40&md5=0b041b1fae4e7cb412c1e67e7abc790e},\nabstract={Software behavior mining is quite meaningful work. Finding meaningful patterns can help program maintainers detect the exception and improve work efficiency. These high utility software executing patterns shed light on software behavior and capture unique characteristic of software traces. In this paper, an efficient method called SHUP- Miner (Software High Utility Pattern Mining) is proposed to mine high utility patterns from software executing traces. In the algorithm, we firstly define the utility of each event in the software executing traces according to their importance of the software behavior. Secondly, a novel structure called improved utility list shorted as IUL is put forward. It stores patterns position and utility information which contributes to pruning space and extending patterns. Moreover, pattern extending strategy and IUCS (Improved Utility Cooccurrence Structure) are incorpora, ted into SHUP-Miner to improve the efficiency. At last, vie conduct experiments on both real and synthetic datasets. The results show that SHUP-Miner incorporating the efficiency-enhanced strategies demonstrates impressive performance. © 2016, IJICIC Editorial Office. All rights reserved.},\nkeywords={Data mining;  Efficiency;  Miners, High utility;  Meaningful works;  Novel structures;  Pattern mining;  Software behavior;  Synthetic datasets;  Utility software;  Work efficiency, Utility programs},\ndocument_type={Article},\nsource={Scopus},\n}\n\n"},{"id":"Geral165","name":"Towards better program obfuscation: Optimization via language models","authors":"Liu, H.","year":2016,"base":["Geral"],"abstract":"As a common practice in software development, program obfuscation aims at deterring reverse engineering and malicious attacks on released source or binary code. Owning ample obfuscation techniques, we have relatively little knowledge on how to most effectively use them. The biggest challenge lies in identifying the most useful combination of these techniques. We propose a unified framework to automatically generate and optimize obfuscation based on an obscurity language model and a Monte Carlo Markov Chain (MCMC) based search algorithm. We further instantiate it for JavaScript programs and developed the Closure∗ tool. Compared to the well-known Google Closure Compiler, Closure∗ outperforms its default setting by 26%. For programs which have already been well obfuscated, Closure∗ can still outperform by 22%. © 2016 Author.","doi":"10.1145/2889160.2891040","bibtex":"@CONFERENCE{Pollock20161,\nauthor={Pollock, L.},\ntitle={Experiences in scaling field studies of software developer behavior},\njournal={Proceedings - 3rd International Workshop on Software Engineering Research and Industrial Practice, SER and IP 2016},\nyear={2016},\npages={1-2},\ndoi={10.1145/2897022.2897838},\nnote={cited By 0},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974569541&doi=10.1145%2f2897022.2897838&partnerID=40&md5=22fd2d8603293561a7681e5620b35a90},\nabstract={Most of our current understanding of how programmers perform various software maintenance and evolution tasks is based on controlled studies or interviews, which are inherently limited in size, scope, and realism. Replicating controlled studies in the field can both explore the findings of these studies in wider contexts and study new factors that have not been previously encountered in the laboratory setting. While replicating controlled studies in the field seems like an obvious next step in scientific progress, it is a step that has rarely been attempted, in part due to its complexity, which requires not only the industrial knowhow to implement a robust, scalable system, but the academic knowledge of how to design rigorous studies. In this talk, I will describe a few examples of successfully scaled studies, contrast them with less successful cases (including our own), and provide lessons learned. I will share the importance of collecting targeted information instead of generic logs, the insight that automated data collection paired with followup surveys is a powerful tool, and the nuances around what researchers can and cannot expect working developers to tolerate for the sake of research. © 2016 ACM.},\nkeywords={Engineering research;  Industrial management;  Software engineering;  Technology transfer, Automated data collection;  Field studies;  Scalable systems;  Scientific progress;  Software developer;  Software maintenance and evolution, Industrial research},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral166","name":"Interactive exploration of developer interaction traces using a Hidden Markov Model","authors":"Damevski, K. and Chen, H. and Shepherd, D. and Pollock, L.","year":2016,"base":["Geral"],"abstract":"Using IDE usage data to analyze the behavior of software developers in the field, during the course of their daily work, can lend support to (or dispute) laboratory studies of developers. This paper describes a technique that leverages Hidden Markov Models (HMMs) as a means of mining high-level developer behavior from low-level IDE interaction traces of many developers in the field. HMMs use dual stochastic processes to model higher-level hidden behavior using observable input sequences of events. We propose an interactive approach of mining interpretable HMMs, based on guiding a human expert in building a high quality HMM in an iterative, one state at a time, manner. The final result is a model that is both representative of the field data and captures the field phenomena of interest. We apply our HMM construction approach to study debugging behavior, using a large IDE interaction dataset collected from nearly 200 developers at ABB, Inc. Our results highlight the different modes and constituent actions in debugging, exhibited by the developers in our dataset. © 2016 ACM.","doi":"10.1145/2901739.2901741","bibtex":"@CONFERENCE{Liu2016680,\nauthor={Liu, H.},\ntitle={Towards better program obfuscation: Optimization via language models},\njournal={Proceedings - International Conference on Software Engineering},\nyear={2016},\npages={680-682},\ndoi={10.1145/2889160.2891040},\nnote={cited By 4},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026665442&doi=10.1145%2f2889160.2891040&partnerID=40&md5=af16d582d793d1fff1c26ebf8dcd0f84},\nabstract={As a common practice in software development, program obfuscation aims at deterring reverse engineering and malicious attacks on released source or binary code. Owning ample obfuscation techniques, we have relatively little knowledge on how to most effectively use them. The biggest challenge lies in identifying the most useful combination of these techniques. We propose a unified framework to automatically generate and optimize obfuscation based on an obscurity language model and a Monte Carlo Markov Chain (MCMC) based search algorithm. We further instantiate it for JavaScript programs and developed the Closure∗ tool. Compared to the well-known Google Closure Compiler, Closure∗ outperforms its default setting by 26%. For programs which have already been well obfuscated, Closure∗ can still outperform by 22%. © 2016 Author.},\nkeywords={Computational linguistics;  Markov processes;  Optimization;  Reverse engineering;  Software engineering, JavaScript programs;  Language model;  Monte Carlo Markov chain;  Obfuscation;  Program obfuscation;  Random searches;  Search Algorithms;  Unified framework, Software design},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral167","name":"A Trace Mechanism of Change History in the Presence of Refactoring","authors":"Yoon, S. and Lee, C. and Park, S. and Hwang, M.","year":2015,"base":["Geral"],"abstract":"Nowadays, changes in software development are inevitable. Therefore, understanding and analysis of changes considered more important than anything else. The existing version control systems, which are used for effective change management, are impossible to trace changes in the presence of refactoring. There have been a number of approaches proposed for trace changes using operation-based mechanisms or rule-based mechanisms. However, it has many limitations with excessive recording or not sufficient coverage. In this approach, we define four rules based on M.Flower's refactoring techniques and find an origin method matching the pattern of changed source code with the pre-defined rules. The case study shows that the proposed mechanism is effective Also, case study looking for open source project code changes about the origin method show the proposed mechanism's effectiveness. © 2015 SERSC.","doi":"10.14257/ijseia.2015.9.9.03","bibtex":"@CONFERENCE{Damevski2016126,\nauthor={Damevski, K. and Chen, H. and Shepherd, D. and Pollock, L.},\ntitle={Interactive exploration of developer interaction traces using a Hidden Markov Model},\njournal={Proceedings - 13th Working Conference on Mining Software Repositories, MSR 2016},\nyear={2016},\npages={126-136},\ndoi={10.1145/2901739.2901741},\nnote={cited By 5},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974534250&doi=10.1145%2f2901739.2901741&partnerID=40&md5=f66cc7a27d811eec03bab028b42e429d},\nabstract={Using IDE usage data to analyze the behavior of software developers in the field, during the course of their daily work, can lend support to (or dispute) laboratory studies of developers. This paper describes a technique that leverages Hidden Markov Models (HMMs) as a means of mining high-level developer behavior from low-level IDE interaction traces of many developers in the field. HMMs use dual stochastic processes to model higher-level hidden behavior using observable input sequences of events. We propose an interactive approach of mining interpretable HMMs, based on guiding a human expert in building a high quality HMM in an iterative, one state at a time, manner. The final result is a model that is both representative of the field data and captures the field phenomena of interest. We apply our HMM construction approach to study debugging behavior, using a large IDE interaction dataset collected from nearly 200 developers at ABB, Inc. Our results highlight the different modes and constituent actions in debugging, exhibited by the developers in our dataset. © 2016 ACM.},\nkeywords={Integrodifferential equations;  Iterative methods;  Markov processes;  Random processes;  Stochastic models;  Stochastic systems;  Trellis codes, Construction approaches;  Developer behavior;  Hidden markov models (HMMs);  Input sequence;  Interactive approach;  Interactive exploration;  Laboratory studies;  Software developer, Hidden Markov models},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral168","name":"A state of the art survey of data mining techniques for software engineering data","authors":"Soni, M. and Singh, H. and Sethi, N.","year":2015,"base":["Geral"],"abstract":"Source code plugins are the additional features provided by the software applications in addition to main functionality. Violating coding rules in the source code of these applications can lead to error patterns in the source code revision histories. Various data mining techniques have been used to mine software engineering data to classify the information about the software system. Mining software engineering data aims at removing irrelevant data and outliers from revision histories of various software applications. It improves the process of software debugging along with software maintenance leading in high quality software by reducing cost and effort. This paper discusses various data mining approaches which are used in mining different software engineering data. © Research India Publications.","bibtex":"@ARTICLE{Yoon201519,\nauthor={Yoon, S. and Lee, C. and Park, S. and Hwang, M.},\ntitle={A Trace Mechanism of Change History in the Presence of Refactoring},\njournal={International Journal of Software Engineering and its Applications},\nyear={2015},\nvolume={9},\nnumber={9},\npages={19-28},\ndoi={10.14257/ijseia.2015.9.9.03},\nnote={cited By 0},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946898698&doi=10.14257%2fijseia.2015.9.9.03&partnerID=40&md5=cf44e5c86134004ee0003fb62b22def3},\nabstract={Nowadays, changes in software development are inevitable. Therefore, understanding and analysis of changes considered more important than anything else. The existing version control systems, which are used for effective change management, are impossible to trace changes in the presence of refactoring. There have been a number of approaches proposed for trace changes using operation-based mechanisms or rule-based mechanisms. However, it has many limitations with excessive recording or not sufficient coverage. In this approach, we define four rules based on M.Flower's refactoring techniques and find an origin method matching the pattern of changed source code with the pre-defined rules. The case study shows that the proposed mechanism is effective Also, case study looking for open source project code changes about the origin method show the proposed mechanism's effectiveness. © 2015 SERSC.},\ndocument_type={Article},\nsource={Scopus},\n}\n\n"},{"id":"Geral169","name":"Software mining studies: Goals, approaches, artifacts, and replicability","authors":"Amann, S. and Beyer, S. and Kevic, K. and Gall, H.","year":2015,"base":["Geral"],"abstract":"The mining of software archives has enabled new ways for increasing the productivity in software development: Analyzing software quality, mining project evolution, investigating change patterns and evolution trends, mining models for development processes, developing methods of integrating mined data from various historical sources, or analyzing natural language artifacts in software repositories, are examples of research topics. Software repositories include various data, ranging from source control systems, issue tracking systems, artifact repositories such as requirements, design and architectural documentation, to archived communication between project members. Practitioners and researchers have recognized the potential of mining these sources to support the maintenance of software, to improve their design or architecture, and to empirically validate development techniques or processes. We revisited software mining studies that were published in recent years in the top venues of software engineering, such as ICSE, ESEC/FSE, and MSR. In analyzing these software mining studies, we highlight different viewpoints: pursued goals, state-of-the-art approaches, mined artifacts, and study replicability. To analyze the mining artifacts, we (lexically) analyzed research papers of more than a decade. In terms of replicability we looked at existing work in the field in mining approaches, tools, and platforms. We address issues of replicability and reproducibility to shed light onto challenges for large-scale mining studies that would enable a stronger conclusion stability. © Springer International Publishing Switzerland 2015.","doi":"10.1007/978-3-319-28406-4_5","bibtex":"@ARTICLE{Soni20151512,\nauthor={Soni, M. and Singh, H. and Sethi, N.},\ntitle={A state of the art survey of data mining techniques for software engineering data},\njournal={International Journal of Applied Engineering Research},\nyear={2015},\nvolume={10},\nnumber={55},\npages={1512-1522},\nnote={cited By 0},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942920938&partnerID=40&md5=46bfa4bda0a0ba0ba691bcde685a3ede},\nabstract={Source code plugins are the additional features provided by the software applications in addition to main functionality. Violating coding rules in the source code of these applications can lead to error patterns in the source code revision histories. Various data mining techniques have been used to mine software engineering data to classify the information about the software system. Mining software engineering data aims at removing irrelevant data and outliers from revision histories of various software applications. It improves the process of software debugging along with software maintenance leading in high quality software by reducing cost and effort. This paper discusses various data mining approaches which are used in mining different software engineering data. © Research India Publications.},\ndocument_type={Article},\nsource={Scopus},\n}\n\n"},{"id":"Geral170","name":"An empirical evaluation and comparison of manual and automated test selection","authors":"Gligoric, M. and Negara, S. and Legunsen, O. and Marinov, D.","year":2014,"base":["Geral"],"abstract":"Regression test selection speeds up regression testing by rerunning only the tests that can be affected by the most recent code changes. Much progress has been made on research in automated test selection over the last three decades, but it has not translated into practical tools that are widely adopted. Therefore, developers either re-run all tests after each change or perform manual test selection. Re-running all tests is expensive, while manual test selection is tedious and error-prone. Despite such a big trade-off, no study assessed how developers perform manual test selection and compared it to automated test selection. This paper reports on our study of manual test selection in practice and our comparison of manual and automated test selection. We are the first to conduct a study that (1) analyzes data from manual test selection, collected in real time from 14 developers during a three-month study and (2) compares manual test selection with an automated state-of-the-research test-selection tool for 450 test sessions. Almost all developers in our study performed manual test selection, and they did so in mostly ad-hoc ways. Comparing manual and automated test selection, we found the two approaches to select different tests in each and every one of the 450 test sessions investigated. Manual selection chose more tests than automated selection 73% of the time (potentially wasting time) and chose fewer tests 27% of the time (potentially missing bugs). These results show the need for better automated test-selection techniques that integrate well with developers' programming environments. © 2014 ACM.","doi":"10.1145/2642937.2643019","bibtex":"@ARTICLE{Amann2015121,\nauthor={Amann, S. and Beyer, S. and Kevic, K. and Gall, H.},\ntitle={Software mining studies: Goals, approaches, artifacts, and replicability},\njournal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},\nyear={2015},\nvolume={8987},\npages={121-158},\ndoi={10.1007/978-3-319-28406-4_5},\nnote={cited By 5},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957030143&doi=10.1007%2f978-3-319-28406-4_5&partnerID=40&md5=7d7486a64aa5349348c2c00c6f013e33},\nabstract={The mining of software archives has enabled new ways for increasing the productivity in software development: Analyzing software quality, mining project evolution, investigating change patterns and evolution trends, mining models for development processes, developing methods of integrating mined data from various historical sources, or analyzing natural language artifacts in software repositories, are examples of research topics. Software repositories include various data, ranging from source control systems, issue tracking systems, artifact repositories such as requirements, design and architectural documentation, to archived communication between project members. Practitioners and researchers have recognized the potential of mining these sources to support the maintenance of software, to improve their design or architecture, and to empirically validate development techniques or processes. We revisited software mining studies that were published in recent years in the top venues of software engineering, such as ICSE, ESEC/FSE, and MSR. In analyzing these software mining studies, we highlight different viewpoints: pursued goals, state-of-the-art approaches, mined artifacts, and study replicability. To analyze the mining artifacts, we (lexically) analyzed research papers of more than a decade. In terms of replicability we looked at existing work in the field in mining approaches, tools, and platforms. We address issues of replicability and reproducibility to shed light onto challenges for large-scale mining studies that would enable a stronger conclusion stability. © Springer International Publishing Switzerland 2015.},\nkeywords={Big data;  Computer software selection and evaluation;  Data mining;  Software engineering;  Teaching, Artifact repositories;  Development process;  Development technique;  Large-scale mining;  Natural languages;  Software repositories;  Source control system;  State-of-the-art approach, Software design},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral171","name":"Providing Meaningful Feedback for Autograding of Programming Assignments","authors":"Haldeman, Georgiana and Tjang, Andrew and Babe\\c{s}-Vroman, Monica and Bartos, Stephen and Shah, Jay and Yucht, Danielle and Nguyen, Thu D.","year":2018,"base":["Geral"],"booktitle":"Proceedings of the 49th ACM Technical Symposium on Computer Science Education","doi":"10.1145/3159450.3159502","bibtex":"@CONFERENCE{Gligoric2014361,\nauthor={Gligoric, M. and Negara, S. and Legunsen, O. and Marinov, D.},\ntitle={An empirical evaluation and comparison of manual and automated test selection},\njournal={ASE 2014 - Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering},\nyear={2014},\npages={361-371},\ndoi={10.1145/2642937.2643019},\nnote={cited By 15},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908627870&doi=10.1145%2f2642937.2643019&partnerID=40&md5=95ef2057c7ca840bbdce2c375ad00494},\nabstract={Regression test selection speeds up regression testing by rerunning only the tests that can be affected by the most recent code changes. Much progress has been made on research in automated test selection over the last three decades, but it has not translated into practical tools that are widely adopted. Therefore, developers either re-run all tests after each change or perform manual test selection. Re-running all tests is expensive, while manual test selection is tedious and error-prone. Despite such a big trade-off, no study assessed how developers perform manual test selection and compared it to automated test selection. This paper reports on our study of manual test selection in practice and our comparison of manual and automated test selection. We are the first to conduct a study that (1) analyzes data from manual test selection, collected in real time from 14 developers during a three-month study and (2) compares manual test selection with an automated state-of-the-research test-selection tool for 450 test sessions. Almost all developers in our study performed manual test selection, and they did so in mostly ad-hoc ways. Comparing manual and automated test selection, we found the two approaches to select different tests in each and every one of the 450 test sessions investigated. Manual selection chose more tests than automated selection 73% of the time (potentially wasting time) and chose fewer tests 27% of the time (potentially missing bugs). These results show the need for better automated test-selection techniques that integrate well with developers' programming environments. © 2014 ACM.},\nkeywords={Economic and social effects;  Software engineering;  Software testing, Automated selection;  Automated test;  Empirical evaluations;  Manual tests;  Programming environment;  Regression test selection;  Regression testing;  Test selection, Automation},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n","abstract":"\nAutograding systems are increasingly being deployed to meet the challenge of teaching programming at scale. We propose a methodology for extending autograders to provide meaningful feedback for incorrect programs. Our methodology starts with the instructor identifying the concepts and skills important to each programming assignment, designing the assignment, and designing a comprehensive test suite. Tests are then applied to code submissions to learn classes of common errors and produce classifiers to automatically categorize errors in future submissions. The instructor maps the errors to concepts and skills and writes hints to help students find their misconceptions and mistakes. We have applied the methodology to two assignments from our Introduction to Computer Science course. We used submissions from one semester of the class to build classifiers and write hints for observed common errors. We manually validated the automatic error categorization and potential usefulness of the hints using submissions from a second semester. We found that the hints given for erroneous submissions should be helpful for 96% or more of the cases. Based on these promising results, we have deployed our hints and are currently collecting submissions and feedback from students and instructors."},{"id":"Geral172","name":"Squeezing the Limeade: Policies and Workflows for Scalable Online Degrees","authors":"Joyner, David","year":2018,"base":["Geral"],"booktitle":"Proceedings of the Fifth Annual ACM Conference on Learning at Scale","doi":"10.1145/3231644.3231649","bibtex":"@inproceedings{Haldeman:2018:PMF:3159450.3159502,\n author = {Haldeman, Georgiana and Tjang, Andrew and Babe\\c{s}-Vroman, Monica and Bartos, Stephen and Shah, Jay and Yucht, Danielle and Nguyen, Thu D.},\n title = {Providing Meaningful Feedback for Autograding of Programming Assignments},\n booktitle = {Proceedings of the 49th ACM Technical Symposium on Computer Science Education},\n series = {SIGCSE '18},\n year = {2018},\n isbn = {978-1-4503-5103-4},\n location = {Baltimore, Maryland, USA},\n pages = {278--283},\n numpages = {6},\n url = {http://doi.acm.org/10.1145/3159450.3159502},\n doi = {10.1145/3159450.3159502},\n acmid = {3159502},\n publisher = {ACM},\n address = {New York, NY, USA},\n keywords = {autograding, concepts/skills-based hints, error categorization},\n} \n\n","abstract":"In recent years, non-credit options for learning at scale have\noutpaced for-credit options. To scale for-credit options,\nworkflows and policies must be devised to preserve the\ncharacteristics of accredited higher education—such as the\npresumption of human evaluation and an assertion of\nacademic integrity—despite increased scale. These efforts\nmust follow as well with shifting from offering isolated\ncourses (or informal collections thereof) to offering full\ndegree programs with additional administrative elements.\nWe see this shift as one from Massive Open Online Courses\n(MOOCs) to Large, Internet-Mediated Asynchronous\nDegrees (Limeades). In this work, we perform a qualitative\nresearch study on one such program that has scaled to 6,500\nstudents while retaining full accreditation. We report a\ntypology of policies and workflows employed by the\nindividual classes to deliver this experience."},{"id":"Geral173","name":"Glanceable Code History: Visualizing Student Code for Better Instructor Feedback","authors":"Cassidy, Caitlin and Goldman, Max and Miller, Robert C.","year":2018,"base":["Geral"],"booktitle":"Proceedings of the Fifth Annual ACM Conference on Learning at Scale","doi":"10.1145/3231644.3231680","bibtex":"@inproceedings{Joyner:2018:SLP:3231644.3231649,\n author = {Joyner, David},\n title = {Squeezing the Limeade: Policies and Workflows for Scalable Online Degrees},\n booktitle = {Proceedings of the Fifth Annual ACM Conference on Learning at Scale},\n series = {L","abstract":"Immediate, individualized feedback on their code helps students learning to program. However, even in short, focused exercises in active learning, teachers do not have much time to write feedback. In addition, only looking at a student's final code hides a lot of the students' learning and discovering process. We created a glanceable code history visualization that enables teachers to view a student's entire coding history quickly and efficiently. A preliminary user study shows that this visualization captures previously unseen information that allows teachers to give students better grades and give students longer feedback and better feedback that focuses not just on their final code, but all their code in between."},{"id":"Geral174","name":"Semi-automatic Suggestion Generation for Young Novice Programmers in an Open-ended Context","authors":"Ichinco, Michelle and Kelleher, Caitlin","year":2018,"base":["Geral"],"booktitle":"Proceedings of the 17th ACM Conference on Interaction Design and Children","doi":"10.1145/3202185.3202762","bibtex":"@S '18},\n year = {2018},\n isbn = {978-1-4503-5886-6},\n location = {London, United Kingdom},\n pages = {53:1--53:10},\n articleno = {53},\n numpages = {10},\n url = {http://doi.acm.org/10.1145/3231644.3231649},\n doi = {10.1145/3231644.3231649},\n acmid = {3231649},\n publisher = {ACM},\n address = {New York, NY, USA},\n keywords = {academic integrity, accreditation, online degrees},\n}\n\n","abstract":"Independent novice programmers in open-ended contexts rely on help systems to support their learning. These help systems are often laboriously hand-authored by experts. This paper describes a semi-automatic process for the creation of a suggestion-based help system. We demonstrate and evaluate the potential utility of our approach within a blocks-based programming environment for children. With less human effort per suggestion, our approach generated a set of suggestions comparable to a hand-authored set and a set of original suggestions. We ran a study to explore the number and types of suggestions children received, accessed, and used. In 30 minutes, children on average received 9 suggestions, accessed 2.6 suggestions, and inserted 0.8 new concepts from suggestions."},{"id":"Geral175","name":"Interactive Guidance Techniques for Improving Creative Feedback","authors":"Ngoon, Tricia J. and Fraser, C. Ailie and Weingarten, Ariel S. and Dontcheva, Mira and Klemmer, Scott","year":2018,"base":["Geral"],"booktitle":"Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems","doi":"10.1145/3173574.3173629","bibtex":"@inproceedings{Cassidy:2018:GCH:3231644.3231680,\n author = {Cassidy, Caitlin and Goldman, Max and Miller, Robert C.},\n title = {Glanceable Code History: Visualizing Student Code for Better Instructor Feedback},\n booktitle = {Proceedings of the Fifth Annual ACM Conference on Learning at Scale},\n series = {L","abstract":"Good feedback is critical to creativity and learning, yet rare. Many people do not know how to actually provide effective feedback. There is increasing demand for quality feedback -- and thus feedback givers -- in learning and professional settings. This paper contributes empirical evidence that two interactive techniques -- reusable suggestions and adaptive guidance -- can improve feedback on creative work. We present these techniques embodied in the CritiqueKit system to help reviewers give specific, actionable, and justified feedback. Two real-world deployment studies and two controlled experiments with CritiqueKit found that adaptively-presented suggestions improve the quality of feedback from novice reviewers. Reviewers also reported that suggestions and guidance helped them describe their thoughts and reminded them to provide effective feedback."},{"id":"Geral176","name":"PETALS: Improving Learning of Expert Skill in Humanitarian Demining","authors":"Jayatilaka, Lahiru and Sengeh, David M. and Herrmann, Charles and Bertuccelli, Luca and Antos, Dimitrios and Grosz, Barbara J. and Gajos, Krzysztof Z.","year":2018,"base":["Geral"],"booktitle":"Proceedings of the 1st ACM SIGCAS Conference on Computing and Sustainable Societies","doi":"10.1145/3209811.3209871","bibtex":"@S '18},\n year = {2018},\n isbn = {978-1-4503-5886-6},\n location = {London, United Kingdom},\n pages = {22:1--22:4},\n articleno = {22},\n numpages = {4},\n url = {http://doi.acm.org/10.1145/3231644.3231680},\n doi = {10.1145/3231644.3231680},\n acmid = {3231680},\n publisher = {ACM},\n address = {New York, NY, USA},\n keywords = {computer programming, learning at scale, visualizations},\n} \n\n","abstract":"To become proficient at landmine detection, novice deminers need to master several kinds of skills: the proper physical operation of the metal detector, the interpretation of the metal detector auditory feedback, and the abstract skill of constructing and interpreting mental representations of the \"metallic signatures\" produced by the buried objects. This last skill is particularly useful for safely dealing with mines laid out in cluster configurations, where their metallic signatures overlap and thus a danger exists that a deminer might either miss some of the mines or incorrectly assess their exact positions. However, some novice deminers find it challenging to learn how to properly reason about metallic signatures. We have developed Petals, a system that explicitly visualizes a trainee's metal detector operation history on a training task as well as the edge points of the metallic signatures that the trainee collected. Petals enables instructors to supervise multiple trainees at a time, to assess their performance at a glance, and to provide immediate and specific feedback both on the correctness of their final judgements about the number and positions of landmines, and on the process through which they arrived at their conclusions. The results of our field evaluations at the Humanitarian Demining Training Center showed that both the instructors and the trainees found the system a valuable addition to the training course. The results of a controlled study demonstrated that trainees who had access to Petals during training made significantly fewer errors (6% error rate) on relevant tasks during the final exam (which was conducted without Petals) than trainees who did not have access to Petals during training (those participants had a 21% error rate)."},{"id":"Geral177","name":"Students, Systems, and Interactions: Synthesizing the First Four Years of Learning@Scale and Charting the Future","authors":"Kross, Sean and Guo, Philip J.","year":2018,"base":["Geral"],"booktitle":"Proceedings of the Fifth Annual ACM Conference on Learning at Scale","doi":"10.1145/3231644.3231662","bibtex":"@inproceedings{Ichinco:2018:SSG:3202185.3202762,\n author = {Ichinco, Michelle and Kelleher, Caitlin},\n title = {Semi-automatic Suggestion Generation for Young Novice Programmers in an Open-ended Context},\n booktitle = {Proceedings of the 17th ACM Conference on Interaction Design and Children},\n series = {IDC '18},\n year = {2018},\n isbn = {978-1-4503-5152-2},\n location = {Trondheim, Norway},\n pages = {405--412},\n numpages = {8},\n url = {http://doi.acm.org/10.1145/3202185.3202762},\n doi = {10.1145/3202185.3202762},\n acmid = {3202762},\n publisher = {ACM},\n address = {New York, NY, USA},\n keywords = {code examples, novice programming, recommender systems},\n}\n\n","abstract":"We survey all four years of papers published so far at the Learning at Scale conference in order to reflect on the major research areas that have been investigated and to chart possible directions for future study. We classified all 69 full papers so far into three categories: Systems for Learning at Scale, Interactions with Sociotechnical Systems, and Understanding Online Students. Systems papers presented technologies that varied by how much they amplify human effort (e.g., one-to-one, one-to-many, many-to-many). Interaction papers studied both individual and group interactions with learning technologies. Finally, student-centric study papers focused on modeling knowledge and on promoting global access and equity. We conclude by charting future research directions related to topics such as going beyond the MOOC hype cycle, axes of scale for systems, more immersive course experiences, learning on mobile devices, diversity in student personas, students as co-creators, and fostering better social connections amongst students."},{"id":"Geral178","name":"Phrase-Based Statistical Translation of Programming Languages","authors":"Karaivanov, Svetoslav and Raychev, Veselin and Vechev, Martin","year":2014,"base":["Geral"],"booktitle":"Proceedings of the 2014 ACM International Symposium on New Ideas, New Paradigms, and Reflections on Programming \\& Software","doi":"10.1145/2661136.2661148","bibtex":"@inproceedings{Ngoon:2018:IGT:3173574.3173629,\n author = {Ngoon, Tricia J. and Fraser, C. Ailie and Weingarten, Ariel S. and Dontcheva, Mira and Klemmer, Scott},\n title = {Interactive Guidance Techniques for Improving Creative Feedback},\n booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},\n series = {CHI '18},\n year = {2018},\n isbn = {978-1-4503-5620-6},\n location = {Montreal QC, Canada},\n pages = {55:1--55:11},\n articleno = {55},\n numpages = {11},\n url = {http://doi.acm.org/10.1145/3173574.3173629},\n doi = {10.1145/3173574.3173629},\n acmid = {3173629},\n publisher = {ACM},\n address = {New York, NY, USA},\n keywords = {creativity, critique, educational technology, feedback},\n}\n\n","abstract":"Phrase-based statistical machine translation approaches have been highly successful in translating between natural languages and are heavily used by commercial systems (e.g. Google Translate).\n\nThe main objective of this work is to investigate the applicability of these approaches for translating between programming languages. Towards that, we investigated several variants of the phrase-based translation approach: i) a direct application of the approach to programming languages, ii) a novel modification of the approach to incorporate the grammatical structure of the target programming language (so to avoid generating target programs which do not parse), and iii) a combination of ii) with custom rules added to improve the quality of the translation.\n\nTo experiment with the above systems, we investigated machine translation from C# to Java. For the training, which takes about 60 hours, we used a parallel corpus of 20,499 C#-to-Java method translations. We then evaluated each of the three systems above by translating 1,000 C# methods. Our experimental results indicate that with the most advanced system, about 60% of the translated methods compile (the top ranked) and out of a random sample of 50 correctly compiled methods, 68% (34 methods) were semantically equivalent to the reference solution."},{"id":"Geral179","name":"Transforming Spreadsheet Data Types Using Examples","authors":"Singh, Rishabh and Gulwani, Sumit","year":2016,"base":["Geral"],"doi":"10.1145/2914770.2837668","bibtex":"@inproceedings{Jayatilaka:2018:PIL:3209811.3209871,\n author = {Jayatilaka, Lahiru and Sengeh, David M. and Herrmann, Charles and Bertuccelli, Luca and Antos, Dimitrios and Grosz, Barbara J. and Gajos, Krzysztof Z.},\n title = {PETALS: Improving Learning of Expert Skill in Humanitarian Demining},\n booktitle = {Proceedings of the 1st ACM SIGCAS Conference on Computing and Sustainable Societies},\n series = {COMPASS '18},\n year = {2018},\n isbn = {978-1-4503-5816-3},\n location = {Menlo Park and San Jose, CA, USA},\n pages = {33:1--33:11},\n articleno = {33},\n numpages = {11},\n url = {http://doi.acm.org/10.1145/3209811.3209871},\n doi = {10.1145/3209811.3209871},\n acmid = {3209871},\n publisher = {ACM},\n address = {New York, NY, USA},\n keywords = {demining, explosives detection, training systems},\n} \n\n","abstract":"Cleaning spreadsheet data types is a common problem faced by millions of spreadsheet users. Data types such as date, time, name, and units are ubiquitous in spreadsheets, and cleaning transformations on these data types involve parsing and pretty printing their string representations. This presents many challenges to users because cleaning such data requires some background knowledge about the data itself and moreover this data is typically non-uniform, unstructured, and ambiguous. Spreadsheet systems and Programming Languages provide some UI-based and programmatic solutions for this problem but they are either insufficient for the user's needs or are beyond their expertise. In this paper, we present a programming by example methodology of cleaning data types that learns the desired transformation from a few input-output examples. We propose a domain specific language with probabilistic semantics that is parameterized with declarative data type definitions. The probabilistic semantics is based on three key aspects: (i) approximate predicate matching, (ii) joint learning of data type interpretation, and (iii) weighted branches. This probabilistic semantics enables the language to handle non-uniform, unstructured, and ambiguous data. We then present a synthesis algorithm that learns the desired program in this language from a set of input-output examples. We have implemented our algorithm as an Excel add-in and present its successful evaluation on 55 benchmark problems obtained from online help forums and Excel product team."},{"id":"Geral180","name":"Deuce: A Lightweight User Interface for Structured Editing","authors":"Hempel, Brian and Lubin, Justin and Lu, Grace and Chugh, Ravi","year":2018,"base":["Geral"],"booktitle":"Proceedings of the 40th International Conference on Software Engineering","doi":"10.1145/3180155.3180165","bibtex":"@inproceedings{Kross:2018:SSI:3231644.3231662,\n author = {Kross, Sean and Guo, Philip J.},\n title = {Students, Systems, and Interactions: Synthesizing the First Four Years of Learning","abstract":"We present a structure-aware code editor, called Deuce, that is equipped with direct manipulation capabilities for invoking automated program transformations. Compared to traditional refactoring environments, DEUCE employs a direct manipulation interface that is tightly integrated within a text-based editing workflow. In particular, Deuce draws (i) clickable widgets atop the source code that allow the user to structurally select the unstructured text for subexpressions and other relevant features, and (ii) a lightweight, interactive menu of potential transformations based on the current selections. We implement and evaluate our design with mostly standard transformations in the context of a small functional programming language. A controlled user study with 21 participants demonstrates that structural selection is preferred to a more traditional text-selection interface and may be faster overall once users gain experience with the tool. These results accord with Deuce's aim to provide human-friendly structural interactions on top of familiar text-based editing."},{"id":"Geral181","name":"FIDEX: Filtering Spreadsheet Data Using Examples","authors":"Wang, Xinyu and Gulwani, Sumit and Singh, Rishabh","year":2016,"base":["Geral"],"booktitle":"Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications","doi":"10.1145/2983990.2984030","bibtex":"@Scale and Charting the Future},\n booktitle = {Proceedings of the Fifth Annual ACM Conference on Learning at Scale},\n series = {L","abstract":"Data filtering in spreadsheets is a common problem faced by millions of end-users. The task of data filtering requires a computational model that can separate intended positive and negative string instances. We present a system, FIDEX, that can efficiently learn desired data filtering expressions from a small set of positive and negative string examples.\n\nThere are two key ideas of our approach. First, we design an expressive DSL to represent disjunctive filter expressions needed for several real-world data filtering tasks. Second, we develop an efficient synthesis algorithm for incrementally learning consistent filter expressions in the DSL from very few positive and negative examples. A DAG-based data structure is used to succinctly represent a large number of filter expressions, and two corresponding operators are defined for algorithmically handling positive and negative examples, namely, the intersection and subtraction operators. FIDEX is able to learn data filters for 452 out of 460 real-world data filtering tasks in real time (0.22s), using only 2.2 positive string instances and 2.7 negative string instances on average."},{"id":"Geral182","name":"Foofah: Transforming Data By Example","authors":"Jin, Zhongjun and Anderson, Michael R. and Cafarella, Michael and Jagadish, H. V.","year":2017,"base":["Geral"],"booktitle":"Proceedings of the 2017 ACM International Conference on Management of Data","doi":"10.1145/3035918.3064034","bibtex":"@S '18},\n year = {2018},\n isbn = {978-1-4503-5886-6},\n location = {London, United Kingdom},\n pages = {2:1--2:10},\n articleno = {2},\n numpages = {10},\n url = {http://doi.acm.org/10.1145/3231644.3231662},\n doi = {10.1145/3231644.3231662},\n acmid = {3231662},\n publisher = {ACM},\n address = {New York, NY, USA},\n keywords = {context/synthesis paper, meta-analysis, survey paper},\n} \n\n","abstract":"Data transformation is a critical first step in modern data analysis: before any analysis can be done, data from a variety of sources must be wrangled into a uniform format that is amenable to the intended analysis and analytical software package. This data transformation task is tedious, time-consuming, and often requires programming skills beyond the expertise of data analysts. In this paper, we develop a technique to synthesize data transformation programs by example, reducing this burden by allowing the analyst to describe the transformation with a small input-output example pair, without being concerned with the transformation steps required to get there. We implemented our technique in a system, FOOFAH, that efficiently searches the space of possible data transformation operations to generate a program that will perform the desired transformation. We experimentally show that data transformation programs can be created quickly with FOOFAH for a wide variety of cases, with 60% less user effort than the well-known WRANGLER system."},{"id":"Geral183","name":"Constraint-Based Refactoring","authors":"Steimann, Friedrich","year":2018,"base":["Geral"],"doi":"10.1145/3156016","bibtex":"@inproceedings{Karaivanov:2014:PST:2661136.2661148,\n author = {Karaivanov, Svetoslav and Raychev, Veselin and Vechev, Martin},\n title = {Phrase-Based Statistical Translation of Programming Languages},\n booktitle = {Proceedings of the 2014 ACM International Symposium on New Ideas, New Paradigms, and Reflections on Programming \\& Software},\n series = {Onward! 2014},\n year = {2014},\n isbn = {978-1-4503-3210-1},\n location = {Portland, Oregon, USA},\n pages = {173--184},\n numpages = {12},\n url = {http://doi.acm.org/10.1145/2661136.2661148},\n doi = {10.1145/2661136.2661148},\n acmid = {2661148},\n publisher = {ACM},\n address = {New York, NY, USA},\n keywords = {programming language translation, statistical machine translation},\n}\n\n"},{"id":"Geral184","name":"Automated Migration of Build Scripts Using Dynamic Analysis and Search-based Refactoring","authors":"Gligoric, Milos and Schulte, Wolfram and Prasad, Chandra and van Velzen, Danny and Narasamdya, Iman and Livshits, Benjamin","year":2014,"base":["Geral"],"doi":"10.1145/2714064.2660239","bibtex":"@article{Singh:2016:TSD:2914770.2837668,\n author = {Singh, Rishabh and Gulwani, Sumit},\n title = {Transforming Spreadsheet Data Types Using Examples},\n journal = {SIGPLAN Not.},\n issue_date = {January 2016},\n volume = {51},\n number = {1},\n month = jan,\n year = {2016},\n issn = {0362-1340},\n pages = {343--356},\n numpages = {14},\n url = {http://doi.acm.org/10.1145/2914770.2837668},\n doi = {10.1145/2914770.2837668},\n acmid = {2837668},\n publisher = {ACM},\n address = {New York, NY, USA},\n keywords = {Noisy Examples, Probabilistic Synthesis, Program Synthesis, Programming By Examples, Spreadsheet Programming},\n} \n\n","abstract":"The efficiency of a build system is an important factor for developer productivity. As a result, developer teams have been increasingly adopting new build systems that allow higher build parallelization. However, migrating the existing legacy build scripts to new build systems is a tedious and error-prone process. Unfortunately, there is insufficient support for automated migration of build scripts, making the migration more problematic.\n\nWe propose the first dynamic approach for automated migration of build scripts to new build systems. Our approach works in two phases. First, from a set of execution traces, we synthesize build scripts that accurately capture the intent of the original build. The synthesized build scripts are typically long and hard to maintain. Second, we apply refactorings that raise the abstraction level of the synthesized scripts (e.g., introduce functions for similar fragments). As different refactoring sequences may lead to different build scripts, we use a search-based approach that explores various sequences to identify the best (e.g., shortest) build script. We optimize search-based refactoring with partial-order reduction to faster explore refactoring sequences. We implemented the proposed two phase migration approach in a tool called METAMORPHOSIS that has been recently used at Microsoft."},{"id":"Geral185","name":"Assessing the Refactoring of Brain Methods","authors":"Vidal, Santiago and berra, I\\~{n}aki and Zulliani, Santiago and Marcos, Claudia and Pace, J. Andr{\\'e}s D\\'{\\i}az","year":2018,"base":["Geral"],"doi":"10.1145/3191314","bibtex":"@inproceedings{Hempel:2018:DEL:3180155.3180165,\n author = {Hempel, Brian and Lubin, Justin and Lu, Grace and Chugh, Ravi},\n title = {Deuce: A Lightweight User Interface for Structured Editing},\n booktitle = {Proceedings of the 40th International Conference on Software Engineering},\n series = {ICSE '18},\n year = {2018},\n isbn = {978-1-4503-5638-1},\n location = {Gothenburg, Sweden},\n pages = {654--664},\n numpages = {11},\n url = {http://doi.acm.org/10.1145/3180155.3180165},\n doi = {10.1145/3180155.3180165},\n acmid = {3180165},\n publisher = {ACM},\n address = {New York, NY, USA},\n keywords = {direct manipulation, refactoring, structured editing},\n} \n\n","abstract":"Tough time-to-market constraints and unanticipated integration or evolution issues lead to design tradeoffs that usually cause flaws in the structure of a software system. Thus, maintenance costs grow significantly. The impact of these design decisions, which provide short-term benefits at the expense of the system’s design integrity, is usually referred to as technical debt. In this paper, I propose a novel framework for assessing technical debt using a technique for detecting design flaws, i.e., specific violations of well-established design principles and rules. To make the framework comprehensive and balanced, it is built on top of a set of metrics-based detection rules for well-known design flaws that cover all of the major aspects of design such as coupling, complexity, and encapsulation. I demonstrate the effectiveness of the framework by assessing the evolution of technical debt symptoms over a total of 63 releases of two popular Eclipse® projects. The case study shows how the framework can detect debt symptoms and past refactoring actions. The experiment also reveals that in the absence of such a framework, restructuring actions are not always coherent and systematic, not even when performed by very experienced developers."},{"id":"Geral186","name":"Fully Automated HTML and Javascript Rewriting for Constructing a Self-Healing Web Proxy","authors":"Durieux, T. and Hamadi, Y. and Monperrus, M.","year":2018,"base":["Geral"],"abstract":"Over the last few years, the complexity of web applications has increased to provide more dynamic web applications to users. The drawback of this complexity is the growing number of errors in the front-end applications. In this paper, we present BikiniProxy, a novel technique to provide self-healing for the web. BikiniProxy is designed as an HTTP proxy that uses five self-healing strategies to rewrite the buggy HTML and Javascript code. We evaluate BikiniProxy with a new benchmark of 555 reproducible Javascript errors, DeadClick. We create DeadClick by randomly crawling the Internet and collect all web pages that contain Javascript errors. Then, we observe how BikiniProxy heals those errors by collecting and comparing the traces of the original and healed pages. To sum up, BikiniProxy is a novel fully-automated self-healing approach that is specific to the web, evaluated on 555 real Javascript errors, and based on original self-healing rewriting strategies for HTML and Javascript. © 2018 IEEE.","doi":"10.1109/ISSRE.2018.00012","bibtex":"@inproceedings{Wang:2016:FFS:2983990.2984030,\n author = {Wang, Xinyu and Gulwani, Sumit and Singh, Rishabh},\n title = {FIDEX: Filtering Spreadsheet Data Using Examples},\n booktitle = {Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications},\n series = {OOPSLA 2016},\n year = {2016},\n isbn = {978-1-4503-4444-9},\n location = {Amsterdam, Netherlands},\n pages = {195--213},\n numpages = {19},\n url = {http://doi.acm.org/10.1145/2983990.2984030},\n doi = {10.1145/2983990.2984030},\n acmid = {2984030},\n publisher = {ACM},\n address = {New York, NY, USA},\n keywords = {Data Filtering, Program Synthesis, Programming By Examples, Regular Expressions},\n}\n\n"},{"id":"Geral187","name":"Survey on Intelligent Search and Construction Methods of Program [智能化的程序搜索与构造方法综述]","authors":"Liu, B.-B. and Dong, W. and Wang, J.","year":2018,"base":["Geral"],"abstract":"The rapid development of Internet, machine learning and artificial intelligence, as well as the appearance of a large number of open-source software and communities, has brought new opportunities and challenges to the development of software engineering. There are billions of lines of code on the Internet. These codes, especially those of high quality and widely used contains all kinds of knowledge, which has led to the new idea of intelligent software development. It tries to make full use of code resources, knowledge and collective intelligence on the Internet to effectively improve the efficiency and quality of software development. The key technology is program search and construction, providing great theoretical and practical value. At present, the research work of these areas mainly focuses on code search, program synthesis, code recommendation and completion, defect detection, code style improvement, and automatic program repair. This paper surveys the current main research work from the above aspects, sorts out the specific theoretical and technical approaches in detail and summarizes the challenges in the current research process. Several directions of research in the future are also proposed. © Copyright 2018, Institute of Software, the Chinese Academy of Sciences. All rights reserved.","doi":"10.13328/j.cnki.jos.005529","bibtex":"@inproceedings{Jin:2017:FTD:3035918.3064034,\n author = {Jin, Zhongjun and Anderson, Michael R. and Cafarella, Michael and Jagadish, H. V.},\n title = {Foofah: Transforming Data By Example},\n booktitle = {Proceedings of the 2017 ACM International Conference on Management of Data},\n series = {SIGMOD '17},\n year = {2017},\n isbn = {978-1-4503-4197-4},\n location = {Chicago, Illinois, USA},\n pages = {683--698},\n numpages = {16},\n url = {http://doi.acm.org/10.1145/3035918.3064034},\n doi = {10.1145/3035918.3064034},\n acmid = {3064034},\n publisher = {ACM},\n address = {New York, NY, USA},\n keywords = {a* algorithm, data transformation, heuristic, program synthesis, programming by example},\n} \n\n"},{"id":"Geral188","name":"Mutode: Generic JavaScript and node.js mutation testing tool","authors":"Rodríguez-Baquero, D. and Linares-Vásquez, M.","year":2018,"base":["Geral"],"abstract":"Mutation testing is a technique in which faults (mutants) are injected into a program or application to assess its test suite effectiveness. It works by inserting mutants and running the application's test suite to identify if the mutants are detected (killed) or not (survived) by the tests. Although computationally expensive, it has proven to be an effective method to assess application test suites. Several mutation testing frameworks and tools have been built for the various programing languages, however, very few tools have been built for the JavaScript language, more specifically, there is a lack of mutation testing tools for the Node.js runtime and npm based applications. The npm Registry is a public collection of modules of open-source code for Node.js, front-end web applications, mobile applications, robots, routers, and countless other needs of the JavaScript community. The over 700,000 packages hosted in npm are downloaded more than 5 billion times per week. More and more software is published in npm every day, representing a huge opportunity to share code and solutions, but also to share bugs and faulty software. In this paper, we briefly describe prior work for mutation operators in JavaScript and Node.js, and propose Mutode, an open source tool which leverages the npm package ecosystem to perform mutation testing for JavaScript and Node.js applications. We empirically evaluated Mutode effectiveness by running it on 12 of the top 20 npm modules that have automated test suites. © 2018 Association for Computing Machinery.","doi":"10.1145/3213846.3229504","bibtex":"@article{Steimann:2018:CR:3173093.3156016,\n author = {Steimann, Friedrich},\n title = {Constraint-Based Refactoring},\n journal = {ACM Trans. Program. Lang. Syst.},\n issue_date = {January 2018},\n volume = {40},\n number = {1},\n month = jan,\n year = {2018},\n issn = {0164-0925},\n pages = {2:1--2:40},\n articleno = {2},\n numpages = {40},\n url = {http://doi.acm.org/10.1145/3156016},\n doi = {10.1145/3156016},\n acmid = {3156016},\n publisher = {ACM},\n address = {New York, NY, USA},\n keywords = {Refactoring, constraint-based repair},\n} \n\n"},{"id":"Geral189","name":"ConflictJS: Finding and understanding conflicts between JavaScript libraries","authors":"Patra, J. and Dixit, P.N. and Pradel, M.","year":2018,"base":["Geral"],"abstract":"It is a common practice for client-side web applications to build on various third-party JavaScript libraries. Due to the lack of namespaces in JavaScript, these libraries all share the same global namespace. As a result, one library may inadvertently modify or even delete the APIs of another library, causing unexpected behavior of library clients. Given the quickly increasing number of libraries, manually keeping track of such conflicts is practically impossible both for library developers and users. This paper presents ConflictJS, an automated and scalable approach to analyze libraries for conflicts. The key idea is to tackle the huge search space of possible conflicts in two phases. At first, a dynamic analysis of individual libraries identifies pairs of potentially conflicting libraries. Then, targeted test synthesis validates potential conflicts by creating a client application that suffers from a conflict. The overall approach is free of false positives, in the sense that it reports a problem only when such a client exists. We use ConflictJS to analyze and study conflicts among 951 real-world libraries. The results show that one out of four libraries is potentially conflicting and that 166 libraries are involved in at least one certain conflict. The detected conflicts cause crashes and other kinds of unexpected behavior. Our work helps library developers to prevent conflicts, library users to avoid combining conflicting libraries, and provides evidence that designing a language without explicit namespaces has undesirable effects. © 2018 ACM.","doi":"10.1145/3180155.3180184","bibtex":"@article{Gligoric:2014:AMB:2714064.2660239,\n author = {Gligoric, Milos and Schulte, Wolfram and Prasad, Chandra and van Velzen, Danny and Narasamdya, Iman and Livshits, Benjamin},\n title = {Automated Migration of Build Scripts Using Dynamic Analysis and Search-based Refactoring},\n journal = {SIGPLAN Not.},\n issue_date = {October 2014},\n volume = {49},\n number = {10},\n month = oct,\n year = {2014},\n issn = {0362-1340},\n pages = {599--616},\n numpages = {18},\n url = {http://doi.acm.org/10.1145/2714064.2660239},\n doi = {10.1145/2714064.2660239},\n acmid = {2660239},\n publisher = {ACM},\n address = {New York, NY, USA},\n keywords = {build system, migration, search-based refactoring},\n}\n\n"},{"id":"Geral190","name":"Automated refactoring of client-side JavaScript code to ES6 modules","authors":"Paltoglou, A. and Zafeiris, V.E. and Giakoumakis, E.A. and Diamantidis, N.A.","year":2018,"base":["Geral"],"abstract":"JavaScript (JS) is a dynamic, weakly-typed and object-based programming language that expanded its reach, in recent years, from the desktop web browser to a wide range of runtime platforms in embedded, mobile and server hosts. Moreover, the scope of functionality implemented in JS scaled from DOM manipulation in dynamic HTML pages to full-scale applications for various domains, stressing the need for code reusability and maintainability. Towards this direction, the ECMAScript 6 (ES6) revision of the language standardized the syntax for class and module definitions, streamlining the encapsulation of data and functionality at various levels of granularity. This work focuses on refactoring client-side web applications for the elimination of code smells, relevant to global variables and functions that are declared in JS files linked to a web page. These declarations 'pollute' the global namespace at runtime and often lead to name conflicts with undesired effects. We propose a method for the encapsulation of global declarations through automated refactoring to ES6 modules. Our approach transforms each linked JS script of a web application to an ES6 module with appropriate import and export declarations that are inferred through static analysis. A prototype implementation of the proposed method, based on WALA libraries, has been evaluated on a set of open source projects. The evaluation results support the applicability and runtime efficiency of the proposed method. © 2018 IEEE.","doi":"10.1109/SANER.2018.8330227","bibtex":"@article{Vidal:2018:ARB:3208361.3191314,\n author = {Vidal, Santiago and berra, I\\~{n}aki and Zulliani, Santiago and Marcos, Claudia and Pace, J. Andr{\\'e}s D\\'{\\i}az},\n title = {Assessing the Refactoring of Brain Methods},\n journal = {ACM Trans. Softw. Eng. Methodol.},\n issue_date = {June 2018},\n volume = {27},\n number = {1},\n month = apr,\n year = {2018},\n issn = {1049-331X},\n pages = {2:1--2:43},\n articleno = {2},\n numpages = {43},\n url = {http://doi.acm.org/10.1145/3191314},\n doi = {10.1145/3191314},\n acmid = {3191314},\n publisher = {ACM},\n address = {New York, NY, USA},\n keywords = {Code smells, brain method, long method, refactoring},\n} \n\n"},{"id":"Geral191","name":"Refactoring asynchrony in JavaScript","authors":"Gallaba, K. and Hanam, Q. and Mesbah, A. and Beschastnikh, I.","year":2017,"base":["Geral"],"abstract":"JavaScript is a widely used programming language that makes extensive use of asynchronous computation, particularly in the form of asynchronous callbacks. These callbacks are used to handle tasks, from GUI events to network messages, in a non-blocking fashion. Asynchronous callbacks present developers with two challenges. First, JavaScript's try/catch error-handling mechanism is not sufficient for proper error handling in asynchronous contexts. In response, the JavaScript community has come to rely on the error-first protocol, an informal programming idiom that is not enforced or checked by the runtime. Second, JavaScript callbacks are frequently nested, making them difficult to handle (also known as callback hell). Fortunately, a recent language extension called promises provides an alternative to asynchronous callbacks. The adoption of promises, however, has been slow as refactoring existing code to use promises is a complex task. We present a set of program analysis techniques to detect instances of asynchronous callbacks and to refactor such callbacks, including callbacks with the error-first protocol, into promises. We implement our techniques in a tool called PROMISESLAND. We perform a manual analysis of four JavaScript applications to evaluate the tool's precision and recall, which are, on average, 100% and 83%, respectively. We evaluate PROMISESLAND on 21 large JavaScript applications, and find that PROMISESLAND (1) correctly refactors callbacks to promises, (2) outperforms a recent related refactoring technique, and (3) runs in under three seconds on all of our evaluation targets. © 2017 IEEE.","doi":"10.1109/ICSME.2017.83","bibtex":"@CONFERENCE{Durieux20181,\nauthor={Durieux, T. and Hamadi, Y. and Monperrus, M.},\ntitle={Fully Automated HTML and Javascript Rewriting for Constructing a Self-Healing Web Proxy},\njournal={Proceedings - International Symposium on Software Reliability Engineering, ISSRE},\nyear={2018},\nvolume={2018-October},\npages={1-12},\ndoi={10.1109/ISSRE.2018.00012},\nart_number={8539064},\nnote={cited By 0},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059636843&doi=10.1109%2fISSRE.2018.00012&partnerID=40&md5=6d04568c2396ac72131373f2d84b5e61},\naffiliation={Inria and University of Lille, Lille, France; Ecole Polytechnique, Paris, France; KTH Royal Institute of Technology, Stockholm, Sweden},\nabstract={Over the last few years, the complexity of web applications has increased to provide more dynamic web applications to users. The drawback of this complexity is the growing number of errors in the front-end applications. In this paper, we present BikiniProxy, a novel technique to provide self-healing for the web. BikiniProxy is designed as an HTTP proxy that uses five self-healing strategies to rewrite the buggy HTML and Javascript code. We evaluate BikiniProxy with a new benchmark of 555 reproducible Javascript errors, DeadClick. We create DeadClick by randomly crawling the Internet and collect all web pages that contain Javascript errors. Then, we observe how BikiniProxy heals those errors by collecting and comparing the traces of the original and healed pages. To sum up, BikiniProxy is a novel fully-automated self-healing approach that is specific to the web, evaluated on 555 real Javascript errors, and based on original self-healing rewriting strategies for HTML and Javascript. © 2018 IEEE.},\nauthor_keywords={Failure oblivious computing;  Javascript;  Repair proxy;  Self healing},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral192","name":"Why do developers use trivial packages? An empirical case study on npm","authors":"Abdalkareem, R. and Nourry, O. and Wehaibi, S. and Mujahid, S. and Shihab, E.","year":2017,"base":["Geral"],"abstract":"Code reuse is traditionally seen as good practice. Recent trends have pushed the concept of code reuse to an extreme, by using packages that implement simple and trivial tasks, which we call trivial packages'. A recent incident where a trivial package led to the breakdown of some of the most popular web applications such as Facebook and Netflix made it imperative to question the growing use of trivial packages. Therefore, in this paper, we mine more than 230, 000 npm packages and 38, 000 JavaScript applications in order to study the prevalence of trivial packages. We found that trivial packages are common and are increasing in popularity, making up 16.8% of the studied npm packages. We performed a survey with 88 Node.js developers who use trivial packages to understand the reasons and drawbacks of their use. Our survey revealed that trivial packages are used because they are perceived to be well implemented and tested pieces of code. However, developers are concerned about maintaining and the risks of breakages due to the extra dependencies trivial packages introduce. To objectively verify the survey results, we empirically validate the most cited reason and drawback and find that, contrary to developers' beliefs, only 45.2% of trivial packages even have tests. However, trivial packages appear to be deployment tested' and to have similar test, usage and community interest as non-trivial packages. On the other hand, we found that 11.5% of the studied trivial packages have more than 20 dependencies. Hence, developers should be careful about which trivial packages they decide to use. © 2017 Association for Computing Machinery.","doi":"10.1145/3106237.3106267","bibtex":"@ARTICLE{Liu20182180,\nauthor={Liu, B.-B. and Dong, W. and Wang, J.},\ntitle={Survey on Intelligent Search and Construction Methods of Program [智能化的程序搜索与构造方法综述]},\njournal={Ruan Jian Xue Bao/Journal of Software},\nyear={2018},\nvolume={29},\nnumber={8},\npages={2180-2197},\ndoi={10.13328/j.cnki.jos.005529},\nnote={cited By 0},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055533864&doi=10.13328%2fj.cnki.jos.005529&partnerID=40&md5=9bb00a7f37ea2f4cbdd83e7b75d24367},\naffiliation={College of Computer, National University of Defense Technology, Changsha, 410073, China},\nabstract={The rapid development of Internet, machine learning and artificial intelligence, as well as the appearance of a large number of open-source software and communities, has brought new opportunities and challenges to the development of software engineering. There are billions of lines of code on the Internet. These codes, especially those of high quality and widely used contains all kinds of knowledge, which has led to the new idea of intelligent software development. It tries to make full use of code resources, knowledge and collective intelligence on the Internet to effectively improve the efficiency and quality of software development. The key technology is program search and construction, providing great theoretical and practical value. At present, the research work of these areas mainly focuses on code search, program synthesis, code recommendation and completion, defect detection, code style improvement, and automatic program repair. This paper surveys the current main research work from the above aspects, sorts out the specific theoretical and technical approaches in detail and summarizes the challenges in the current research process. Several directions of research in the future are also proposed. © Copyright 2018, Institute of Software, the Chinese Academy of Sciences. All rights reserved.},\nauthor_keywords={Collective intelligence;  Intelligent software development;  Machine learning;  Program construction;  Program search},\ndocument_type={Review},\nsource={Scopus},\n}\n\n"},{"id":"Geral193","name":"To Type or Not to Type: Quantifying Detectable Bugs in JavaScript","authors":"Gao, Z. and Bird, C. and Barr, E.T.","year":2017,"base":["Geral"],"abstract":"JavaScript is growing explosively and is now used in large mature projects even outside the web domain. JavaScript is also a dynamically typed language for which static type systems, notably Facebook's Flow and Microsoft's TypeScript, have been written. What benefits do these static type systems provide? Leveraging JavaScript project histories, we select a fixed bug and check out the code just prior to the fix. We manually add type annotations to the buggy code and test whether Flow and TypeScript report an error on the buggy code, thereby possibly prompting a developer to fix the bug before its public release. We then report the proportion of bugs on which these type systems reported an error. Evaluating static type systems against public bugs, which have survived testing and review, is conservative: it understates their effectiveness at detecting bugs during private development, not to mention their other benefits such as facilitating code search/completion and serving as documentation. Despite this uneven playing field, our central finding is that both static type systems find an important percentage of public bugs: both Flow 0.30 and TypeScript 2.0 successfully detect 15%!. © 2017 IEEE.","doi":"10.1109/ICSE.2017.75","bibtex":"@CONFERENCE{Rodríguez-Baquero2018372,\nauthor={Rodríguez-Baquero, D. and Linares-Vásquez, M.},\ntitle={Mutode: Generic JavaScript and node.js mutation testing tool},\njournal={ISSTA 2018 - Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis},\nyear={2018},\npages={372-375},\ndoi={10.1145/3213846.3229504},\nnote={cited By 0},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051542060&doi=10.1145%2f3213846.3229504&partnerID=40&md5=5dd1d9ff26b71cff00afe02abc382604},\naffiliation={Universidad de Los Andes, Bogotá, Colombia},\nabstract={Mutation testing is a technique in which faults (mutants) are injected into a program or application to assess its test suite effectiveness. It works by inserting mutants and running the application's test suite to identify if the mutants are detected (killed) or not (survived) by the tests. Although computationally expensive, it has proven to be an effective method to assess application test suites. Several mutation testing frameworks and tools have been built for the various programing languages, however, very few tools have been built for the JavaScript language, more specifically, there is a lack of mutation testing tools for the Node.js runtime and npm based applications. The npm Registry is a public collection of modules of open-source code for Node.js, front-end web applications, mobile applications, robots, routers, and countless other needs of the JavaScript community. The over 700,000 packages hosted in npm are downloaded more than 5 billion times per week. More and more software is published in npm every day, representing a huge opportunity to share code and solutions, but also to share bugs and faulty software. In this paper, we briefly describe prior work for mutation operators in JavaScript and Node.js, and propose Mutode, an open source tool which leverages the npm package ecosystem to perform mutation testing for JavaScript and Node.js applications. We empirically evaluated Mutode effectiveness by running it on 12 of the top 20 npm modules that have automated test suites. © 2018 Association for Computing Machinery.},\nauthor_keywords={JavaScript;  Mutation testing;  Node.js;  Operators},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"},{"id":"Geral194","name":"A Study of Causes and Consequences of Client-Side JavaScript Bugs","authors":"Ocariza, F.S., Jr. and Bajaj, K. and Pattabiraman, K. and Mesbah, A.","year":2017,"base":["Geral"],"abstract":"Client-side JavaScript is widely used in web applications to improve user-interactivity and minimize client-server communications. Unfortunately, JavaScript is known to be error-prone. While prior studies have demonstrated the prevalence of JavaScript faults, no attempts have been made to determine their causes and consequences. The goal of our study is to understand the root causes and impact of JavaScript faults and how the results can impact JavaScript programmers, testers and tool developers. We perform an empirical study of 502 bug reports from 19 bug repositories. The bug reports are thoroughly examined to classify and extract information about each bug' cause (the error) and consequence (the failure and impact). Our results show that the majority (68 percent) of JavaScript faults are DOM-related, meaning they are caused by faulty interactions of the JavaScript code with the Document Object Model (DOM). Further, 80 percent of the highest impact JavaScript faults are DOM-related. Finally, most JavaScript faults originate from programmer mistakes committed in the JavaScript code itself, as opposed to other web application components. These results indicate that JavaScript programmers and testers need tools that can help them reason about the DOM. Additionally, developers can use the error patterns we found to design more powerful static analysis tools for JavaScript. © 2016 IEEE.","doi":"10.1109/TSE.2016.2586066","bibtex":"@CONFERENCE{Patra2018741,\nauthor={Patra, J. and Dixit, P.N. and Pradel, M.},\ntitle={ConflictJS: Finding and understanding conflicts between JavaScript libraries},\njournal={Proceedings - International Conference on Software Engineering},\nyear={2018},\npages={741-751},\ndoi={10.1145/3180155.3180184},\nnote={cited By 1},\nurl={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049388864&doi=10.1145%2f3180155.3180184&partnerID=40&md5=526a1366a1fcb43a7e1a425e6faf4916},\naffiliation={TU Darmstadt, Germany},\nabstract={It is a common practice for client-side web applications to build on various third-party JavaScript libraries. Due to the lack of namespaces in JavaScript, these libraries all share the same global namespace. As a result, one library may inadvertently modify or even delete the APIs of another library, causing unexpected behavior of library clients. Given the quickly increasing number of libraries, manually keeping track of such conflicts is practically impossible both for library developers and users. This paper presents ConflictJS, an automated and scalable approach to analyze libraries for conflicts. The key idea is to tackle the huge search space of possible conflicts in two phases. At first, a dynamic analysis of individual libraries identifies pairs of potentially conflicting libraries. Then, targeted test synthesis validates potential conflicts by creating a client application that suffers from a conflict. The overall approach is free of false positives, in the sense that it reports a problem only when such a client exists. We use ConflictJS to analyze and study conflicts among 951 real-world libraries. The results show that one out of four libraries is potentially conflicting and that 166 libraries are involved in at least one certain conflict. The detected conflicts cause crashes and other kinds of unexpected behavior. Our work helps library developers to prevent conflicts, library users to avoid combining conflicting libraries, and provides evidence that designing a language without explicit namespaces has undesirable effects. © 2018 ACM.},\ndocument_type={Conference Paper},\nsource={Scopus},\n}\n\n"}],"first":[{"id":"Geral0","analysis":{"result":false,"criterion":["CR4"]}},{"id":"Geral1","analysis":{"result":false,"criterion":["CR8"]}},{"id":"Geral2","analysis":{"result":true,"criterion":["CA1"]}},{"id":"Geral3","analysis":{"result":false,"criterion":["CR10"]}},{"id":"Geral4","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral5","analysis":{"result":false,"criterion":["CR10"]}},{"id":"Geral6","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral7","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral8","analysis":{"result":false,"criterion":["CR12"]}},{"id":"Geral9","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral10","analysis":{"result":true,"criterion":["CA1"]}},{"id":"Geral11","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral12","analysis":{"result":true,"criterion":["CA1","CA3"]}},{"id":"Geral13","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral14","analysis":{"result":true,"criterion":["CA1","CA3"]}},{"id":"Geral15","analysis":{"result":false,"criterion":["CR10"]}},{"id":"Geral16","analysis":{"result":true,"criterion":["CA0","CA1","CA3"]}},{"id":"Geral17","analysis":{"result":false,"criterion":["CR8"]}},{"id":"Geral18","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral19","analysis":{"result":false,"criterion":["CR8","CR8"]}},{"id":"Geral20","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral21","analysis":{"result":true,"criterion":["CA3"]}},{"id":"Geral22","analysis":{"result":true,"criterion":["CA3"]}},{"id":"Geral23","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral24","analysis":{"result":true,"criterion":["CA1","CA3"]}},{"id":"Geral25","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral26","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral27","analysis":{"result":true,"criterion":["CA1","CA3"]}},{"id":"Geral28","analysis":{"result":false,"criterion":["CR10"]}},{"id":"Geral29","analysis":{"result":true,"criterion":["CA3"]}},{"id":"Geral30","analysis":{"result":false,"criterion":["CR8"]}},{"id":"Geral32","analysis":{"result":true,"criterion":["CA0","CA1","CA4","CA6"]}},{"id":"Geral34","analysis":{"result":false,"criterion":["CR10"]}},{"id":"Geral35","analysis":{"result":false,"criterion":["CR12"]}},{"id":"Geral36","analysis":{"result":false,"criterion":["CR10"]}},{"id":"Geral37","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral38","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral39","analysis":{"result":true,"criterion":["CA1","CA4"]}},{"id":"Geral40","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral41","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral42","analysis":{"result":true,"criterion":["CA3"]}},{"id":"Geral43","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral44","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral45","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral46","analysis":{"result":false,"criterion":["CR8"]}},{"id":"Geral47","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral48","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral49","analysis":{"result":false,"criterion":["CR10"]}},{"id":"Geral50","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral51","analysis":{"result":false,"criterion":["CR4"]}},{"id":"Geral52","analysis":{"result":false,"criterion":["CR10"]}},{"id":"Geral53","analysis":{"result":false,"criterion":["CR10"]}},{"id":"Geral54","analysis":{"result":true,"criterion":["CA3"]}},{"id":"Geral55","analysis":{"result":false,"criterion":["CR8"]}},{"id":"Geral56","analysis":{"result":false,"criterion":["CR4"]}},{"id":"Geral57","analysis":{"result":true,"criterion":["CA3"]}},{"id":"Geral58","analysis":{"result":true,"criterion":["CA7"]}},{"id":"Geral59","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral60","analysis":{"result":false,"criterion":["CR8"]}},{"id":"Geral61","analysis":{"result":true,"criterion":["CA7"]}},{"id":"Geral62","analysis":{"result":true,"criterion":["CA3"]}},{"id":"Geral63","analysis":{"result":true,"criterion":["CA1"]}},{"id":"Geral64","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral65","analysis":{"result":false,"criterion":["CR8"]}},{"id":"Geral66","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral67","analysis":{"result":false,"criterion":["CR8"]}},{"id":"Geral68","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral69","analysis":{"result":false,"criterion":["CR10"]}},{"id":"Geral70","analysis":{"result":true,"criterion":["CA1","CA3"]}},{"id":"Geral71","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral72","analysis":{"result":true,"criterion":["CA0","CA1","CA3","CA7"]}},{"id":"Geral73","analysis":{"result":true,"criterion":["CA3","CA7"]}},{"id":"Geral74","analysis":{"result":true,"criterion":["CA3"]}},{"id":"Geral75","analysis":{"result":true,"criterion":["CA3"]}},{"id":"Geral76","analysis":{"result":true,"criterion":["CA3"]}},{"id":"Geral77","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral78","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral79","analysis":{"result":false,"criterion":["CR10"]}},{"id":"Geral80","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral81","analysis":{"result":false,"criterion":["CR8"]}},{"id":"Geral82","analysis":{"result":false,"criterion":["CR8"]}},{"id":"Geral83","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral84","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral85","analysis":{"result":false,"criterion":["CR8","CR8"]}},{"id":"Geral86","analysis":{"result":false,"criterion":["CR8"]}},{"id":"Geral87","analysis":{"result":false,"criterion":["CR8"]}},{"id":"Geral88","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral89","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral90","analysis":{"result":false,"criterion":["CR8"]}},{"id":"Geral91","analysis":{"result":false,"criterion":["CR4"]}},{"id":"Geral92","analysis":{"result":false,"criterion":["CR8"]}},{"id":"Geral93","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral94","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral95","analysis":{"result":true,"criterion":["CA7"]}},{"id":"Geral96","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral97","analysis":{"result":true,"criterion":["CA4"]}},{"id":"Geral98","analysis":{"result":false,"criterion":["CR10"]}},{"id":"Geral99","analysis":{"result":false,"criterion":["CR10"]}},{"id":"Geral100","analysis":{"result":false,"criterion":["CR10"]}},{"id":"Geral101","analysis":{"result":true,"criterion":["CA1","CA3"]}},{"id":"Geral102","analysis":{"result":false,"criterion":["CR8"]}},{"id":"Geral103","analysis":{"result":false,"criterion":["CR10"]}},{"id":"Geral104","analysis":{"result":false,"criterion":["CR8"]}},{"id":"Geral105","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral106","analysis":{"result":false,"criterion":["CR12"]}},{"id":"Geral107","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral108","analysis":{"result":true,"criterion":["CA3"]}},{"id":"Geral109","analysis":{"result":false,"criterion":["CR10"]}},{"id":"Geral110","analysis":{"result":false,"criterion":["CR8"]}},{"id":"Geral111","analysis":{"result":false,"criterion":["CR11"]}},{"id":"Geral112","analysis":{"result":false,"criterion":["CR10"]}},{"id":"Geral113","analysis":{"result":false,"criterion":["CR8"]}},{"id":"Geral114","analysis":{"result":false,"criterion":["CR10"]}},{"id":"Geral115","analysis":{"result":false,"criterion":["CR10"]}},{"id":"Geral116","analysis":{"result":true,"criterion":["CA7"]}},{"id":"Geral117","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral118","analysis":{"result":true,"criterion":["CA3"]}},{"id":"Geral119","analysis":{"result":false,"criterion":["CR8"]}},{"id":"Geral120","analysis":{"result":true,"criterion":["CA7"]}},{"id":"Geral121","analysis":{"result":false,"criterion":["CR4"]}},{"id":"Geral122","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral123","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral124","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral125","analysis":{"result":false,"criterion":["CR8"]}},{"id":"Geral126","analysis":{"result":false,"criterion":["CR8","CR8"]}},{"id":"Geral127","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral128","analysis":{"result":false,"criterion":["CR10"]}},{"id":"Geral129","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral130","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral131","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral132","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral133","analysis":{"result":false,"criterion":["CR11","CR12","CR8"]}},{"id":"Geral134","analysis":{"result":true,"criterion":["CA3"]}},{"id":"Geral135","analysis":{"result":true,"criterion":["CA3"]}},{"id":"Geral136","analysis":{"result":false,"criterion":["CR10"]}},{"id":"Geral137","analysis":{"result":false,"criterion":["CR8"]}},{"id":"Geral138","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral139","analysis":{"result":false,"criterion":["CR10"]}},{"id":"Geral140","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral141","analysis":{"result":false,"criterion":["CR8"]}},{"id":"Geral142","analysis":{"result":false,"criterion":["CR8"]}},{"id":"Geral143","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral144","analysis":{"result":false,"criterion":["CR10"]}},{"id":"Geral145","analysis":{"result":false,"criterion":["CR10"]}},{"id":"Geral146","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral147","analysis":{"result":true,"criterion":["CA3"]}},{"id":"Geral148","analysis":{"result":true,"criterion":["CA7"]}},{"id":"Geral150","analysis":{"result":false,"criterion":["CR11","CR12","CR8"]}},{"id":"Geral151","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral152","analysis":{"result":true,"criterion":["CA1"]}},{"id":"Geral153","analysis":{"result":false,"criterion":["CR10"]}},{"id":"Geral154","analysis":{"result":true,"criterion":["CA7"]}},{"id":"Geral155","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral156","analysis":{"result":false,"criterion":["CR8"]}},{"id":"Geral157","analysis":{"result":false,"criterion":["CR8"]}},{"id":"Geral158","analysis":{"result":false,"criterion":["CR8"]}},{"id":"Geral159","analysis":{"result":false,"criterion":["CR8"]}},{"id":"Geral160","analysis":{"result":false,"criterion":["CR10"]}},{"id":"Geral161","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral162","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral163","analysis":{"result":false,"criterion":["CR8"]}},{"id":"Geral164","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral165","analysis":{"result":false,"criterion":["CR8"]}},{"id":"Geral166","analysis":{"result":false,"criterion":["CR10"]}},{"id":"Geral167","analysis":{"result":false,"criterion":["CR8"]}},{"id":"Geral168","analysis":{"result":false,"criterion":["CR4"]}},{"id":"Geral169","analysis":{"result":false,"criterion":["CR4"]}},{"id":"Geral170","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral171","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral172","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral173","analysis":{"result":false,"criterion":["CR12"]}},{"id":"Geral174","analysis":{"result":false,"criterion":["CR8"]}},{"id":"Geral175","analysis":{"result":false,"criterion":["CR10"]}},{"id":"Geral176","analysis":{"result":false,"criterion":["CR10"]}},{"id":"Geral177","analysis":{"result":false,"criterion":["CR4"]}},{"id":"Geral178","analysis":{"result":false,"criterion":["CR8"]}},{"id":"Geral179","analysis":{"result":false,"criterion":["CR10"]}},{"id":"Geral180","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral181","analysis":{"result":false,"criterion":["CR10"]}},{"id":"Geral182","analysis":{"result":true,"criterion":["CA3"]}},{"id":"Geral183","analysis":{"result":false,"criterion":["CR0"]}},{"id":"Geral184","analysis":{"result":false,"criterion":["CR8"]}},{"id":"Geral185","analysis":{"result":false,"criterion":["CR8"]}},{"id":"Geral186","analysis":{"result":false,"criterion":["CR8"]}},{"id":"Geral187","analysis":{"result":false,"criterion":["CR4"]}},{"id":"Geral188","analysis":{"result":false,"criterion":["CR8"]}},{"id":"Geral189","analysis":{"result":false,"criterion":["CR11","CR12"]}},{"id":"Geral190","analysis":{"result":false,"criterion":["CR8"]}},{"id":"Geral191","analysis":{"result":false,"criterion":["CR8"]}},{"id":"Geral192","analysis":{"result":false,"criterion":["CR10"]}},{"id":"Geral193","analysis":{"result":false,"criterion":["CR8"]}},{"id":"Geral194","analysis":{"result":false,"criterion":["CR11","CR12"]}}],"second":[],"result":[]}